{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58b4026-f9e1-4a11-94f4-800fbe45d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers,layers,regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "from keras.models import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Activation\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,BatchNormalization,Concatenate\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24615253-c8c0-42f8-80ad-ba75cfb20835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 1.15.0\n",
      "Keras Version: 2.2.4-tf\n",
      "\n",
      "Python 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n",
      "Pandas 1.3.5\n",
      "Scikit-Learn 1.0.2\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# What version of Python do you have?\n",
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.compat.v1.config.experimental.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5300195f-3c3c-4b56-b482-2bb9f84a9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b91543-a7da-4c16-a9af-196de6b640de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(seq):\n",
    "    bases = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "    X = np.zeros((len(seq),len(seq[0]),len(bases)))\n",
    "    for i,m in enumerate(seq):\n",
    "        for l,s in enumerate(m):\n",
    "    #         print(s)\n",
    "            if s in bases:\n",
    "                X[i,l,bases.index(s)] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e1c7a9-6960-4aa2-b6f0-9d3ce1a4bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    '''File_path: Path to the fasta file\n",
    "       Returns: List of sequence\n",
    "    '''\n",
    "    one=list(SeqIO.parse(file_path,'fasta'))\n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480e26dc-97c8-4f2b-b48d-7103ee5cdab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data=read_fasta('acp_train_1912_padded.fasta')\n",
    "negative_data=read_fasta('neg_acp_train_1912_padded.fasta')\n",
    "data_pure=positive_data+negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b9cf4f-17d5-4701-ab3c-71a478c585ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_positive=onehot(positive_data)\n",
    "one_hot_negative=onehot(negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1dd1ad2-5efb-4208-99a4-0abdcde08691",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.concatenate((one_hot_positive,one_hot_negative),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85284992-9c26-4337-9e84-55375b31e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label=one_hot_positive.shape[0]*[[0,1]]+one_hot_negative.shape[0]*[[1,0]]\n",
    "training_label=np.asarray(training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce47cb3c-27f7-429f-8dad-53a92f5fbbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3824, 55, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3aafb2f-5584-4e8c-a601-29118737176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd18ad5-c3e8-47c5-9659-0864a577677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capsule Net for Anticancer peptide Prediction Starts from Here\n",
    "#Our Capsnets code is modified version and origin is based on the keras implementation of CapsuleNet by Xifeng by Xifeng Guo,\n",
    "#E-mail: guoxifeng1990@163.com, Github: `https://github.com/XifengGuo/CapsNet-Keras\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "208f41e1-cc75-4e4f-a596-2c3180a1f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead89f41-8ae1-4450-bfb1-8be48bf14c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(\n",
    "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing by aggrement  algorithm 1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            \n",
    "            # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "                # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7171c5e7-de3b-40a1-853a-f1a6be1b72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv1D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv1d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0043d3-fdf4-4174-9f33-f2e4c2609a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    conv1 = layers.Conv1D(filters=64, kernel_size=7, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    conv1=Dropout(0.7)(conv1)\n",
    "    lstm = layers.LSTM(units=128,   name='Lstm',  return_sequences=True)(conv1)\n",
    "    # Layer 2: Conv1D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(lstm, dim_vector=8, n_channels=16, kernel_size=3, strides=1, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    KcrCaps = CapsuleLayer(num_capsule=n_class, dim_vector=8, num_routing=num_routing, name='KcrCaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out = Length(name='capsnet')(KcrCaps)\n",
    "    #model\n",
    "    train_model = Model(x, out)\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b17382f7-a4d7-4dc0-ae76-1f7f6303da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "     When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae3cc550-99f5-47c0-8d4e-6756c3554cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #155: KMP_AFFINITY: Initial OS proc set respected: 0-39\n",
      "OMP: Info #216: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #157: KMP_AFFINITY: 40 available OS procs\n",
      "OMP: Info #158: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"LL cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L3 cache\" is equivalent to \"socket\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L2 cache\" is equivalent to \"core\".\n",
      "OMP: Info #287: KMP_AFFINITY: topology layer \"L1 cache\" is equivalent to \"core\".\n",
      "OMP: Info #192: KMP_AFFINITY: 2 sockets x 10 cores/socket x 2 threads/core (20 total cores)\n",
      "OMP: Info #218: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 20 maps to socket 0 core 0 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 1 maps to socket 0 core 1 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 21 maps to socket 0 core 1 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 2 maps to socket 0 core 2 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 22 maps to socket 0 core 2 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 3 maps to socket 0 core 3 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 23 maps to socket 0 core 3 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 4 maps to socket 0 core 4 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 24 maps to socket 0 core 4 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 5 maps to socket 0 core 8 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 25 maps to socket 0 core 8 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 6 maps to socket 0 core 9 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 26 maps to socket 0 core 9 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 7 maps to socket 0 core 10 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 27 maps to socket 0 core 10 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 8 maps to socket 0 core 11 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 28 maps to socket 0 core 11 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 9 maps to socket 0 core 12 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 29 maps to socket 0 core 12 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 10 maps to socket 1 core 0 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 30 maps to socket 1 core 0 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 11 maps to socket 1 core 1 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 31 maps to socket 1 core 1 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 12 maps to socket 1 core 2 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 32 maps to socket 1 core 2 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 13 maps to socket 1 core 3 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 33 maps to socket 1 core 3 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 14 maps to socket 1 core 4 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 34 maps to socket 1 core 4 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 15 maps to socket 1 core 8 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 35 maps to socket 1 core 8 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 16 maps to socket 1 core 9 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 36 maps to socket 1 core 9 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 17 maps to socket 1 core 10 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 37 maps to socket 1 core 10 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 18 maps to socket 1 core 11 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 38 maps to socket 1 core 11 thread 1 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 19 maps to socket 1 core 12 thread 0 \n",
      "OMP: Info #172: KMP_AFFINITY: OS proc 39 maps to socket 1 core 12 thread 1 \n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8120 thread 0 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8173 thread 1 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8174 thread 2 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8176 thread 4 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8175 thread 3 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8177 thread 5 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8178 thread 6 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8179 thread 7 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8180 thread 8 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8181 thread 9 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8182 thread 10 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8183 thread 11 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8184 thread 12 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8185 thread 13 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8186 thread 14 bound to OS proc set 14\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8187 thread 15 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8188 thread 16 bound to OS proc set 16\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8189 thread 17 bound to OS proc set 17\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8190 thread 18 bound to OS proc set 18\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8191 thread 19 bound to OS proc set 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_8120/1706877149.py:88: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 55, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 49, 64)            9024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 49, 64)            0         \n",
      "_________________________________________________________________\n",
      "Lstm (LSTM)                  (None, 49, 128)           98816     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 47, 128)           49280     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 752, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 752, 8)            0         \n",
      "_________________________________________________________________\n",
      "KcrCaps (CapsuleLayer)       (None, 2, 8)              97760     \n",
      "_________________________________________________________________\n",
      "capsnet (Length)             (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 254,880\n",
      "Trainable params: 253,376\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#55 is length of each sequence and 20 is the one-hot encoding vector for each amino acids, which means we used 55 lenthg of sequence\n",
    "mode1= CapsNet(input_shape=(55,20),n_class=2,num_routing=3)\n",
    "mode1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5d9537f-91af-4bc6-87dd-105effb173cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "kf= KFold(n_splits=5, shuffle=True)\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "base_fpr[-1]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49946387-7ee8-4f20-a47f-43134f183ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 3059 samples, validate on 765 samples\n",
      "Epoch 1/600\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sadik/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 00:28:11.589486: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 00:28:11.616717: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400050000 Hz\n",
      "2024-05-02 00:28:11.616954: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fd25f095b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-02 00:28:11.616974: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-02 00:28:11.617089: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8201 thread 20 bound to OS proc set 20\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8208 thread 21 bound to OS proc set 21\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8209 thread 22 bound to OS proc set 22\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8210 thread 23 bound to OS proc set 23\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8211 thread 24 bound to OS proc set 24\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8212 thread 25 bound to OS proc set 25\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8213 thread 26 bound to OS proc set 26\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8214 thread 27 bound to OS proc set 27\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8215 thread 28 bound to OS proc set 28\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8216 thread 29 bound to OS proc set 29\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8217 thread 30 bound to OS proc set 30\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8219 thread 32 bound to OS proc set 32\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8218 thread 31 bound to OS proc set 31\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8220 thread 33 bound to OS proc set 33\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8221 thread 34 bound to OS proc set 34\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8222 thread 35 bound to OS proc set 35\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8224 thread 37 bound to OS proc set 37\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8226 thread 39 bound to OS proc set 39\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8223 thread 36 bound to OS proc set 36\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8225 thread 38 bound to OS proc set 38\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8227 thread 40 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8202 thread 41 bound to OS proc set 1\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8228 thread 42 bound to OS proc set 2\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8229 thread 43 bound to OS proc set 3\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8230 thread 44 bound to OS proc set 4\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8231 thread 45 bound to OS proc set 5\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8233 thread 47 bound to OS proc set 7\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8234 thread 48 bound to OS proc set 8\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8232 thread 46 bound to OS proc set 6\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8235 thread 49 bound to OS proc set 9\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8236 thread 50 bound to OS proc set 10\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8238 thread 52 bound to OS proc set 12\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8237 thread 51 bound to OS proc set 11\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8239 thread 53 bound to OS proc set 13\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8240 thread 54 bound to OS proc set 14\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8241 thread 55 bound to OS proc set 15\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8242 thread 56 bound to OS proc set 16\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8243 thread 57 bound to OS proc set 17\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8244 thread 58 bound to OS proc set 18\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8245 thread 59 bound to OS proc set 19\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8247 thread 61 bound to OS proc set 21\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8246 thread 60 bound to OS proc set 20\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8248 thread 62 bound to OS proc set 22\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8249 thread 63 bound to OS proc set 23\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8251 thread 65 bound to OS proc set 25\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8252 thread 66 bound to OS proc set 26\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8250 thread 64 bound to OS proc set 24\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8253 thread 67 bound to OS proc set 27\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8254 thread 68 bound to OS proc set 28\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8255 thread 69 bound to OS proc set 29\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8256 thread 70 bound to OS proc set 30\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8257 thread 71 bound to OS proc set 31\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8259 thread 73 bound to OS proc set 33\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8261 thread 75 bound to OS proc set 35\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8262 thread 76 bound to OS proc set 36\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8263 thread 77 bound to OS proc set 37\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8264 thread 78 bound to OS proc set 38\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8265 thread 79 bound to OS proc set 39\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8260 thread 74 bound to OS proc set 34\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8266 thread 80 bound to OS proc set 0\n",
      "OMP: Info #254: KMP_AFFINITY: pid 8120 tid 8258 thread 72 bound to OS proc set 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.5407 - acc: 0.5492 - val_loss: 0.2716 - val_acc: 0.5948\n",
      "Epoch 2/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2476 - acc: 0.6018 - val_loss: 0.2225 - val_acc: 0.6105\n",
      "Epoch 3/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.2168 - acc: 0.6087 - val_loss: 0.2079 - val_acc: 0.6013\n",
      "Epoch 4/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2077 - acc: 0.6129 - val_loss: 0.2026 - val_acc: 0.6078\n",
      "Epoch 5/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.2053 - acc: 0.6136 - val_loss: 0.2006 - val_acc: 0.6118\n",
      "Epoch 6/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.2024 - acc: 0.6214 - val_loss: 0.1998 - val_acc: 0.6144\n",
      "Epoch 7/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2003 - acc: 0.6339 - val_loss: 0.2032 - val_acc: 0.5922\n",
      "Epoch 8/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1971 - acc: 0.6339 - val_loss: 0.2028 - val_acc: 0.5922\n",
      "Epoch 9/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1926 - acc: 0.6568 - val_loss: 0.1892 - val_acc: 0.6484\n",
      "Epoch 10/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1851 - acc: 0.6836 - val_loss: 0.1853 - val_acc: 0.6301\n",
      "Epoch 11/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1713 - acc: 0.7202 - val_loss: 0.1735 - val_acc: 0.6810\n",
      "Epoch 12/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1530 - acc: 0.7676 - val_loss: 0.1490 - val_acc: 0.7961\n",
      "Epoch 13/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1409 - acc: 0.7833 - val_loss: 0.1464 - val_acc: 0.7608\n",
      "Epoch 14/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1335 - acc: 0.7918 - val_loss: 0.1413 - val_acc: 0.7725\n",
      "Epoch 15/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1278 - acc: 0.8022 - val_loss: 0.1353 - val_acc: 0.7882\n",
      "Epoch 16/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1266 - acc: 0.8061 - val_loss: 0.1313 - val_acc: 0.8000\n",
      "Epoch 17/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1233 - acc: 0.8081 - val_loss: 0.1309 - val_acc: 0.8000\n",
      "Epoch 18/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1231 - acc: 0.8107 - val_loss: 0.1327 - val_acc: 0.8026\n",
      "Epoch 19/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1195 - acc: 0.8209 - val_loss: 0.1280 - val_acc: 0.8013\n",
      "Epoch 20/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.1210 - acc: 0.8182 - val_loss: 0.1267 - val_acc: 0.8052\n",
      "Epoch 21/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1166 - acc: 0.8238 - val_loss: 0.1231 - val_acc: 0.8078\n",
      "Epoch 22/600\n",
      "3059/3059 [==============================] - 32s 11ms/step - loss: 0.1164 - acc: 0.8212 - val_loss: 0.1276 - val_acc: 0.8078\n",
      "Epoch 23/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1146 - acc: 0.8320 - val_loss: 0.1280 - val_acc: 0.8000\n",
      "Epoch 24/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1128 - acc: 0.8297 - val_loss: 0.1210 - val_acc: 0.8118\n",
      "Epoch 25/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1091 - acc: 0.8385 - val_loss: 0.1232 - val_acc: 0.8052\n",
      "Epoch 26/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1118 - acc: 0.8310 - val_loss: 0.1196 - val_acc: 0.8157\n",
      "Epoch 27/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1113 - acc: 0.8339 - val_loss: 0.1210 - val_acc: 0.8118\n",
      "Epoch 28/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1082 - acc: 0.8352 - val_loss: 0.1202 - val_acc: 0.8170\n",
      "Epoch 29/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1077 - acc: 0.8428 - val_loss: 0.1199 - val_acc: 0.8118\n",
      "Epoch 30/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1080 - acc: 0.8395 - val_loss: 0.1178 - val_acc: 0.8235\n",
      "Epoch 31/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1063 - acc: 0.8444 - val_loss: 0.1195 - val_acc: 0.8222\n",
      "Epoch 32/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1053 - acc: 0.8418 - val_loss: 0.1170 - val_acc: 0.8209\n",
      "Epoch 33/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1024 - acc: 0.8477 - val_loss: 0.1152 - val_acc: 0.8196\n",
      "Epoch 34/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1016 - acc: 0.8513 - val_loss: 0.1296 - val_acc: 0.8052\n",
      "Epoch 35/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1044 - acc: 0.8513 - val_loss: 0.1128 - val_acc: 0.8222\n",
      "Epoch 36/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1028 - acc: 0.8549 - val_loss: 0.1163 - val_acc: 0.8248\n",
      "Epoch 37/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1018 - acc: 0.8509 - val_loss: 0.1130 - val_acc: 0.8209\n",
      "Epoch 38/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0994 - acc: 0.8565 - val_loss: 0.1191 - val_acc: 0.8275\n",
      "Epoch 39/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1005 - acc: 0.8513 - val_loss: 0.1107 - val_acc: 0.8261\n",
      "Epoch 40/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0990 - acc: 0.8496 - val_loss: 0.1099 - val_acc: 0.8235\n",
      "Epoch 41/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0988 - acc: 0.8516 - val_loss: 0.1115 - val_acc: 0.8327\n",
      "Epoch 42/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0992 - acc: 0.8490 - val_loss: 0.1173 - val_acc: 0.8235\n",
      "Epoch 43/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0979 - acc: 0.8571 - val_loss: 0.1129 - val_acc: 0.8353\n",
      "Epoch 44/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0964 - acc: 0.8607 - val_loss: 0.1067 - val_acc: 0.8327\n",
      "Epoch 45/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0959 - acc: 0.8591 - val_loss: 0.1047 - val_acc: 0.8353\n",
      "Epoch 46/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0941 - acc: 0.8607 - val_loss: 0.1060 - val_acc: 0.8340\n",
      "Epoch 47/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0948 - acc: 0.8598 - val_loss: 0.1056 - val_acc: 0.8340\n",
      "Epoch 48/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0942 - acc: 0.8591 - val_loss: 0.1049 - val_acc: 0.8353\n",
      "Epoch 49/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0936 - acc: 0.8549 - val_loss: 0.1027 - val_acc: 0.8353\n",
      "Epoch 50/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0917 - acc: 0.8650 - val_loss: 0.1015 - val_acc: 0.8392\n",
      "Epoch 51/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0919 - acc: 0.8634 - val_loss: 0.1027 - val_acc: 0.8379\n",
      "Epoch 52/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0921 - acc: 0.8709 - val_loss: 0.1057 - val_acc: 0.8484\n",
      "Epoch 53/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0903 - acc: 0.8679 - val_loss: 0.0992 - val_acc: 0.8418\n",
      "Epoch 54/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0898 - acc: 0.8764 - val_loss: 0.1001 - val_acc: 0.8353\n",
      "Epoch 55/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0907 - acc: 0.8679 - val_loss: 0.1021 - val_acc: 0.8536\n",
      "Epoch 56/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0882 - acc: 0.8735 - val_loss: 0.0979 - val_acc: 0.8392\n",
      "Epoch 57/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0889 - acc: 0.8748 - val_loss: 0.0974 - val_acc: 0.8405\n",
      "Epoch 58/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0890 - acc: 0.8732 - val_loss: 0.1022 - val_acc: 0.8627\n",
      "Epoch 59/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0885 - acc: 0.8705 - val_loss: 0.0968 - val_acc: 0.8418\n",
      "Epoch 60/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0884 - acc: 0.8679 - val_loss: 0.0989 - val_acc: 0.8575\n",
      "Epoch 61/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0881 - acc: 0.8709 - val_loss: 0.0958 - val_acc: 0.8431\n",
      "Epoch 62/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0860 - acc: 0.8751 - val_loss: 0.0969 - val_acc: 0.8627\n",
      "Epoch 63/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0842 - acc: 0.8754 - val_loss: 0.0954 - val_acc: 0.8497\n",
      "Epoch 64/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0854 - acc: 0.8758 - val_loss: 0.0984 - val_acc: 0.8641\n",
      "Epoch 65/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0865 - acc: 0.8774 - val_loss: 0.0961 - val_acc: 0.8588\n",
      "Epoch 66/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0871 - acc: 0.8719 - val_loss: 0.0938 - val_acc: 0.8523\n",
      "Epoch 67/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0874 - acc: 0.8696 - val_loss: 0.0939 - val_acc: 0.8549\n",
      "Epoch 68/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0881 - acc: 0.8705 - val_loss: 0.0939 - val_acc: 0.8627\n",
      "Epoch 69/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0836 - acc: 0.8817 - val_loss: 0.0970 - val_acc: 0.8680\n",
      "Epoch 70/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0847 - acc: 0.8797 - val_loss: 0.0937 - val_acc: 0.8523\n",
      "Epoch 71/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0832 - acc: 0.8820 - val_loss: 0.0931 - val_acc: 0.8601\n",
      "Epoch 72/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0827 - acc: 0.8826 - val_loss: 0.0923 - val_acc: 0.8536\n",
      "Epoch 73/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0826 - acc: 0.8804 - val_loss: 0.0933 - val_acc: 0.8601\n",
      "Epoch 74/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0864 - acc: 0.8754 - val_loss: 0.0909 - val_acc: 0.8588\n",
      "Epoch 75/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0825 - acc: 0.8804 - val_loss: 0.0922 - val_acc: 0.8575\n",
      "Epoch 76/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0797 - acc: 0.8839 - val_loss: 0.0931 - val_acc: 0.8601\n",
      "Epoch 77/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0806 - acc: 0.8839 - val_loss: 0.0909 - val_acc: 0.8654\n",
      "Epoch 78/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0826 - acc: 0.8820 - val_loss: 0.0907 - val_acc: 0.8627\n",
      "Epoch 79/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0835 - acc: 0.8807 - val_loss: 0.0958 - val_acc: 0.8680\n",
      "Epoch 80/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0828 - acc: 0.8804 - val_loss: 0.0930 - val_acc: 0.8654\n",
      "Epoch 81/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0804 - acc: 0.8823 - val_loss: 0.0932 - val_acc: 0.8641\n",
      "Epoch 82/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0803 - acc: 0.8849 - val_loss: 0.0940 - val_acc: 0.8693\n",
      "Epoch 83/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0810 - acc: 0.8813 - val_loss: 0.0886 - val_acc: 0.8601\n",
      "Epoch 84/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0788 - acc: 0.8826 - val_loss: 0.0934 - val_acc: 0.8627\n",
      "Epoch 85/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0803 - acc: 0.8875 - val_loss: 0.0916 - val_acc: 0.8680\n",
      "Epoch 86/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0799 - acc: 0.8872 - val_loss: 0.0921 - val_acc: 0.8680\n",
      "Epoch 87/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0787 - acc: 0.8892 - val_loss: 0.0920 - val_acc: 0.8706\n",
      "Epoch 88/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0786 - acc: 0.8859 - val_loss: 0.0903 - val_acc: 0.8627\n",
      "Epoch 89/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0795 - acc: 0.8836 - val_loss: 0.0954 - val_acc: 0.8719\n",
      "Epoch 90/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0759 - acc: 0.8915 - val_loss: 0.0884 - val_acc: 0.8614\n",
      "Epoch 91/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0787 - acc: 0.8820 - val_loss: 0.0980 - val_acc: 0.8654\n",
      "Epoch 92/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0771 - acc: 0.8869 - val_loss: 0.0887 - val_acc: 0.8667\n",
      "Epoch 93/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0780 - acc: 0.8885 - val_loss: 0.0876 - val_acc: 0.8654\n",
      "Epoch 94/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0774 - acc: 0.8856 - val_loss: 0.0875 - val_acc: 0.8667\n",
      "Epoch 95/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0751 - acc: 0.8934 - val_loss: 0.0888 - val_acc: 0.8641\n",
      "Epoch 96/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0772 - acc: 0.8895 - val_loss: 0.0864 - val_acc: 0.8654\n",
      "Epoch 97/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0753 - acc: 0.8924 - val_loss: 0.0903 - val_acc: 0.8758\n",
      "Epoch 98/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0739 - acc: 0.8970 - val_loss: 0.0887 - val_acc: 0.8641\n",
      "Epoch 99/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0745 - acc: 0.8895 - val_loss: 0.0871 - val_acc: 0.8654\n",
      "Epoch 100/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0744 - acc: 0.8931 - val_loss: 0.0866 - val_acc: 0.8680\n",
      "Epoch 101/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0725 - acc: 0.8931 - val_loss: 0.0865 - val_acc: 0.8654\n",
      "Epoch 102/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0731 - acc: 0.8947 - val_loss: 0.0885 - val_acc: 0.8693\n",
      "Epoch 103/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0764 - acc: 0.8879 - val_loss: 0.0888 - val_acc: 0.8706\n",
      "Epoch 104/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0737 - acc: 0.8977 - val_loss: 0.0868 - val_acc: 0.8693\n",
      "Epoch 105/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0740 - acc: 0.8980 - val_loss: 0.0866 - val_acc: 0.8693\n",
      "Epoch 106/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0730 - acc: 0.8947 - val_loss: 0.0852 - val_acc: 0.8771\n",
      "Epoch 107/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0739 - acc: 0.8960 - val_loss: 0.0913 - val_acc: 0.8758\n",
      "Epoch 108/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0728 - acc: 0.8941 - val_loss: 0.0856 - val_acc: 0.8719\n",
      "Epoch 109/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0733 - acc: 0.8931 - val_loss: 0.0877 - val_acc: 0.8719\n",
      "Epoch 110/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0719 - acc: 0.8980 - val_loss: 0.0889 - val_acc: 0.8745\n",
      "Epoch 111/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0723 - acc: 0.8951 - val_loss: 0.0843 - val_acc: 0.8745\n",
      "Epoch 112/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0734 - acc: 0.8954 - val_loss: 0.0861 - val_acc: 0.8601\n",
      "Epoch 113/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0716 - acc: 0.8964 - val_loss: 0.0850 - val_acc: 0.8758\n",
      "Epoch 114/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0712 - acc: 0.9009 - val_loss: 0.0847 - val_acc: 0.8732\n",
      "Epoch 115/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0718 - acc: 0.8990 - val_loss: 0.0890 - val_acc: 0.8745\n",
      "Epoch 116/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0721 - acc: 0.8957 - val_loss: 0.0860 - val_acc: 0.8758\n",
      "Epoch 117/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0711 - acc: 0.8993 - val_loss: 0.0837 - val_acc: 0.8693\n",
      "Epoch 118/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0693 - acc: 0.9009 - val_loss: 0.0828 - val_acc: 0.8784\n",
      "Epoch 119/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0692 - acc: 0.8996 - val_loss: 0.0838 - val_acc: 0.8719\n",
      "Epoch 120/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0693 - acc: 0.8996 - val_loss: 0.0868 - val_acc: 0.8745\n",
      "Epoch 121/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0694 - acc: 0.8980 - val_loss: 0.0846 - val_acc: 0.8758\n",
      "Epoch 122/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0700 - acc: 0.8944 - val_loss: 0.0866 - val_acc: 0.8784\n",
      "Epoch 123/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0691 - acc: 0.9003 - val_loss: 0.0824 - val_acc: 0.8797\n",
      "Epoch 124/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0698 - acc: 0.8996 - val_loss: 0.0841 - val_acc: 0.8745\n",
      "Epoch 125/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0680 - acc: 0.9013 - val_loss: 0.0820 - val_acc: 0.8810\n",
      "Epoch 126/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0696 - acc: 0.8947 - val_loss: 0.0832 - val_acc: 0.8784\n",
      "Epoch 127/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0673 - acc: 0.9055 - val_loss: 0.0829 - val_acc: 0.8784\n",
      "Epoch 128/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0690 - acc: 0.8993 - val_loss: 0.0855 - val_acc: 0.8771\n",
      "Epoch 129/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0685 - acc: 0.9000 - val_loss: 0.0825 - val_acc: 0.8745\n",
      "Epoch 130/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0675 - acc: 0.9078 - val_loss: 0.0852 - val_acc: 0.8797\n",
      "Epoch 131/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0660 - acc: 0.9039 - val_loss: 0.0837 - val_acc: 0.8797\n",
      "Epoch 132/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0661 - acc: 0.9059 - val_loss: 0.0844 - val_acc: 0.8810\n",
      "Epoch 133/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0660 - acc: 0.9075 - val_loss: 0.0854 - val_acc: 0.8797\n",
      "Epoch 134/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0631 - acc: 0.9104 - val_loss: 0.0822 - val_acc: 0.8784\n",
      "Epoch 135/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0684 - acc: 0.9013 - val_loss: 0.0839 - val_acc: 0.8784\n",
      "Epoch 136/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0660 - acc: 0.9075 - val_loss: 0.0822 - val_acc: 0.8784\n",
      "Epoch 137/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0667 - acc: 0.9039 - val_loss: 0.0836 - val_acc: 0.8824\n",
      "Epoch 138/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0677 - acc: 0.9000 - val_loss: 0.0815 - val_acc: 0.8784\n",
      "Epoch 139/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0651 - acc: 0.9068 - val_loss: 0.0927 - val_acc: 0.8706\n",
      "Epoch 140/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0640 - acc: 0.9081 - val_loss: 0.0819 - val_acc: 0.8810\n",
      "Epoch 141/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0659 - acc: 0.9055 - val_loss: 0.0855 - val_acc: 0.8732\n",
      "Epoch 142/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0654 - acc: 0.9081 - val_loss: 0.0825 - val_acc: 0.8824\n",
      "Epoch 143/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0663 - acc: 0.9026 - val_loss: 0.0816 - val_acc: 0.8797\n",
      "Epoch 144/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0655 - acc: 0.9039 - val_loss: 0.0834 - val_acc: 0.8758\n",
      "Epoch 145/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0652 - acc: 0.9036 - val_loss: 0.0806 - val_acc: 0.8797\n",
      "Epoch 146/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0630 - acc: 0.9108 - val_loss: 0.0849 - val_acc: 0.8784\n",
      "Epoch 147/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0643 - acc: 0.9108 - val_loss: 0.0823 - val_acc: 0.8824\n",
      "Epoch 148/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0632 - acc: 0.9085 - val_loss: 0.0824 - val_acc: 0.8810\n",
      "Epoch 149/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0624 - acc: 0.9108 - val_loss: 0.0826 - val_acc: 0.8797\n",
      "Epoch 150/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0626 - acc: 0.9104 - val_loss: 0.0839 - val_acc: 0.8784\n",
      "Epoch 151/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0635 - acc: 0.9081 - val_loss: 0.0842 - val_acc: 0.8824\n",
      "Epoch 152/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0650 - acc: 0.9068 - val_loss: 0.0824 - val_acc: 0.8810\n",
      "Epoch 153/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0628 - acc: 0.9104 - val_loss: 0.0829 - val_acc: 0.8771\n",
      "Epoch 154/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0647 - acc: 0.9042 - val_loss: 0.0815 - val_acc: 0.8824\n",
      "Epoch 155/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0626 - acc: 0.9114 - val_loss: 0.0813 - val_acc: 0.8784\n",
      "Epoch 156/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0606 - acc: 0.9160 - val_loss: 0.0819 - val_acc: 0.8784\n",
      "Epoch 157/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0631 - acc: 0.9098 - val_loss: 0.0819 - val_acc: 0.8784\n",
      "Epoch 158/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0616 - acc: 0.9094 - val_loss: 0.0808 - val_acc: 0.8784\n",
      "Epoch 159/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0594 - acc: 0.9170 - val_loss: 0.0890 - val_acc: 0.8784\n",
      "Epoch 160/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0661 - acc: 0.9088 - val_loss: 0.0801 - val_acc: 0.8837\n",
      "Epoch 161/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0609 - acc: 0.9150 - val_loss: 0.0809 - val_acc: 0.8824\n",
      "Epoch 162/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0620 - acc: 0.9072 - val_loss: 0.0838 - val_acc: 0.8732\n",
      "Epoch 163/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0605 - acc: 0.9124 - val_loss: 0.0801 - val_acc: 0.8837\n",
      "Epoch 164/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0626 - acc: 0.9085 - val_loss: 0.0794 - val_acc: 0.8850\n",
      "Epoch 165/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0600 - acc: 0.9153 - val_loss: 0.0839 - val_acc: 0.8784\n",
      "Epoch 166/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0619 - acc: 0.9091 - val_loss: 0.0789 - val_acc: 0.8850\n",
      "Epoch 167/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0597 - acc: 0.9140 - val_loss: 0.0817 - val_acc: 0.8837\n",
      "Epoch 168/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0615 - acc: 0.9104 - val_loss: 0.0820 - val_acc: 0.8824\n",
      "Epoch 169/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0617 - acc: 0.9081 - val_loss: 0.0793 - val_acc: 0.8837\n",
      "Epoch 170/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0556 - acc: 0.9170 - val_loss: 0.0787 - val_acc: 0.8837\n",
      "Epoch 171/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0591 - acc: 0.9117 - val_loss: 0.0811 - val_acc: 0.8824\n",
      "Epoch 172/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0607 - acc: 0.9121 - val_loss: 0.0874 - val_acc: 0.8719\n",
      "Epoch 173/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0590 - acc: 0.9166 - val_loss: 0.0781 - val_acc: 0.8810\n",
      "Epoch 174/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0570 - acc: 0.9209 - val_loss: 0.0818 - val_acc: 0.8850\n",
      "Epoch 175/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0561 - acc: 0.9193 - val_loss: 0.0788 - val_acc: 0.8810\n",
      "Epoch 176/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0601 - acc: 0.9114 - val_loss: 0.0862 - val_acc: 0.8745\n",
      "Epoch 177/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0595 - acc: 0.9137 - val_loss: 0.0797 - val_acc: 0.8810\n",
      "Epoch 178/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0580 - acc: 0.9150 - val_loss: 0.0833 - val_acc: 0.8797\n",
      "Epoch 179/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0605 - acc: 0.9091 - val_loss: 0.0800 - val_acc: 0.8850\n",
      "Epoch 180/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0600 - acc: 0.9111 - val_loss: 0.0804 - val_acc: 0.8810\n",
      "Epoch 181/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0574 - acc: 0.9202 - val_loss: 0.0827 - val_acc: 0.8784\n",
      "Epoch 182/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0581 - acc: 0.9206 - val_loss: 0.0819 - val_acc: 0.8824\n",
      "Epoch 183/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0582 - acc: 0.9117 - val_loss: 0.0798 - val_acc: 0.8863\n",
      "Epoch 184/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0582 - acc: 0.9153 - val_loss: 0.0812 - val_acc: 0.8850\n",
      "Epoch 185/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0557 - acc: 0.9232 - val_loss: 0.0800 - val_acc: 0.8876\n",
      "Epoch 186/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0582 - acc: 0.9157 - val_loss: 0.0774 - val_acc: 0.8889\n",
      "Epoch 187/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0548 - acc: 0.9222 - val_loss: 0.0777 - val_acc: 0.8837\n",
      "Epoch 188/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0541 - acc: 0.9238 - val_loss: 0.0786 - val_acc: 0.8837\n",
      "Epoch 189/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0593 - acc: 0.9153 - val_loss: 0.0825 - val_acc: 0.8810\n",
      "Epoch 190/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0572 - acc: 0.9179 - val_loss: 0.0890 - val_acc: 0.8758\n",
      "Epoch 191/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0567 - acc: 0.9179 - val_loss: 0.0779 - val_acc: 0.8863\n",
      "Epoch 192/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0571 - acc: 0.9153 - val_loss: 0.0783 - val_acc: 0.8824\n",
      "Epoch 193/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0549 - acc: 0.9212 - val_loss: 0.0813 - val_acc: 0.8850\n",
      "Epoch 194/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0569 - acc: 0.9199 - val_loss: 0.0755 - val_acc: 0.8824\n",
      "Epoch 195/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0566 - acc: 0.9170 - val_loss: 0.0821 - val_acc: 0.8810\n",
      "Epoch 196/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0562 - acc: 0.9202 - val_loss: 0.0794 - val_acc: 0.8824\n",
      "Epoch 197/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0556 - acc: 0.9215 - val_loss: 0.0772 - val_acc: 0.8850\n",
      "Epoch 198/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0561 - acc: 0.9157 - val_loss: 0.0798 - val_acc: 0.8810\n",
      "Epoch 199/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0547 - acc: 0.9219 - val_loss: 0.0777 - val_acc: 0.8850\n",
      "Epoch 200/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0553 - acc: 0.9173 - val_loss: 0.0779 - val_acc: 0.8824\n",
      "Epoch 201/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0562 - acc: 0.9176 - val_loss: 0.0779 - val_acc: 0.8837\n",
      "Epoch 202/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0550 - acc: 0.9242 - val_loss: 0.0794 - val_acc: 0.8824\n",
      "Epoch 203/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0538 - acc: 0.9258 - val_loss: 0.0771 - val_acc: 0.8863\n",
      "Epoch 204/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0545 - acc: 0.9225 - val_loss: 0.0778 - val_acc: 0.8876\n",
      "Epoch 205/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0523 - acc: 0.9268 - val_loss: 0.0766 - val_acc: 0.8863\n",
      "Epoch 206/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0547 - acc: 0.9202 - val_loss: 0.0784 - val_acc: 0.8850\n",
      "Epoch 207/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0552 - acc: 0.9202 - val_loss: 0.0775 - val_acc: 0.8889\n",
      "Epoch 208/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0535 - acc: 0.9287 - val_loss: 0.0791 - val_acc: 0.8889\n",
      "Epoch 209/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0548 - acc: 0.9248 - val_loss: 0.0771 - val_acc: 0.8837\n",
      "Epoch 210/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0541 - acc: 0.9189 - val_loss: 0.0797 - val_acc: 0.8850\n",
      "Epoch 211/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0557 - acc: 0.9199 - val_loss: 0.0800 - val_acc: 0.8824\n",
      "Epoch 212/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0537 - acc: 0.9225 - val_loss: 0.0811 - val_acc: 0.8810\n",
      "Epoch 213/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0535 - acc: 0.9219 - val_loss: 0.0803 - val_acc: 0.8810\n",
      "Epoch 214/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0550 - acc: 0.9202 - val_loss: 0.0759 - val_acc: 0.8863\n",
      "Epoch 215/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0538 - acc: 0.9215 - val_loss: 0.0816 - val_acc: 0.8837\n",
      "Epoch 216/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0531 - acc: 0.9255 - val_loss: 0.0832 - val_acc: 0.8837\n",
      "Epoch 217/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0526 - acc: 0.9238 - val_loss: 0.0808 - val_acc: 0.8863\n",
      "Epoch 218/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0538 - acc: 0.9193 - val_loss: 0.0769 - val_acc: 0.8863\n",
      "Epoch 219/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0514 - acc: 0.9264 - val_loss: 0.0772 - val_acc: 0.8889\n",
      "765/765 [==============================] - 4s 5ms/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadik/.local/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3059 samples, validate on 765 samples\n",
      "Epoch 1/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.5461 - acc: 0.4989 - val_loss: 0.2681 - val_acc: 0.5033\n",
      "Epoch 2/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.2523 - acc: 0.5757 - val_loss: 0.2225 - val_acc: 0.6039\n",
      "Epoch 3/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2175 - acc: 0.6100 - val_loss: 0.2070 - val_acc: 0.6052\n",
      "Epoch 4/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.2094 - acc: 0.6156 - val_loss: 0.2054 - val_acc: 0.6105\n",
      "Epoch 5/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2060 - acc: 0.6169 - val_loss: 0.2031 - val_acc: 0.6013\n",
      "Epoch 6/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2029 - acc: 0.6208 - val_loss: 0.2022 - val_acc: 0.6157\n",
      "Epoch 7/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2009 - acc: 0.6355 - val_loss: 0.2046 - val_acc: 0.5948\n",
      "Epoch 8/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.2004 - acc: 0.6306 - val_loss: 0.1986 - val_acc: 0.6392\n",
      "Epoch 9/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1979 - acc: 0.6342 - val_loss: 0.2008 - val_acc: 0.6314\n",
      "Epoch 10/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1939 - acc: 0.6541 - val_loss: 0.1939 - val_acc: 0.6641\n",
      "Epoch 11/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1869 - acc: 0.6639 - val_loss: 0.1823 - val_acc: 0.6667\n",
      "Epoch 12/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1750 - acc: 0.7146 - val_loss: 0.1684 - val_acc: 0.7752\n",
      "Epoch 13/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1585 - acc: 0.7601 - val_loss: 0.1500 - val_acc: 0.7961\n",
      "Epoch 14/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1442 - acc: 0.7764 - val_loss: 0.1446 - val_acc: 0.7987\n",
      "Epoch 15/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1361 - acc: 0.7914 - val_loss: 0.1305 - val_acc: 0.8118\n",
      "Epoch 16/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.1293 - acc: 0.8045 - val_loss: 0.1338 - val_acc: 0.8196\n",
      "Epoch 17/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1286 - acc: 0.8058 - val_loss: 0.1265 - val_acc: 0.8052\n",
      "Epoch 18/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1241 - acc: 0.8101 - val_loss: 0.1251 - val_acc: 0.8078\n",
      "Epoch 19/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1237 - acc: 0.8042 - val_loss: 0.1428 - val_acc: 0.7660\n",
      "Epoch 20/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1200 - acc: 0.8166 - val_loss: 0.1280 - val_acc: 0.8301\n",
      "Epoch 21/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1231 - acc: 0.8114 - val_loss: 0.1246 - val_acc: 0.8039\n",
      "Epoch 22/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1200 - acc: 0.8228 - val_loss: 0.1218 - val_acc: 0.8092\n",
      "Epoch 23/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1191 - acc: 0.8212 - val_loss: 0.1172 - val_acc: 0.8209\n",
      "Epoch 24/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1166 - acc: 0.8258 - val_loss: 0.1153 - val_acc: 0.8275\n",
      "Epoch 25/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1181 - acc: 0.8195 - val_loss: 0.1189 - val_acc: 0.8353\n",
      "Epoch 26/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1161 - acc: 0.8258 - val_loss: 0.1165 - val_acc: 0.8366\n",
      "Epoch 27/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1144 - acc: 0.8245 - val_loss: 0.1139 - val_acc: 0.8261\n",
      "Epoch 28/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1121 - acc: 0.8346 - val_loss: 0.1149 - val_acc: 0.8405\n",
      "Epoch 29/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1132 - acc: 0.8316 - val_loss: 0.1140 - val_acc: 0.8353\n",
      "Epoch 30/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1105 - acc: 0.8356 - val_loss: 0.1118 - val_acc: 0.8405\n",
      "Epoch 31/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1097 - acc: 0.8385 - val_loss: 0.1133 - val_acc: 0.8288\n",
      "Epoch 32/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1096 - acc: 0.8375 - val_loss: 0.1111 - val_acc: 0.8444\n",
      "Epoch 33/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1080 - acc: 0.8421 - val_loss: 0.1226 - val_acc: 0.8183\n",
      "Epoch 34/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1085 - acc: 0.8372 - val_loss: 0.1095 - val_acc: 0.8366\n",
      "Epoch 35/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1090 - acc: 0.8398 - val_loss: 0.1110 - val_acc: 0.8327\n",
      "Epoch 36/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1057 - acc: 0.8444 - val_loss: 0.1086 - val_acc: 0.8392\n",
      "Epoch 37/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1036 - acc: 0.8441 - val_loss: 0.1096 - val_acc: 0.8340\n",
      "Epoch 38/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1047 - acc: 0.8450 - val_loss: 0.1077 - val_acc: 0.8510\n",
      "Epoch 39/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1034 - acc: 0.8450 - val_loss: 0.1072 - val_acc: 0.8431\n",
      "Epoch 40/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1027 - acc: 0.8434 - val_loss: 0.1091 - val_acc: 0.8353\n",
      "Epoch 41/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1012 - acc: 0.8513 - val_loss: 0.1034 - val_acc: 0.8471\n",
      "Epoch 42/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1006 - acc: 0.8516 - val_loss: 0.1047 - val_acc: 0.8444\n",
      "Epoch 43/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1023 - acc: 0.8493 - val_loss: 0.1048 - val_acc: 0.8497\n",
      "Epoch 44/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1003 - acc: 0.8500 - val_loss: 0.1052 - val_acc: 0.8444\n",
      "Epoch 45/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0980 - acc: 0.8552 - val_loss: 0.1004 - val_acc: 0.8510\n",
      "Epoch 46/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0961 - acc: 0.8604 - val_loss: 0.1011 - val_acc: 0.8536\n",
      "Epoch 47/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0965 - acc: 0.8571 - val_loss: 0.1037 - val_acc: 0.8458\n",
      "Epoch 48/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0959 - acc: 0.8591 - val_loss: 0.1018 - val_acc: 0.8536\n",
      "Epoch 49/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0925 - acc: 0.8620 - val_loss: 0.0993 - val_acc: 0.8614\n",
      "Epoch 50/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0942 - acc: 0.8607 - val_loss: 0.0979 - val_acc: 0.8601\n",
      "Epoch 51/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0971 - acc: 0.8545 - val_loss: 0.0972 - val_acc: 0.8614\n",
      "Epoch 52/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0929 - acc: 0.8578 - val_loss: 0.1135 - val_acc: 0.8366\n",
      "Epoch 53/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0937 - acc: 0.8647 - val_loss: 0.0961 - val_acc: 0.8693\n",
      "Epoch 54/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0921 - acc: 0.8637 - val_loss: 0.0962 - val_acc: 0.8667\n",
      "Epoch 55/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0919 - acc: 0.8653 - val_loss: 0.0947 - val_acc: 0.8706\n",
      "Epoch 56/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0897 - acc: 0.8679 - val_loss: 0.0960 - val_acc: 0.8627\n",
      "Epoch 57/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0904 - acc: 0.8647 - val_loss: 0.0941 - val_acc: 0.8680\n",
      "Epoch 58/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0909 - acc: 0.8647 - val_loss: 0.0997 - val_acc: 0.8536\n",
      "Epoch 59/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0892 - acc: 0.8683 - val_loss: 0.0956 - val_acc: 0.8641\n",
      "Epoch 60/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0876 - acc: 0.8705 - val_loss: 0.0994 - val_acc: 0.8562\n",
      "Epoch 61/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0884 - acc: 0.8705 - val_loss: 0.0955 - val_acc: 0.8732\n",
      "Epoch 62/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0883 - acc: 0.8669 - val_loss: 0.0968 - val_acc: 0.8575\n",
      "Epoch 63/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0868 - acc: 0.8692 - val_loss: 0.0953 - val_acc: 0.8641\n",
      "Epoch 64/600\n",
      "3059/3059 [==============================] - 38s 13ms/step - loss: 0.0869 - acc: 0.8748 - val_loss: 0.0935 - val_acc: 0.8641\n",
      "Epoch 65/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0866 - acc: 0.8738 - val_loss: 0.0929 - val_acc: 0.8732\n",
      "Epoch 66/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0871 - acc: 0.8712 - val_loss: 0.0917 - val_acc: 0.8745\n",
      "Epoch 67/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0878 - acc: 0.8686 - val_loss: 0.0934 - val_acc: 0.8627\n",
      "Epoch 68/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0870 - acc: 0.8735 - val_loss: 0.0948 - val_acc: 0.8614\n",
      "Epoch 69/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0846 - acc: 0.8764 - val_loss: 0.0917 - val_acc: 0.8654\n",
      "Epoch 70/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0841 - acc: 0.8748 - val_loss: 0.0953 - val_acc: 0.8588\n",
      "Epoch 71/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0864 - acc: 0.8715 - val_loss: 0.0939 - val_acc: 0.8654\n",
      "Epoch 72/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0869 - acc: 0.8702 - val_loss: 0.0928 - val_acc: 0.8732\n",
      "Epoch 73/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0836 - acc: 0.8784 - val_loss: 0.0933 - val_acc: 0.8654\n",
      "Epoch 74/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0842 - acc: 0.8768 - val_loss: 0.0911 - val_acc: 0.8680\n",
      "Epoch 75/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0823 - acc: 0.8777 - val_loss: 0.0941 - val_acc: 0.8667\n",
      "Epoch 76/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0817 - acc: 0.8804 - val_loss: 0.0986 - val_acc: 0.8601\n",
      "Epoch 77/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0828 - acc: 0.8761 - val_loss: 0.0913 - val_acc: 0.8667\n",
      "Epoch 78/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0825 - acc: 0.8797 - val_loss: 0.0887 - val_acc: 0.8732\n",
      "Epoch 79/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0838 - acc: 0.8764 - val_loss: 0.0910 - val_acc: 0.8719\n",
      "Epoch 80/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0832 - acc: 0.8794 - val_loss: 0.0892 - val_acc: 0.8758\n",
      "Epoch 81/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0820 - acc: 0.8790 - val_loss: 0.0886 - val_acc: 0.8719\n",
      "Epoch 82/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0820 - acc: 0.8804 - val_loss: 0.0917 - val_acc: 0.8693\n",
      "Epoch 83/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0792 - acc: 0.8833 - val_loss: 0.0878 - val_acc: 0.8745\n",
      "Epoch 84/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0794 - acc: 0.8797 - val_loss: 0.0881 - val_acc: 0.8706\n",
      "Epoch 85/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0813 - acc: 0.8836 - val_loss: 0.0884 - val_acc: 0.8667\n",
      "Epoch 86/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0780 - acc: 0.8892 - val_loss: 0.0918 - val_acc: 0.8719\n",
      "Epoch 87/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0828 - acc: 0.8768 - val_loss: 0.0900 - val_acc: 0.8706\n",
      "Epoch 88/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0763 - acc: 0.8859 - val_loss: 0.0887 - val_acc: 0.8758\n",
      "Epoch 89/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0791 - acc: 0.8875 - val_loss: 0.0883 - val_acc: 0.8745\n",
      "Epoch 90/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0772 - acc: 0.8879 - val_loss: 0.0924 - val_acc: 0.8758\n",
      "Epoch 91/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0800 - acc: 0.8790 - val_loss: 0.0893 - val_acc: 0.8719\n",
      "Epoch 92/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0789 - acc: 0.8859 - val_loss: 0.0916 - val_acc: 0.8667\n",
      "Epoch 93/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0787 - acc: 0.8859 - val_loss: 0.0867 - val_acc: 0.8771\n",
      "Epoch 94/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0795 - acc: 0.8800 - val_loss: 0.0892 - val_acc: 0.8732\n",
      "Epoch 95/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0776 - acc: 0.8833 - val_loss: 0.0867 - val_acc: 0.8732\n",
      "Epoch 96/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0770 - acc: 0.8836 - val_loss: 0.0857 - val_acc: 0.8771\n",
      "Epoch 97/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0759 - acc: 0.8872 - val_loss: 0.0853 - val_acc: 0.8784\n",
      "Epoch 98/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0762 - acc: 0.8820 - val_loss: 0.0871 - val_acc: 0.8771\n",
      "Epoch 99/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0785 - acc: 0.8817 - val_loss: 0.0934 - val_acc: 0.8575\n",
      "Epoch 100/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0770 - acc: 0.8849 - val_loss: 0.0880 - val_acc: 0.8758\n",
      "Epoch 101/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0733 - acc: 0.8974 - val_loss: 0.0854 - val_acc: 0.8745\n",
      "Epoch 102/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0755 - acc: 0.8879 - val_loss: 0.0903 - val_acc: 0.8680\n",
      "Epoch 103/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0771 - acc: 0.8833 - val_loss: 0.0843 - val_acc: 0.8771\n",
      "Epoch 104/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0748 - acc: 0.8908 - val_loss: 0.0848 - val_acc: 0.8745\n",
      "Epoch 105/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0771 - acc: 0.8830 - val_loss: 0.0844 - val_acc: 0.8758\n",
      "Epoch 106/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0742 - acc: 0.8934 - val_loss: 0.0855 - val_acc: 0.8732\n",
      "Epoch 107/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0738 - acc: 0.8911 - val_loss: 0.0853 - val_acc: 0.8758\n",
      "Epoch 108/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0723 - acc: 0.8957 - val_loss: 0.0848 - val_acc: 0.8758\n",
      "Epoch 109/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0730 - acc: 0.8892 - val_loss: 0.0873 - val_acc: 0.8732\n",
      "Epoch 110/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0736 - acc: 0.8928 - val_loss: 0.0919 - val_acc: 0.8641\n",
      "Epoch 111/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0719 - acc: 0.8921 - val_loss: 0.0869 - val_acc: 0.8732\n",
      "Epoch 112/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0717 - acc: 0.8938 - val_loss: 0.0869 - val_acc: 0.8719\n",
      "Epoch 113/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0717 - acc: 0.8974 - val_loss: 0.0830 - val_acc: 0.8771\n",
      "Epoch 114/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0709 - acc: 0.8941 - val_loss: 0.0833 - val_acc: 0.8771\n",
      "Epoch 115/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0714 - acc: 0.8970 - val_loss: 0.0836 - val_acc: 0.8771\n",
      "Epoch 116/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0703 - acc: 0.8970 - val_loss: 0.0835 - val_acc: 0.8732\n",
      "Epoch 117/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0713 - acc: 0.8908 - val_loss: 0.0835 - val_acc: 0.8771\n",
      "Epoch 118/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0725 - acc: 0.8931 - val_loss: 0.0834 - val_acc: 0.8758\n",
      "Epoch 119/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0695 - acc: 0.9026 - val_loss: 0.0812 - val_acc: 0.8771\n",
      "Epoch 120/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0698 - acc: 0.8951 - val_loss: 0.0848 - val_acc: 0.8706\n",
      "Epoch 121/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0717 - acc: 0.8941 - val_loss: 0.0835 - val_acc: 0.8693\n",
      "Epoch 122/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0708 - acc: 0.8944 - val_loss: 0.0817 - val_acc: 0.8745\n",
      "Epoch 123/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0690 - acc: 0.9000 - val_loss: 0.0820 - val_acc: 0.8758\n",
      "Epoch 124/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0718 - acc: 0.8964 - val_loss: 0.0830 - val_acc: 0.8693\n",
      "Epoch 125/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0715 - acc: 0.8970 - val_loss: 0.0815 - val_acc: 0.8693\n",
      "Epoch 126/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0705 - acc: 0.9003 - val_loss: 0.0822 - val_acc: 0.8797\n",
      "Epoch 127/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0691 - acc: 0.8964 - val_loss: 0.0857 - val_acc: 0.8706\n",
      "Epoch 128/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0710 - acc: 0.8964 - val_loss: 0.0848 - val_acc: 0.8771\n",
      "Epoch 129/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0719 - acc: 0.9009 - val_loss: 0.0826 - val_acc: 0.8810\n",
      "Epoch 130/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0694 - acc: 0.8983 - val_loss: 0.0807 - val_acc: 0.8706\n",
      "Epoch 131/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0671 - acc: 0.9003 - val_loss: 0.0829 - val_acc: 0.8810\n",
      "Epoch 132/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0721 - acc: 0.8974 - val_loss: 0.0817 - val_acc: 0.8771\n",
      "Epoch 133/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0677 - acc: 0.8996 - val_loss: 0.0818 - val_acc: 0.8745\n",
      "Epoch 134/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0698 - acc: 0.8970 - val_loss: 0.0803 - val_acc: 0.8758\n",
      "Epoch 135/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0681 - acc: 0.8980 - val_loss: 0.0825 - val_acc: 0.8810\n",
      "Epoch 136/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0662 - acc: 0.9049 - val_loss: 0.0797 - val_acc: 0.8745\n",
      "Epoch 137/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0689 - acc: 0.8974 - val_loss: 0.0810 - val_acc: 0.8758\n",
      "Epoch 138/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0663 - acc: 0.8993 - val_loss: 0.0826 - val_acc: 0.8771\n",
      "Epoch 139/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0690 - acc: 0.8967 - val_loss: 0.0810 - val_acc: 0.8771\n",
      "Epoch 140/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0679 - acc: 0.9036 - val_loss: 0.0802 - val_acc: 0.8758\n",
      "Epoch 141/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0669 - acc: 0.8996 - val_loss: 0.0796 - val_acc: 0.8784\n",
      "Epoch 142/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0661 - acc: 0.9052 - val_loss: 0.0796 - val_acc: 0.8745\n",
      "Epoch 143/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0682 - acc: 0.9029 - val_loss: 0.0798 - val_acc: 0.8784\n",
      "Epoch 144/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0665 - acc: 0.9032 - val_loss: 0.0796 - val_acc: 0.8784\n",
      "Epoch 145/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0685 - acc: 0.8980 - val_loss: 0.0781 - val_acc: 0.8758\n",
      "Epoch 146/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0660 - acc: 0.9039 - val_loss: 0.0788 - val_acc: 0.8824\n",
      "Epoch 147/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0648 - acc: 0.9032 - val_loss: 0.0860 - val_acc: 0.8771\n",
      "Epoch 148/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0650 - acc: 0.9068 - val_loss: 0.0782 - val_acc: 0.8784\n",
      "Epoch 149/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0638 - acc: 0.9101 - val_loss: 0.0830 - val_acc: 0.8797\n",
      "Epoch 150/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0644 - acc: 0.9032 - val_loss: 0.0810 - val_acc: 0.8810\n",
      "Epoch 151/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0653 - acc: 0.9049 - val_loss: 0.0816 - val_acc: 0.8824\n",
      "Epoch 152/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0661 - acc: 0.9055 - val_loss: 0.0791 - val_acc: 0.8810\n",
      "Epoch 153/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0647 - acc: 0.9065 - val_loss: 0.0806 - val_acc: 0.8797\n",
      "Epoch 154/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0648 - acc: 0.9029 - val_loss: 0.0776 - val_acc: 0.8810\n",
      "Epoch 155/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0653 - acc: 0.9013 - val_loss: 0.0779 - val_acc: 0.8784\n",
      "Epoch 156/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0652 - acc: 0.9016 - val_loss: 0.0792 - val_acc: 0.8837\n",
      "Epoch 157/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0628 - acc: 0.9065 - val_loss: 0.0838 - val_acc: 0.8771\n",
      "Epoch 158/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0654 - acc: 0.8977 - val_loss: 0.0790 - val_acc: 0.8810\n",
      "Epoch 159/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0654 - acc: 0.9019 - val_loss: 0.0791 - val_acc: 0.8850\n",
      "Epoch 160/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0632 - acc: 0.9052 - val_loss: 0.0794 - val_acc: 0.8810\n",
      "Epoch 161/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0640 - acc: 0.9029 - val_loss: 0.0792 - val_acc: 0.8771\n",
      "Epoch 162/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0623 - acc: 0.9091 - val_loss: 0.0808 - val_acc: 0.8771\n",
      "Epoch 163/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0676 - acc: 0.9003 - val_loss: 0.0763 - val_acc: 0.8824\n",
      "Epoch 164/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0627 - acc: 0.9094 - val_loss: 0.0792 - val_acc: 0.8784\n",
      "Epoch 165/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0625 - acc: 0.9085 - val_loss: 0.0779 - val_acc: 0.8797\n",
      "Epoch 166/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0639 - acc: 0.9026 - val_loss: 0.0784 - val_acc: 0.8824\n",
      "Epoch 167/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0645 - acc: 0.9075 - val_loss: 0.0760 - val_acc: 0.8824\n",
      "Epoch 168/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0620 - acc: 0.9108 - val_loss: 0.0762 - val_acc: 0.8837\n",
      "Epoch 169/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0615 - acc: 0.9121 - val_loss: 0.0764 - val_acc: 0.8824\n",
      "Epoch 170/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0607 - acc: 0.9134 - val_loss: 0.0767 - val_acc: 0.8837\n",
      "Epoch 171/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0610 - acc: 0.9085 - val_loss: 0.0805 - val_acc: 0.8810\n",
      "Epoch 172/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0605 - acc: 0.9094 - val_loss: 0.0757 - val_acc: 0.8824\n",
      "Epoch 173/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0611 - acc: 0.9124 - val_loss: 0.0797 - val_acc: 0.8797\n",
      "Epoch 174/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0603 - acc: 0.9153 - val_loss: 0.0763 - val_acc: 0.8889\n",
      "Epoch 175/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0597 - acc: 0.9124 - val_loss: 0.0792 - val_acc: 0.8797\n",
      "Epoch 176/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0615 - acc: 0.9117 - val_loss: 0.0760 - val_acc: 0.8876\n",
      "Epoch 177/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0613 - acc: 0.9117 - val_loss: 0.0825 - val_acc: 0.8784\n",
      "Epoch 178/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0611 - acc: 0.9117 - val_loss: 0.0791 - val_acc: 0.8784\n",
      "Epoch 179/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0601 - acc: 0.9127 - val_loss: 0.0771 - val_acc: 0.8784\n",
      "Epoch 180/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0594 - acc: 0.9166 - val_loss: 0.0799 - val_acc: 0.8797\n",
      "Epoch 181/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0598 - acc: 0.9104 - val_loss: 0.0767 - val_acc: 0.8876\n",
      "Epoch 182/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0599 - acc: 0.9140 - val_loss: 0.0758 - val_acc: 0.8863\n",
      "Epoch 183/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0587 - acc: 0.9150 - val_loss: 0.0808 - val_acc: 0.8850\n",
      "Epoch 184/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0606 - acc: 0.9101 - val_loss: 0.0847 - val_acc: 0.8771\n",
      "Epoch 185/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0592 - acc: 0.9114 - val_loss: 0.0781 - val_acc: 0.8837\n",
      "Epoch 186/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0598 - acc: 0.9147 - val_loss: 0.0766 - val_acc: 0.8850\n",
      "Epoch 187/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0584 - acc: 0.9144 - val_loss: 0.0761 - val_acc: 0.8810\n",
      "Epoch 188/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0607 - acc: 0.9121 - val_loss: 0.0750 - val_acc: 0.8876\n",
      "Epoch 189/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0580 - acc: 0.9176 - val_loss: 0.0745 - val_acc: 0.8863\n",
      "Epoch 190/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0569 - acc: 0.9153 - val_loss: 0.0756 - val_acc: 0.8850\n",
      "Epoch 191/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0561 - acc: 0.9215 - val_loss: 0.0760 - val_acc: 0.8902\n",
      "Epoch 192/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0606 - acc: 0.9121 - val_loss: 0.0769 - val_acc: 0.8810\n",
      "Epoch 193/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0576 - acc: 0.9173 - val_loss: 0.0760 - val_acc: 0.8889\n",
      "Epoch 194/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0575 - acc: 0.9114 - val_loss: 0.0755 - val_acc: 0.8902\n",
      "Epoch 195/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0593 - acc: 0.9144 - val_loss: 0.0784 - val_acc: 0.8837\n",
      "Epoch 196/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0579 - acc: 0.9147 - val_loss: 0.0844 - val_acc: 0.8732\n",
      "Epoch 197/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0566 - acc: 0.9193 - val_loss: 0.0762 - val_acc: 0.8876\n",
      "Epoch 198/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0562 - acc: 0.9215 - val_loss: 0.0757 - val_acc: 0.8915\n",
      "Epoch 199/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0573 - acc: 0.9179 - val_loss: 0.0784 - val_acc: 0.8810\n",
      "Epoch 200/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0569 - acc: 0.9183 - val_loss: 0.0774 - val_acc: 0.8837\n",
      "Epoch 201/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0584 - acc: 0.9140 - val_loss: 0.0764 - val_acc: 0.8863\n",
      "Epoch 202/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0569 - acc: 0.9173 - val_loss: 0.0764 - val_acc: 0.8863\n",
      "Epoch 203/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0581 - acc: 0.9173 - val_loss: 0.0754 - val_acc: 0.8889\n",
      "Epoch 204/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0565 - acc: 0.9193 - val_loss: 0.0735 - val_acc: 0.8915\n",
      "Epoch 205/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0565 - acc: 0.9160 - val_loss: 0.0763 - val_acc: 0.8915\n",
      "Epoch 206/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0549 - acc: 0.9196 - val_loss: 0.0770 - val_acc: 0.8902\n",
      "Epoch 207/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0564 - acc: 0.9212 - val_loss: 0.0741 - val_acc: 0.8863\n",
      "Epoch 208/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0564 - acc: 0.9193 - val_loss: 0.0754 - val_acc: 0.8863\n",
      "Epoch 209/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0567 - acc: 0.9170 - val_loss: 0.0739 - val_acc: 0.8889\n",
      "Epoch 210/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0543 - acc: 0.9242 - val_loss: 0.0737 - val_acc: 0.8850\n",
      "Epoch 211/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0563 - acc: 0.9160 - val_loss: 0.0830 - val_acc: 0.8745\n",
      "Epoch 212/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0558 - acc: 0.9206 - val_loss: 0.0741 - val_acc: 0.8863\n",
      "Epoch 213/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0540 - acc: 0.9196 - val_loss: 0.0758 - val_acc: 0.8863\n",
      "Epoch 214/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0545 - acc: 0.9186 - val_loss: 0.0741 - val_acc: 0.8915\n",
      "Epoch 215/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0568 - acc: 0.9163 - val_loss: 0.0731 - val_acc: 0.8941\n",
      "Epoch 216/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0553 - acc: 0.9206 - val_loss: 0.0744 - val_acc: 0.8863\n",
      "Epoch 217/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0557 - acc: 0.9189 - val_loss: 0.0731 - val_acc: 0.8863\n",
      "Epoch 218/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0547 - acc: 0.9271 - val_loss: 0.0765 - val_acc: 0.8876\n",
      "Epoch 219/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0536 - acc: 0.9215 - val_loss: 0.0755 - val_acc: 0.8863\n",
      "Epoch 220/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0551 - acc: 0.9232 - val_loss: 0.0834 - val_acc: 0.8719\n",
      "Epoch 221/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0560 - acc: 0.9183 - val_loss: 0.0754 - val_acc: 0.8902\n",
      "Epoch 222/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0546 - acc: 0.9212 - val_loss: 0.0735 - val_acc: 0.8889\n",
      "Epoch 223/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0542 - acc: 0.9209 - val_loss: 0.0772 - val_acc: 0.8889\n",
      "Epoch 224/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0550 - acc: 0.9219 - val_loss: 0.0743 - val_acc: 0.8902\n",
      "Epoch 225/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0555 - acc: 0.9183 - val_loss: 0.0752 - val_acc: 0.8915\n",
      "Epoch 226/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0543 - acc: 0.9225 - val_loss: 0.0760 - val_acc: 0.8889\n",
      "Epoch 227/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0528 - acc: 0.9264 - val_loss: 0.0753 - val_acc: 0.8915\n",
      "Epoch 228/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0532 - acc: 0.9245 - val_loss: 0.0742 - val_acc: 0.8876\n",
      "Epoch 229/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0529 - acc: 0.9222 - val_loss: 0.0737 - val_acc: 0.8902\n",
      "Epoch 230/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0540 - acc: 0.9199 - val_loss: 0.0744 - val_acc: 0.8915\n",
      "Epoch 231/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0525 - acc: 0.9258 - val_loss: 0.0739 - val_acc: 0.8915\n",
      "Epoch 232/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0544 - acc: 0.9212 - val_loss: 0.0745 - val_acc: 0.8941\n",
      "Epoch 233/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0562 - acc: 0.9209 - val_loss: 0.0737 - val_acc: 0.8928\n",
      "Epoch 234/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0556 - acc: 0.9235 - val_loss: 0.0726 - val_acc: 0.8941\n",
      "Epoch 235/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0537 - acc: 0.9196 - val_loss: 0.0725 - val_acc: 0.8889\n",
      "Epoch 236/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0528 - acc: 0.9258 - val_loss: 0.0738 - val_acc: 0.8902\n",
      "Epoch 237/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0543 - acc: 0.9209 - val_loss: 0.0744 - val_acc: 0.8915\n",
      "Epoch 238/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0533 - acc: 0.9225 - val_loss: 0.0731 - val_acc: 0.8889\n",
      "Epoch 239/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0535 - acc: 0.9238 - val_loss: 0.0741 - val_acc: 0.8902\n",
      "Epoch 240/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0521 - acc: 0.9219 - val_loss: 0.0745 - val_acc: 0.8928\n",
      "Epoch 241/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0520 - acc: 0.9238 - val_loss: 0.0747 - val_acc: 0.8889\n",
      "Epoch 242/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0531 - acc: 0.9235 - val_loss: 0.0738 - val_acc: 0.8915\n",
      "Epoch 243/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0501 - acc: 0.9261 - val_loss: 0.0719 - val_acc: 0.8902\n",
      "Epoch 244/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0509 - acc: 0.9294 - val_loss: 0.0756 - val_acc: 0.8876\n",
      "Epoch 245/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0518 - acc: 0.9248 - val_loss: 0.0721 - val_acc: 0.8928\n",
      "Epoch 246/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0507 - acc: 0.9314 - val_loss: 0.0770 - val_acc: 0.8889\n",
      "Epoch 247/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0499 - acc: 0.9264 - val_loss: 0.0754 - val_acc: 0.8902\n",
      "Epoch 248/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0526 - acc: 0.9238 - val_loss: 0.0744 - val_acc: 0.8928\n",
      "Epoch 249/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0521 - acc: 0.9232 - val_loss: 0.0726 - val_acc: 0.8941\n",
      "Epoch 250/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0527 - acc: 0.9278 - val_loss: 0.0738 - val_acc: 0.8954\n",
      "Epoch 251/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0522 - acc: 0.9264 - val_loss: 0.0730 - val_acc: 0.8928\n",
      "Epoch 252/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0523 - acc: 0.9251 - val_loss: 0.0724 - val_acc: 0.8954\n",
      "Epoch 253/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0503 - acc: 0.9287 - val_loss: 0.0741 - val_acc: 0.8941\n",
      "Epoch 254/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0510 - acc: 0.9255 - val_loss: 0.0750 - val_acc: 0.8902\n",
      "Epoch 255/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0504 - acc: 0.9281 - val_loss: 0.0748 - val_acc: 0.8954\n",
      "Epoch 256/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0490 - acc: 0.9310 - val_loss: 0.0739 - val_acc: 0.8980\n",
      "Epoch 257/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0521 - acc: 0.9245 - val_loss: 0.0751 - val_acc: 0.8876\n",
      "Epoch 258/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0494 - acc: 0.9287 - val_loss: 0.0748 - val_acc: 0.8954\n",
      "Epoch 259/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0483 - acc: 0.9330 - val_loss: 0.0738 - val_acc: 0.8954\n",
      "Epoch 260/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0503 - acc: 0.9300 - val_loss: 0.0732 - val_acc: 0.8954\n",
      "Epoch 261/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0491 - acc: 0.9297 - val_loss: 0.0727 - val_acc: 0.8967\n",
      "Epoch 262/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0495 - acc: 0.9255 - val_loss: 0.0731 - val_acc: 0.8967\n",
      "Epoch 263/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0482 - acc: 0.9317 - val_loss: 0.0746 - val_acc: 0.8889\n",
      "Epoch 264/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0505 - acc: 0.9281 - val_loss: 0.0757 - val_acc: 0.8915\n",
      "Epoch 265/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0481 - acc: 0.9281 - val_loss: 0.0726 - val_acc: 0.8941\n",
      "Epoch 266/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0503 - acc: 0.9248 - val_loss: 0.0743 - val_acc: 0.8941\n",
      "Epoch 267/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0516 - acc: 0.9271 - val_loss: 0.0708 - val_acc: 0.8993\n",
      "Epoch 268/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0490 - acc: 0.9314 - val_loss: 0.0856 - val_acc: 0.8784\n",
      "Epoch 269/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0494 - acc: 0.9304 - val_loss: 0.0714 - val_acc: 0.8967\n",
      "Epoch 270/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0454 - acc: 0.9363 - val_loss: 0.0746 - val_acc: 0.8889\n",
      "Epoch 271/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0494 - acc: 0.9323 - val_loss: 0.0759 - val_acc: 0.8928\n",
      "Epoch 272/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0491 - acc: 0.9291 - val_loss: 0.0727 - val_acc: 0.8967\n",
      "Epoch 273/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0494 - acc: 0.9281 - val_loss: 0.0815 - val_acc: 0.8797\n",
      "Epoch 274/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0491 - acc: 0.9284 - val_loss: 0.0717 - val_acc: 0.8980\n",
      "Epoch 275/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0471 - acc: 0.9336 - val_loss: 0.0732 - val_acc: 0.8967\n",
      "Epoch 276/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0488 - acc: 0.9284 - val_loss: 0.0736 - val_acc: 0.8954\n",
      "Epoch 277/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0474 - acc: 0.9314 - val_loss: 0.0730 - val_acc: 0.8863\n",
      "Epoch 278/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0497 - acc: 0.9278 - val_loss: 0.0720 - val_acc: 0.8993\n",
      "Epoch 279/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0464 - acc: 0.9310 - val_loss: 0.0727 - val_acc: 0.8967\n",
      "Epoch 280/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0480 - acc: 0.9327 - val_loss: 0.0728 - val_acc: 0.8902\n",
      "Epoch 281/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0488 - acc: 0.9271 - val_loss: 0.0718 - val_acc: 0.8980\n",
      "Epoch 282/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0465 - acc: 0.9327 - val_loss: 0.0712 - val_acc: 0.8980\n",
      "Epoch 283/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0482 - acc: 0.9310 - val_loss: 0.0743 - val_acc: 0.8941\n",
      "Epoch 284/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0468 - acc: 0.9336 - val_loss: 0.0729 - val_acc: 0.8993\n",
      "Epoch 285/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0446 - acc: 0.9340 - val_loss: 0.0713 - val_acc: 0.8928\n",
      "Epoch 286/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0498 - acc: 0.9294 - val_loss: 0.0737 - val_acc: 0.8967\n",
      "Epoch 287/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0484 - acc: 0.9304 - val_loss: 0.0732 - val_acc: 0.9046\n",
      "Epoch 288/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0463 - acc: 0.9307 - val_loss: 0.0751 - val_acc: 0.8928\n",
      "Epoch 289/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0467 - acc: 0.9320 - val_loss: 0.0711 - val_acc: 0.8980\n",
      "Epoch 290/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0467 - acc: 0.9323 - val_loss: 0.0742 - val_acc: 0.8980\n",
      "Epoch 291/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0455 - acc: 0.9314 - val_loss: 0.0720 - val_acc: 0.9007\n",
      "Epoch 292/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0478 - acc: 0.9310 - val_loss: 0.0708 - val_acc: 0.8941\n",
      "765/765 [==============================] - 4s 6ms/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadik/.local/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3059 samples, validate on 765 samples\n",
      "Epoch 1/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.5333 - acc: 0.5842 - val_loss: 0.2696 - val_acc: 0.6013\n",
      "Epoch 2/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.2487 - acc: 0.6087 - val_loss: 0.2176 - val_acc: 0.6000\n",
      "Epoch 3/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.2151 - acc: 0.6028 - val_loss: 0.2053 - val_acc: 0.6131\n",
      "Epoch 4/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2069 - acc: 0.6136 - val_loss: 0.2023 - val_acc: 0.6118\n",
      "Epoch 5/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2051 - acc: 0.6146 - val_loss: 0.2065 - val_acc: 0.6105\n",
      "Epoch 6/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2031 - acc: 0.6241 - val_loss: 0.2001 - val_acc: 0.6209\n",
      "Epoch 7/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2006 - acc: 0.6241 - val_loss: 0.1958 - val_acc: 0.6314\n",
      "Epoch 8/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1991 - acc: 0.6335 - val_loss: 0.1976 - val_acc: 0.6052\n",
      "Epoch 9/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1938 - acc: 0.6404 - val_loss: 0.1906 - val_acc: 0.6850\n",
      "Epoch 10/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1870 - acc: 0.6718 - val_loss: 0.1796 - val_acc: 0.7281\n",
      "Epoch 11/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1750 - acc: 0.7077 - val_loss: 0.1641 - val_acc: 0.7843\n",
      "Epoch 12/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1604 - acc: 0.7594 - val_loss: 0.1484 - val_acc: 0.8026\n",
      "Epoch 13/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1478 - acc: 0.7712 - val_loss: 0.1427 - val_acc: 0.7817\n",
      "Epoch 14/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1394 - acc: 0.7842 - val_loss: 0.1264 - val_acc: 0.8209\n",
      "Epoch 15/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1353 - acc: 0.7878 - val_loss: 0.1226 - val_acc: 0.8209\n",
      "Epoch 16/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1314 - acc: 0.7986 - val_loss: 0.1209 - val_acc: 0.8222\n",
      "Epoch 17/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1277 - acc: 0.8120 - val_loss: 0.1168 - val_acc: 0.8222\n",
      "Epoch 18/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1291 - acc: 0.8019 - val_loss: 0.1190 - val_acc: 0.8209\n",
      "Epoch 19/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1245 - acc: 0.8117 - val_loss: 0.1232 - val_acc: 0.8144\n",
      "Epoch 20/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1258 - acc: 0.8065 - val_loss: 0.1134 - val_acc: 0.8222\n",
      "Epoch 21/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1234 - acc: 0.8143 - val_loss: 0.1131 - val_acc: 0.8288\n",
      "Epoch 22/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1209 - acc: 0.8117 - val_loss: 0.1159 - val_acc: 0.8275\n",
      "Epoch 23/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1194 - acc: 0.8169 - val_loss: 0.1134 - val_acc: 0.8353\n",
      "Epoch 24/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1196 - acc: 0.8156 - val_loss: 0.1106 - val_acc: 0.8366\n",
      "Epoch 25/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1154 - acc: 0.8251 - val_loss: 0.1135 - val_acc: 0.8379\n",
      "Epoch 26/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1137 - acc: 0.8294 - val_loss: 0.1071 - val_acc: 0.8484\n",
      "Epoch 27/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1163 - acc: 0.8251 - val_loss: 0.1100 - val_acc: 0.8340\n",
      "Epoch 28/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1116 - acc: 0.8365 - val_loss: 0.1050 - val_acc: 0.8497\n",
      "Epoch 29/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1099 - acc: 0.8375 - val_loss: 0.1032 - val_acc: 0.8562\n",
      "Epoch 30/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1124 - acc: 0.8290 - val_loss: 0.1057 - val_acc: 0.8484\n",
      "Epoch 31/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1107 - acc: 0.8274 - val_loss: 0.1060 - val_acc: 0.8340\n",
      "Epoch 32/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1102 - acc: 0.8349 - val_loss: 0.1048 - val_acc: 0.8510\n",
      "Epoch 33/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1085 - acc: 0.8352 - val_loss: 0.1016 - val_acc: 0.8627\n",
      "Epoch 34/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1082 - acc: 0.8307 - val_loss: 0.1043 - val_acc: 0.8471\n",
      "Epoch 35/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1066 - acc: 0.8290 - val_loss: 0.1042 - val_acc: 0.8549\n",
      "Epoch 36/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1065 - acc: 0.8411 - val_loss: 0.1054 - val_acc: 0.8327\n",
      "Epoch 37/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1040 - acc: 0.8408 - val_loss: 0.1003 - val_acc: 0.8601\n",
      "Epoch 38/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1048 - acc: 0.8411 - val_loss: 0.1004 - val_acc: 0.8641\n",
      "Epoch 39/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1036 - acc: 0.8431 - val_loss: 0.0991 - val_acc: 0.8667\n",
      "Epoch 40/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0992 - acc: 0.8473 - val_loss: 0.1014 - val_acc: 0.8418\n",
      "Epoch 41/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1020 - acc: 0.8486 - val_loss: 0.0978 - val_acc: 0.8654\n",
      "Epoch 42/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1018 - acc: 0.8431 - val_loss: 0.0975 - val_acc: 0.8654\n",
      "Epoch 43/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0994 - acc: 0.8529 - val_loss: 0.0964 - val_acc: 0.8627\n",
      "Epoch 44/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0975 - acc: 0.8585 - val_loss: 0.0984 - val_acc: 0.8588\n",
      "Epoch 45/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0985 - acc: 0.8522 - val_loss: 0.0988 - val_acc: 0.8601\n",
      "Epoch 46/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0983 - acc: 0.8571 - val_loss: 0.0969 - val_acc: 0.8627\n",
      "Epoch 47/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0978 - acc: 0.8562 - val_loss: 0.0979 - val_acc: 0.8601\n",
      "Epoch 48/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0945 - acc: 0.8611 - val_loss: 0.0953 - val_acc: 0.8667\n",
      "Epoch 49/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0916 - acc: 0.8598 - val_loss: 0.0952 - val_acc: 0.8654\n",
      "Epoch 50/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0926 - acc: 0.8617 - val_loss: 0.0939 - val_acc: 0.8627\n",
      "Epoch 51/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0932 - acc: 0.8620 - val_loss: 0.0944 - val_acc: 0.8575\n",
      "Epoch 52/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0922 - acc: 0.8594 - val_loss: 0.0966 - val_acc: 0.8614\n",
      "Epoch 53/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0902 - acc: 0.8614 - val_loss: 0.0935 - val_acc: 0.8654\n",
      "Epoch 54/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0913 - acc: 0.8598 - val_loss: 0.0919 - val_acc: 0.8667\n",
      "Epoch 55/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0927 - acc: 0.8666 - val_loss: 0.0915 - val_acc: 0.8614\n",
      "Epoch 56/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0899 - acc: 0.8676 - val_loss: 0.0937 - val_acc: 0.8614\n",
      "Epoch 57/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0914 - acc: 0.8614 - val_loss: 0.0910 - val_acc: 0.8614\n",
      "Epoch 58/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0891 - acc: 0.8673 - val_loss: 0.0939 - val_acc: 0.8614\n",
      "Epoch 59/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0894 - acc: 0.8696 - val_loss: 0.0967 - val_acc: 0.8641\n",
      "Epoch 60/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0896 - acc: 0.8702 - val_loss: 0.0908 - val_acc: 0.8641\n",
      "Epoch 61/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0903 - acc: 0.8653 - val_loss: 0.0891 - val_acc: 0.8654\n",
      "Epoch 62/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0881 - acc: 0.8696 - val_loss: 0.0893 - val_acc: 0.8641\n",
      "Epoch 63/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0872 - acc: 0.8679 - val_loss: 0.0925 - val_acc: 0.8654\n",
      "Epoch 64/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0884 - acc: 0.8722 - val_loss: 0.0902 - val_acc: 0.8601\n",
      "Epoch 65/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0869 - acc: 0.8751 - val_loss: 0.0915 - val_acc: 0.8575\n",
      "Epoch 66/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0880 - acc: 0.8702 - val_loss: 0.0908 - val_acc: 0.8706\n",
      "Epoch 67/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0857 - acc: 0.8738 - val_loss: 0.0905 - val_acc: 0.8706\n",
      "Epoch 68/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0869 - acc: 0.8719 - val_loss: 0.0903 - val_acc: 0.8641\n",
      "Epoch 69/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0841 - acc: 0.8758 - val_loss: 0.0910 - val_acc: 0.8641\n",
      "Epoch 70/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0827 - acc: 0.8820 - val_loss: 0.0874 - val_acc: 0.8719\n",
      "Epoch 71/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0849 - acc: 0.8804 - val_loss: 0.0904 - val_acc: 0.8588\n",
      "Epoch 72/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0867 - acc: 0.8745 - val_loss: 0.0867 - val_acc: 0.8706\n",
      "Epoch 73/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0835 - acc: 0.8810 - val_loss: 0.0879 - val_acc: 0.8719\n",
      "Epoch 74/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0818 - acc: 0.8804 - val_loss: 0.0882 - val_acc: 0.8667\n",
      "Epoch 75/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0826 - acc: 0.8774 - val_loss: 0.0877 - val_acc: 0.8706\n",
      "Epoch 76/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0814 - acc: 0.8823 - val_loss: 0.0864 - val_acc: 0.8680\n",
      "Epoch 77/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0817 - acc: 0.8768 - val_loss: 0.0855 - val_acc: 0.8745\n",
      "Epoch 78/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0827 - acc: 0.8787 - val_loss: 0.0877 - val_acc: 0.8693\n",
      "Epoch 79/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0799 - acc: 0.8823 - val_loss: 0.0875 - val_acc: 0.8758\n",
      "Epoch 80/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0803 - acc: 0.8853 - val_loss: 0.0884 - val_acc: 0.8667\n",
      "Epoch 81/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0802 - acc: 0.8862 - val_loss: 0.0859 - val_acc: 0.8719\n",
      "Epoch 82/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0818 - acc: 0.8817 - val_loss: 0.0862 - val_acc: 0.8732\n",
      "Epoch 83/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0805 - acc: 0.8810 - val_loss: 0.0879 - val_acc: 0.8641\n",
      "Epoch 84/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0818 - acc: 0.8787 - val_loss: 0.0870 - val_acc: 0.8797\n",
      "Epoch 85/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0791 - acc: 0.8817 - val_loss: 0.0853 - val_acc: 0.8784\n",
      "Epoch 86/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0813 - acc: 0.8768 - val_loss: 0.0840 - val_acc: 0.8797\n",
      "Epoch 87/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0806 - acc: 0.8813 - val_loss: 0.0841 - val_acc: 0.8758\n",
      "Epoch 88/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0783 - acc: 0.8889 - val_loss: 0.0847 - val_acc: 0.8732\n",
      "Epoch 89/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0788 - acc: 0.8882 - val_loss: 0.0857 - val_acc: 0.8771\n",
      "Epoch 90/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0793 - acc: 0.8872 - val_loss: 0.0857 - val_acc: 0.8758\n",
      "Epoch 91/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0766 - acc: 0.8859 - val_loss: 0.0855 - val_acc: 0.8797\n",
      "Epoch 92/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0777 - acc: 0.8895 - val_loss: 0.0827 - val_acc: 0.8810\n",
      "Epoch 93/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0781 - acc: 0.8892 - val_loss: 0.0832 - val_acc: 0.8810\n",
      "Epoch 94/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0761 - acc: 0.8915 - val_loss: 0.0827 - val_acc: 0.8810\n",
      "Epoch 95/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0761 - acc: 0.8889 - val_loss: 0.0840 - val_acc: 0.8810\n",
      "Epoch 96/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0783 - acc: 0.8820 - val_loss: 0.0867 - val_acc: 0.8641\n",
      "Epoch 97/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0753 - acc: 0.8879 - val_loss: 0.0834 - val_acc: 0.8824\n",
      "Epoch 98/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0746 - acc: 0.8970 - val_loss: 0.0841 - val_acc: 0.8824\n",
      "Epoch 99/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0739 - acc: 0.8954 - val_loss: 0.0821 - val_acc: 0.8797\n",
      "Epoch 100/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0747 - acc: 0.8941 - val_loss: 0.0838 - val_acc: 0.8706\n",
      "Epoch 101/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0754 - acc: 0.8879 - val_loss: 0.0826 - val_acc: 0.8797\n",
      "Epoch 102/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0733 - acc: 0.8934 - val_loss: 0.0852 - val_acc: 0.8706\n",
      "Epoch 103/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0724 - acc: 0.8938 - val_loss: 0.0818 - val_acc: 0.8810\n",
      "Epoch 104/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0754 - acc: 0.8895 - val_loss: 0.0833 - val_acc: 0.8837\n",
      "Epoch 105/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0732 - acc: 0.8918 - val_loss: 0.0817 - val_acc: 0.8837\n",
      "Epoch 106/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0751 - acc: 0.8908 - val_loss: 0.0815 - val_acc: 0.8837\n",
      "Epoch 107/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0728 - acc: 0.8928 - val_loss: 0.0810 - val_acc: 0.8863\n",
      "Epoch 108/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0726 - acc: 0.8954 - val_loss: 0.0823 - val_acc: 0.8784\n",
      "Epoch 109/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0736 - acc: 0.8902 - val_loss: 0.0811 - val_acc: 0.8863\n",
      "Epoch 110/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0703 - acc: 0.8996 - val_loss: 0.0811 - val_acc: 0.8837\n",
      "Epoch 111/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0715 - acc: 0.9003 - val_loss: 0.0829 - val_acc: 0.8850\n",
      "Epoch 112/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0742 - acc: 0.8947 - val_loss: 0.0820 - val_acc: 0.8850\n",
      "Epoch 113/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0725 - acc: 0.8934 - val_loss: 0.0818 - val_acc: 0.8784\n",
      "Epoch 114/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0724 - acc: 0.8921 - val_loss: 0.0795 - val_acc: 0.8850\n",
      "Epoch 115/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0711 - acc: 0.8964 - val_loss: 0.0894 - val_acc: 0.8810\n",
      "Epoch 116/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0727 - acc: 0.8918 - val_loss: 0.0843 - val_acc: 0.8706\n",
      "Epoch 117/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0719 - acc: 0.8951 - val_loss: 0.0805 - val_acc: 0.8837\n",
      "Epoch 118/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0721 - acc: 0.8954 - val_loss: 0.0814 - val_acc: 0.8771\n",
      "Epoch 119/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0706 - acc: 0.8941 - val_loss: 0.0831 - val_acc: 0.8863\n",
      "Epoch 120/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0704 - acc: 0.8931 - val_loss: 0.0858 - val_acc: 0.8837\n",
      "Epoch 121/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0708 - acc: 0.8967 - val_loss: 0.0793 - val_acc: 0.8850\n",
      "Epoch 122/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0676 - acc: 0.9059 - val_loss: 0.0802 - val_acc: 0.8850\n",
      "Epoch 123/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0695 - acc: 0.8983 - val_loss: 0.0792 - val_acc: 0.8876\n",
      "Epoch 124/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0703 - acc: 0.8970 - val_loss: 0.0801 - val_acc: 0.8876\n",
      "Epoch 125/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0716 - acc: 0.8898 - val_loss: 0.0783 - val_acc: 0.8810\n",
      "Epoch 126/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0687 - acc: 0.9019 - val_loss: 0.0806 - val_acc: 0.8797\n",
      "Epoch 127/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0680 - acc: 0.9013 - val_loss: 0.0827 - val_acc: 0.8758\n",
      "Epoch 128/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0693 - acc: 0.9006 - val_loss: 0.0790 - val_acc: 0.8850\n",
      "Epoch 129/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0680 - acc: 0.9019 - val_loss: 0.0788 - val_acc: 0.8824\n",
      "Epoch 130/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0681 - acc: 0.9006 - val_loss: 0.0800 - val_acc: 0.8863\n",
      "Epoch 131/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0674 - acc: 0.9029 - val_loss: 0.0799 - val_acc: 0.8850\n",
      "Epoch 132/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0663 - acc: 0.9072 - val_loss: 0.0805 - val_acc: 0.8876\n",
      "Epoch 133/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0670 - acc: 0.9036 - val_loss: 0.0802 - val_acc: 0.8824\n",
      "Epoch 134/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0677 - acc: 0.9013 - val_loss: 0.0771 - val_acc: 0.8876\n",
      "Epoch 135/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0670 - acc: 0.9042 - val_loss: 0.0807 - val_acc: 0.8850\n",
      "Epoch 136/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0650 - acc: 0.9039 - val_loss: 0.0822 - val_acc: 0.8850\n",
      "Epoch 137/600\n",
      "3059/3059 [==============================] - 53s 17ms/step - loss: 0.0685 - acc: 0.9006 - val_loss: 0.0769 - val_acc: 0.8876\n",
      "Epoch 138/600\n",
      "3059/3059 [==============================] - 53s 17ms/step - loss: 0.0666 - acc: 0.9029 - val_loss: 0.0778 - val_acc: 0.8863\n",
      "Epoch 139/600\n",
      "3059/3059 [==============================] - 55s 18ms/step - loss: 0.0675 - acc: 0.9039 - val_loss: 0.0804 - val_acc: 0.8876\n",
      "Epoch 140/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0659 - acc: 0.9039 - val_loss: 0.0768 - val_acc: 0.8863\n",
      "Epoch 141/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0655 - acc: 0.9055 - val_loss: 0.0765 - val_acc: 0.8889\n",
      "Epoch 142/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0649 - acc: 0.9065 - val_loss: 0.0768 - val_acc: 0.8876\n",
      "Epoch 143/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0650 - acc: 0.9032 - val_loss: 0.0773 - val_acc: 0.8902\n",
      "Epoch 144/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0667 - acc: 0.8993 - val_loss: 0.0765 - val_acc: 0.8876\n",
      "Epoch 145/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0648 - acc: 0.9026 - val_loss: 0.0773 - val_acc: 0.8915\n",
      "Epoch 146/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0652 - acc: 0.9032 - val_loss: 0.0778 - val_acc: 0.8902\n",
      "Epoch 147/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0658 - acc: 0.9036 - val_loss: 0.0781 - val_acc: 0.8876\n",
      "Epoch 148/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0628 - acc: 0.9101 - val_loss: 0.0785 - val_acc: 0.8889\n",
      "Epoch 149/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0652 - acc: 0.9059 - val_loss: 0.0761 - val_acc: 0.8889\n",
      "Epoch 150/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0629 - acc: 0.9098 - val_loss: 0.0793 - val_acc: 0.8837\n",
      "Epoch 151/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0620 - acc: 0.9094 - val_loss: 0.0754 - val_acc: 0.8889\n",
      "Epoch 152/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0652 - acc: 0.9049 - val_loss: 0.0770 - val_acc: 0.8863\n",
      "Epoch 153/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0644 - acc: 0.9059 - val_loss: 0.0761 - val_acc: 0.8928\n",
      "Epoch 154/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0647 - acc: 0.9088 - val_loss: 0.0803 - val_acc: 0.8915\n",
      "Epoch 155/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0615 - acc: 0.9144 - val_loss: 0.0786 - val_acc: 0.8889\n",
      "Epoch 156/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0638 - acc: 0.9108 - val_loss: 0.0751 - val_acc: 0.8876\n",
      "Epoch 157/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0634 - acc: 0.9023 - val_loss: 0.0807 - val_acc: 0.8706\n",
      "Epoch 158/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0632 - acc: 0.9081 - val_loss: 0.0760 - val_acc: 0.8902\n",
      "Epoch 159/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0633 - acc: 0.9124 - val_loss: 0.0801 - val_acc: 0.8876\n",
      "Epoch 160/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0630 - acc: 0.9094 - val_loss: 0.0746 - val_acc: 0.8889\n",
      "Epoch 161/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0622 - acc: 0.9075 - val_loss: 0.0756 - val_acc: 0.8915\n",
      "Epoch 162/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0616 - acc: 0.9075 - val_loss: 0.0778 - val_acc: 0.8915\n",
      "Epoch 163/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0597 - acc: 0.9150 - val_loss: 0.0756 - val_acc: 0.8915\n",
      "Epoch 164/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0619 - acc: 0.9101 - val_loss: 0.0762 - val_acc: 0.8928\n",
      "Epoch 165/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0617 - acc: 0.9111 - val_loss: 0.0774 - val_acc: 0.8902\n",
      "Epoch 166/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0608 - acc: 0.9137 - val_loss: 0.0737 - val_acc: 0.8915\n",
      "Epoch 167/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0603 - acc: 0.9144 - val_loss: 0.0790 - val_acc: 0.8889\n",
      "Epoch 168/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0627 - acc: 0.9108 - val_loss: 0.0745 - val_acc: 0.8967\n",
      "Epoch 169/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0602 - acc: 0.9104 - val_loss: 0.0800 - val_acc: 0.8797\n",
      "Epoch 170/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0602 - acc: 0.9124 - val_loss: 0.0758 - val_acc: 0.8928\n",
      "Epoch 171/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0602 - acc: 0.9114 - val_loss: 0.0769 - val_acc: 0.8915\n",
      "Epoch 172/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0588 - acc: 0.9160 - val_loss: 0.0765 - val_acc: 0.8915\n",
      "Epoch 173/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0603 - acc: 0.9137 - val_loss: 0.0747 - val_acc: 0.8954\n",
      "Epoch 174/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0603 - acc: 0.9124 - val_loss: 0.0756 - val_acc: 0.8902\n",
      "Epoch 175/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0590 - acc: 0.9117 - val_loss: 0.0751 - val_acc: 0.8941\n",
      "Epoch 176/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0587 - acc: 0.9157 - val_loss: 0.0754 - val_acc: 0.8954\n",
      "Epoch 177/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0599 - acc: 0.9127 - val_loss: 0.0747 - val_acc: 0.8928\n",
      "Epoch 178/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0573 - acc: 0.9183 - val_loss: 0.0781 - val_acc: 0.8915\n",
      "Epoch 179/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0604 - acc: 0.9104 - val_loss: 0.0764 - val_acc: 0.8902\n",
      "Epoch 180/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0576 - acc: 0.9170 - val_loss: 0.0754 - val_acc: 0.8915\n",
      "Epoch 181/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0583 - acc: 0.9134 - val_loss: 0.0750 - val_acc: 0.8915\n",
      "Epoch 182/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0566 - acc: 0.9160 - val_loss: 0.0756 - val_acc: 0.8915\n",
      "Epoch 183/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0582 - acc: 0.9153 - val_loss: 0.0739 - val_acc: 0.8941\n",
      "Epoch 184/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0548 - acc: 0.9232 - val_loss: 0.0765 - val_acc: 0.8954\n",
      "Epoch 185/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0584 - acc: 0.9137 - val_loss: 0.0734 - val_acc: 0.8928\n",
      "Epoch 186/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0579 - acc: 0.9150 - val_loss: 0.0758 - val_acc: 0.8954\n",
      "Epoch 187/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0588 - acc: 0.9144 - val_loss: 0.0738 - val_acc: 0.8941\n",
      "Epoch 188/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0584 - acc: 0.9130 - val_loss: 0.0738 - val_acc: 0.8928\n",
      "Epoch 189/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0573 - acc: 0.9157 - val_loss: 0.0758 - val_acc: 0.8967\n",
      "Epoch 190/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0590 - acc: 0.9157 - val_loss: 0.0745 - val_acc: 0.8954\n",
      "Epoch 191/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0568 - acc: 0.9153 - val_loss: 0.0749 - val_acc: 0.8967\n",
      "Epoch 192/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0564 - acc: 0.9144 - val_loss: 0.0731 - val_acc: 0.8993\n",
      "Epoch 193/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0576 - acc: 0.9130 - val_loss: 0.0743 - val_acc: 0.8954\n",
      "Epoch 194/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0559 - acc: 0.9173 - val_loss: 0.0734 - val_acc: 0.8980\n",
      "Epoch 195/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0580 - acc: 0.9166 - val_loss: 0.0741 - val_acc: 0.8863\n",
      "Epoch 196/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0545 - acc: 0.9199 - val_loss: 0.0747 - val_acc: 0.8928\n",
      "Epoch 197/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0522 - acc: 0.9235 - val_loss: 0.0752 - val_acc: 0.8941\n",
      "Epoch 198/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0567 - acc: 0.9134 - val_loss: 0.0734 - val_acc: 0.8941\n",
      "Epoch 199/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0568 - acc: 0.9170 - val_loss: 0.0781 - val_acc: 0.8810\n",
      "Epoch 200/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0554 - acc: 0.9242 - val_loss: 0.0765 - val_acc: 0.8902\n",
      "Epoch 201/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0596 - acc: 0.9153 - val_loss: 0.0771 - val_acc: 0.8889\n",
      "Epoch 202/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0529 - acc: 0.9202 - val_loss: 0.0764 - val_acc: 0.8941\n",
      "Epoch 203/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0573 - acc: 0.9134 - val_loss: 0.0770 - val_acc: 0.8967\n",
      "Epoch 204/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0556 - acc: 0.9153 - val_loss: 0.0797 - val_acc: 0.8915\n",
      "Epoch 205/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0551 - acc: 0.9215 - val_loss: 0.0782 - val_acc: 0.8928\n",
      "Epoch 206/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0528 - acc: 0.9229 - val_loss: 0.0744 - val_acc: 0.8941\n",
      "Epoch 207/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0534 - acc: 0.9235 - val_loss: 0.0753 - val_acc: 0.8967\n",
      "Epoch 208/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0556 - acc: 0.9183 - val_loss: 0.0742 - val_acc: 0.8954\n",
      "Epoch 209/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0554 - acc: 0.9121 - val_loss: 0.0734 - val_acc: 0.8993\n",
      "Epoch 210/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0527 - acc: 0.9229 - val_loss: 0.0738 - val_acc: 0.8941\n",
      "Epoch 211/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0539 - acc: 0.9229 - val_loss: 0.0727 - val_acc: 0.8980\n",
      "Epoch 212/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0522 - acc: 0.9271 - val_loss: 0.0741 - val_acc: 0.8993\n",
      "Epoch 213/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0535 - acc: 0.9225 - val_loss: 0.0759 - val_acc: 0.8941\n",
      "Epoch 214/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0529 - acc: 0.9268 - val_loss: 0.0731 - val_acc: 0.8928\n",
      "Epoch 215/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0532 - acc: 0.9268 - val_loss: 0.0729 - val_acc: 0.8941\n",
      "Epoch 216/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0514 - acc: 0.9248 - val_loss: 0.0758 - val_acc: 0.8967\n",
      "Epoch 217/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0531 - acc: 0.9238 - val_loss: 0.0740 - val_acc: 0.8928\n",
      "Epoch 218/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0526 - acc: 0.9235 - val_loss: 0.0763 - val_acc: 0.8915\n",
      "Epoch 219/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0528 - acc: 0.9232 - val_loss: 0.0753 - val_acc: 0.8889\n",
      "Epoch 220/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0531 - acc: 0.9229 - val_loss: 0.0744 - val_acc: 0.8967\n",
      "Epoch 221/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0540 - acc: 0.9242 - val_loss: 0.0735 - val_acc: 0.8954\n",
      "Epoch 222/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0543 - acc: 0.9199 - val_loss: 0.0734 - val_acc: 0.8928\n",
      "Epoch 223/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0511 - acc: 0.9264 - val_loss: 0.0713 - val_acc: 0.8954\n",
      "Epoch 224/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0507 - acc: 0.9294 - val_loss: 0.0738 - val_acc: 0.8954\n",
      "Epoch 225/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0520 - acc: 0.9255 - val_loss: 0.0726 - val_acc: 0.9007\n",
      "Epoch 226/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0495 - acc: 0.9284 - val_loss: 0.0717 - val_acc: 0.8993\n",
      "Epoch 227/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0511 - acc: 0.9271 - val_loss: 0.0750 - val_acc: 0.8928\n",
      "Epoch 228/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0507 - acc: 0.9291 - val_loss: 0.0714 - val_acc: 0.8967\n",
      "Epoch 229/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0516 - acc: 0.9245 - val_loss: 0.0707 - val_acc: 0.8993\n",
      "Epoch 230/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0529 - acc: 0.9258 - val_loss: 0.0749 - val_acc: 0.8980\n",
      "Epoch 231/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0511 - acc: 0.9242 - val_loss: 0.0725 - val_acc: 0.8928\n",
      "Epoch 232/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0505 - acc: 0.9245 - val_loss: 0.0761 - val_acc: 0.8954\n",
      "Epoch 233/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0505 - acc: 0.9268 - val_loss: 0.0763 - val_acc: 0.8915\n",
      "Epoch 234/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0506 - acc: 0.9255 - val_loss: 0.0741 - val_acc: 0.8980\n",
      "Epoch 235/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0514 - acc: 0.9287 - val_loss: 0.0726 - val_acc: 0.8967\n",
      "Epoch 236/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0513 - acc: 0.9278 - val_loss: 0.0733 - val_acc: 0.8993\n",
      "Epoch 237/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0505 - acc: 0.9297 - val_loss: 0.0795 - val_acc: 0.8967\n",
      "Epoch 238/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0515 - acc: 0.9248 - val_loss: 0.0748 - val_acc: 0.8941\n",
      "Epoch 239/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0480 - acc: 0.9343 - val_loss: 0.0724 - val_acc: 0.8980\n",
      "Epoch 240/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0513 - acc: 0.9264 - val_loss: 0.0773 - val_acc: 0.8889\n",
      "Epoch 241/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0528 - acc: 0.9248 - val_loss: 0.0750 - val_acc: 0.8967\n",
      "Epoch 242/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0493 - acc: 0.9317 - val_loss: 0.0731 - val_acc: 0.9020\n",
      "Epoch 243/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0510 - acc: 0.9261 - val_loss: 0.0765 - val_acc: 0.8993\n",
      "Epoch 244/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0468 - acc: 0.9314 - val_loss: 0.0741 - val_acc: 0.8954\n",
      "Epoch 245/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0514 - acc: 0.9284 - val_loss: 0.0723 - val_acc: 0.8980\n",
      "Epoch 246/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0506 - acc: 0.9287 - val_loss: 0.0728 - val_acc: 0.8980\n",
      "Epoch 247/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0505 - acc: 0.9268 - val_loss: 0.0733 - val_acc: 0.8941\n",
      "Epoch 248/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0504 - acc: 0.9271 - val_loss: 0.0730 - val_acc: 0.9007\n",
      "Epoch 249/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0491 - acc: 0.9278 - val_loss: 0.0722 - val_acc: 0.8993\n",
      "Epoch 250/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0491 - acc: 0.9291 - val_loss: 0.0737 - val_acc: 0.8967\n",
      "Epoch 251/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0493 - acc: 0.9297 - val_loss: 0.0716 - val_acc: 0.8928\n",
      "Epoch 252/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0488 - acc: 0.9287 - val_loss: 0.0723 - val_acc: 0.8954\n",
      "Epoch 253/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0479 - acc: 0.9310 - val_loss: 0.0732 - val_acc: 0.8928\n",
      "Epoch 254/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0486 - acc: 0.9297 - val_loss: 0.0750 - val_acc: 0.8967\n",
      "765/765 [==============================] - 4s 6ms/step\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadik/.local/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3059 samples, validate on 765 samples\n",
      "Epoch 1/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.5516 - acc: 0.5848 - val_loss: 0.2735 - val_acc: 0.6248\n",
      "Epoch 2/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.2549 - acc: 0.5989 - val_loss: 0.2267 - val_acc: 0.6209\n",
      "Epoch 3/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2195 - acc: 0.5953 - val_loss: 0.2055 - val_acc: 0.6327\n",
      "Epoch 4/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.2104 - acc: 0.6067 - val_loss: 0.1987 - val_acc: 0.6379\n",
      "Epoch 5/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.2075 - acc: 0.6067 - val_loss: 0.1964 - val_acc: 0.6471\n",
      "Epoch 6/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.2045 - acc: 0.6133 - val_loss: 0.1950 - val_acc: 0.6562\n",
      "Epoch 7/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.2003 - acc: 0.6267 - val_loss: 0.1999 - val_acc: 0.6052\n",
      "Epoch 8/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1964 - acc: 0.6453 - val_loss: 0.1828 - val_acc: 0.6745\n",
      "Epoch 9/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1897 - acc: 0.6777 - val_loss: 0.1708 - val_acc: 0.7647\n",
      "Epoch 10/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1756 - acc: 0.7244 - val_loss: 0.1502 - val_acc: 0.7373\n",
      "Epoch 11/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1598 - acc: 0.7551 - val_loss: 0.1315 - val_acc: 0.7974\n",
      "Epoch 12/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1475 - acc: 0.7725 - val_loss: 0.1213 - val_acc: 0.8052\n",
      "Epoch 13/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1390 - acc: 0.7954 - val_loss: 0.1173 - val_acc: 0.8222\n",
      "Epoch 14/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1367 - acc: 0.7862 - val_loss: 0.1118 - val_acc: 0.8209\n",
      "Epoch 15/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1320 - acc: 0.8012 - val_loss: 0.1100 - val_acc: 0.8275\n",
      "Epoch 16/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1278 - acc: 0.8081 - val_loss: 0.1072 - val_acc: 0.8327\n",
      "Epoch 17/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1296 - acc: 0.8022 - val_loss: 0.1064 - val_acc: 0.8353\n",
      "Epoch 18/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1286 - acc: 0.7993 - val_loss: 0.1046 - val_acc: 0.8379\n",
      "Epoch 19/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1244 - acc: 0.8107 - val_loss: 0.1022 - val_acc: 0.8549\n",
      "Epoch 20/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1242 - acc: 0.8153 - val_loss: 0.1019 - val_acc: 0.8484\n",
      "Epoch 21/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1233 - acc: 0.8173 - val_loss: 0.1043 - val_acc: 0.8549\n",
      "Epoch 22/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1237 - acc: 0.8192 - val_loss: 0.0984 - val_acc: 0.8575\n",
      "Epoch 23/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1235 - acc: 0.8137 - val_loss: 0.1123 - val_acc: 0.8327\n",
      "Epoch 24/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1222 - acc: 0.8205 - val_loss: 0.0965 - val_acc: 0.8654\n",
      "Epoch 25/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1214 - acc: 0.8205 - val_loss: 0.0959 - val_acc: 0.8680\n",
      "Epoch 26/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1159 - acc: 0.8300 - val_loss: 0.0985 - val_acc: 0.8575\n",
      "Epoch 27/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1185 - acc: 0.8248 - val_loss: 0.1049 - val_acc: 0.8458\n",
      "Epoch 28/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1153 - acc: 0.8280 - val_loss: 0.1097 - val_acc: 0.8314\n",
      "Epoch 29/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1178 - acc: 0.8228 - val_loss: 0.0945 - val_acc: 0.8680\n",
      "Epoch 30/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1167 - acc: 0.8280 - val_loss: 0.0958 - val_acc: 0.8680\n",
      "Epoch 31/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1169 - acc: 0.8277 - val_loss: 0.1002 - val_acc: 0.8536\n",
      "Epoch 32/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1145 - acc: 0.8261 - val_loss: 0.0887 - val_acc: 0.8784\n",
      "Epoch 33/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1144 - acc: 0.8267 - val_loss: 0.0894 - val_acc: 0.8771\n",
      "Epoch 34/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1124 - acc: 0.8290 - val_loss: 0.0905 - val_acc: 0.8784\n",
      "Epoch 35/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1123 - acc: 0.8362 - val_loss: 0.0882 - val_acc: 0.8758\n",
      "Epoch 36/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1090 - acc: 0.8434 - val_loss: 0.0869 - val_acc: 0.8771\n",
      "Epoch 37/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1090 - acc: 0.8424 - val_loss: 0.0878 - val_acc: 0.8810\n",
      "Epoch 38/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1079 - acc: 0.8362 - val_loss: 0.0849 - val_acc: 0.8810\n",
      "Epoch 39/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1079 - acc: 0.8418 - val_loss: 0.0846 - val_acc: 0.8876\n",
      "Epoch 40/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1074 - acc: 0.8418 - val_loss: 0.0877 - val_acc: 0.8719\n",
      "Epoch 41/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.1066 - acc: 0.8385 - val_loss: 0.0827 - val_acc: 0.8889\n",
      "Epoch 42/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1073 - acc: 0.8473 - val_loss: 0.0830 - val_acc: 0.8824\n",
      "Epoch 43/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1054 - acc: 0.8467 - val_loss: 0.0818 - val_acc: 0.8889\n",
      "Epoch 44/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.1021 - acc: 0.8532 - val_loss: 0.0846 - val_acc: 0.8850\n",
      "Epoch 45/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.1045 - acc: 0.8441 - val_loss: 0.0837 - val_acc: 0.8889\n",
      "Epoch 46/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1054 - acc: 0.8421 - val_loss: 0.0798 - val_acc: 0.8876\n",
      "Epoch 47/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1065 - acc: 0.8421 - val_loss: 0.0827 - val_acc: 0.8863\n",
      "Epoch 48/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1010 - acc: 0.8545 - val_loss: 0.0782 - val_acc: 0.8863\n",
      "Epoch 49/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1001 - acc: 0.8535 - val_loss: 0.0803 - val_acc: 0.8850\n",
      "Epoch 50/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1040 - acc: 0.8444 - val_loss: 0.0782 - val_acc: 0.8824\n",
      "Epoch 51/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1015 - acc: 0.8513 - val_loss: 0.0759 - val_acc: 0.8876\n",
      "Epoch 52/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.1011 - acc: 0.8509 - val_loss: 0.0800 - val_acc: 0.8915\n",
      "Epoch 53/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0992 - acc: 0.8565 - val_loss: 0.0773 - val_acc: 0.8941\n",
      "Epoch 54/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.1004 - acc: 0.8542 - val_loss: 0.0780 - val_acc: 0.8941\n",
      "Epoch 55/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0986 - acc: 0.8594 - val_loss: 0.0760 - val_acc: 0.8850\n",
      "Epoch 56/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0984 - acc: 0.8542 - val_loss: 0.0807 - val_acc: 0.8967\n",
      "Epoch 57/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0979 - acc: 0.8578 - val_loss: 0.0758 - val_acc: 0.8863\n",
      "Epoch 58/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0979 - acc: 0.8549 - val_loss: 0.0753 - val_acc: 0.8863\n",
      "Epoch 59/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0978 - acc: 0.8585 - val_loss: 0.0735 - val_acc: 0.8928\n",
      "Epoch 60/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0969 - acc: 0.8552 - val_loss: 0.0724 - val_acc: 0.8954\n",
      "Epoch 61/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0952 - acc: 0.8588 - val_loss: 0.0846 - val_acc: 0.8902\n",
      "Epoch 62/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0979 - acc: 0.8549 - val_loss: 0.0735 - val_acc: 0.8902\n",
      "Epoch 63/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0962 - acc: 0.8594 - val_loss: 0.0743 - val_acc: 0.8993\n",
      "Epoch 64/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0946 - acc: 0.8575 - val_loss: 0.0729 - val_acc: 0.8889\n",
      "Epoch 65/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0931 - acc: 0.8656 - val_loss: 0.0717 - val_acc: 0.9007\n",
      "Epoch 66/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0938 - acc: 0.8637 - val_loss: 0.0699 - val_acc: 0.8941\n",
      "Epoch 67/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0929 - acc: 0.8620 - val_loss: 0.0718 - val_acc: 0.8928\n",
      "Epoch 68/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0958 - acc: 0.8591 - val_loss: 0.0708 - val_acc: 0.9007\n",
      "Epoch 69/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0927 - acc: 0.8620 - val_loss: 0.0705 - val_acc: 0.8980\n",
      "Epoch 70/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0910 - acc: 0.8709 - val_loss: 0.0749 - val_acc: 0.8889\n",
      "Epoch 71/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0905 - acc: 0.8699 - val_loss: 0.0707 - val_acc: 0.9007\n",
      "Epoch 72/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0894 - acc: 0.8679 - val_loss: 0.0686 - val_acc: 0.8915\n",
      "Epoch 73/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0910 - acc: 0.8653 - val_loss: 0.0707 - val_acc: 0.8902\n",
      "Epoch 74/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0894 - acc: 0.8673 - val_loss: 0.0758 - val_acc: 0.8876\n",
      "Epoch 75/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0931 - acc: 0.8643 - val_loss: 0.0691 - val_acc: 0.8889\n",
      "Epoch 76/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0881 - acc: 0.8728 - val_loss: 0.0729 - val_acc: 0.9059\n",
      "Epoch 77/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0885 - acc: 0.8738 - val_loss: 0.0674 - val_acc: 0.8941\n",
      "Epoch 78/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0896 - acc: 0.8673 - val_loss: 0.0687 - val_acc: 0.9046\n",
      "Epoch 79/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0884 - acc: 0.8683 - val_loss: 0.0668 - val_acc: 0.8967\n",
      "Epoch 80/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0912 - acc: 0.8637 - val_loss: 0.0663 - val_acc: 0.8967\n",
      "Epoch 81/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0897 - acc: 0.8660 - val_loss: 0.0707 - val_acc: 0.8954\n",
      "Epoch 82/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0885 - acc: 0.8722 - val_loss: 0.0687 - val_acc: 0.8993\n",
      "Epoch 83/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0878 - acc: 0.8732 - val_loss: 0.0704 - val_acc: 0.9033\n",
      "Epoch 84/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0853 - acc: 0.8676 - val_loss: 0.0679 - val_acc: 0.9059\n",
      "Epoch 85/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0864 - acc: 0.8751 - val_loss: 0.0662 - val_acc: 0.8967\n",
      "Epoch 86/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0885 - acc: 0.8686 - val_loss: 0.0674 - val_acc: 0.9059\n",
      "Epoch 87/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0873 - acc: 0.8751 - val_loss: 0.0763 - val_acc: 0.8837\n",
      "Epoch 88/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0869 - acc: 0.8666 - val_loss: 0.0692 - val_acc: 0.8954\n",
      "Epoch 89/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0848 - acc: 0.8748 - val_loss: 0.0664 - val_acc: 0.9059\n",
      "Epoch 90/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0829 - acc: 0.8820 - val_loss: 0.0656 - val_acc: 0.9007\n",
      "Epoch 91/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0850 - acc: 0.8764 - val_loss: 0.0684 - val_acc: 0.9020\n",
      "Epoch 92/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0842 - acc: 0.8751 - val_loss: 0.0687 - val_acc: 0.9085\n",
      "Epoch 93/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0850 - acc: 0.8702 - val_loss: 0.0655 - val_acc: 0.9020\n",
      "Epoch 94/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0843 - acc: 0.8748 - val_loss: 0.0705 - val_acc: 0.9020\n",
      "Epoch 95/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0849 - acc: 0.8781 - val_loss: 0.0730 - val_acc: 0.8954\n",
      "Epoch 96/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0836 - acc: 0.8768 - val_loss: 0.0674 - val_acc: 0.9007\n",
      "Epoch 97/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0834 - acc: 0.8768 - val_loss: 0.0652 - val_acc: 0.9033\n",
      "Epoch 98/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0843 - acc: 0.8758 - val_loss: 0.0643 - val_acc: 0.9007\n",
      "Epoch 99/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0809 - acc: 0.8777 - val_loss: 0.0668 - val_acc: 0.9046\n",
      "Epoch 100/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0845 - acc: 0.8725 - val_loss: 0.0657 - val_acc: 0.8993\n",
      "Epoch 101/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0832 - acc: 0.8745 - val_loss: 0.0654 - val_acc: 0.9085\n",
      "Epoch 102/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0810 - acc: 0.8781 - val_loss: 0.0658 - val_acc: 0.9046\n",
      "Epoch 103/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0811 - acc: 0.8820 - val_loss: 0.0657 - val_acc: 0.9072\n",
      "Epoch 104/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0818 - acc: 0.8761 - val_loss: 0.0681 - val_acc: 0.9046\n",
      "Epoch 105/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0803 - acc: 0.8846 - val_loss: 0.0641 - val_acc: 0.9059\n",
      "Epoch 106/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0801 - acc: 0.8797 - val_loss: 0.0628 - val_acc: 0.9085\n",
      "Epoch 107/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0823 - acc: 0.8794 - val_loss: 0.0631 - val_acc: 0.9072\n",
      "Epoch 108/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0827 - acc: 0.8768 - val_loss: 0.0676 - val_acc: 0.9033\n",
      "Epoch 109/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0811 - acc: 0.8777 - val_loss: 0.0656 - val_acc: 0.9059\n",
      "Epoch 110/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0804 - acc: 0.8830 - val_loss: 0.0652 - val_acc: 0.9046\n",
      "Epoch 111/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0812 - acc: 0.8790 - val_loss: 0.0637 - val_acc: 0.9098\n",
      "Epoch 112/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0780 - acc: 0.8862 - val_loss: 0.0647 - val_acc: 0.9059\n",
      "Epoch 113/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0783 - acc: 0.8866 - val_loss: 0.0638 - val_acc: 0.9111\n",
      "Epoch 114/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0798 - acc: 0.8810 - val_loss: 0.0631 - val_acc: 0.9098\n",
      "Epoch 115/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0796 - acc: 0.8826 - val_loss: 0.0636 - val_acc: 0.9124\n",
      "Epoch 116/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0771 - acc: 0.8908 - val_loss: 0.0652 - val_acc: 0.9072\n",
      "Epoch 117/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0778 - acc: 0.8875 - val_loss: 0.0629 - val_acc: 0.9124\n",
      "Epoch 118/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0790 - acc: 0.8866 - val_loss: 0.0644 - val_acc: 0.9150\n",
      "Epoch 119/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0775 - acc: 0.8885 - val_loss: 0.0631 - val_acc: 0.9111\n",
      "Epoch 120/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0758 - acc: 0.8849 - val_loss: 0.0647 - val_acc: 0.9137\n",
      "Epoch 121/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0772 - acc: 0.8830 - val_loss: 0.0635 - val_acc: 0.9150\n",
      "Epoch 122/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0763 - acc: 0.8908 - val_loss: 0.0619 - val_acc: 0.9098\n",
      "Epoch 123/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0761 - acc: 0.8879 - val_loss: 0.0613 - val_acc: 0.9137\n",
      "Epoch 124/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0764 - acc: 0.8872 - val_loss: 0.0618 - val_acc: 0.9150\n",
      "Epoch 125/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0761 - acc: 0.8892 - val_loss: 0.0649 - val_acc: 0.9085\n",
      "Epoch 126/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0765 - acc: 0.8882 - val_loss: 0.0611 - val_acc: 0.9163\n",
      "Epoch 127/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0757 - acc: 0.8911 - val_loss: 0.0635 - val_acc: 0.9163\n",
      "Epoch 128/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0777 - acc: 0.8817 - val_loss: 0.0610 - val_acc: 0.9111\n",
      "Epoch 129/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0770 - acc: 0.8875 - val_loss: 0.0647 - val_acc: 0.9137\n",
      "Epoch 130/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0747 - acc: 0.8938 - val_loss: 0.0646 - val_acc: 0.9098\n",
      "Epoch 131/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0752 - acc: 0.8921 - val_loss: 0.0658 - val_acc: 0.9124\n",
      "Epoch 132/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0735 - acc: 0.8918 - val_loss: 0.0649 - val_acc: 0.9137\n",
      "Epoch 133/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0751 - acc: 0.8892 - val_loss: 0.0617 - val_acc: 0.9137\n",
      "Epoch 134/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0742 - acc: 0.8918 - val_loss: 0.0619 - val_acc: 0.9111\n",
      "Epoch 135/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0739 - acc: 0.8938 - val_loss: 0.0609 - val_acc: 0.9190\n",
      "Epoch 136/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0732 - acc: 0.8882 - val_loss: 0.0668 - val_acc: 0.8980\n",
      "Epoch 137/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0736 - acc: 0.8905 - val_loss: 0.0611 - val_acc: 0.9150\n",
      "Epoch 138/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0750 - acc: 0.8869 - val_loss: 0.0608 - val_acc: 0.9137\n",
      "Epoch 139/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0712 - acc: 0.8983 - val_loss: 0.0725 - val_acc: 0.8941\n",
      "Epoch 140/600\n",
      "3059/3059 [==============================] - 38s 13ms/step - loss: 0.0731 - acc: 0.8941 - val_loss: 0.0600 - val_acc: 0.9150\n",
      "Epoch 141/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0719 - acc: 0.8924 - val_loss: 0.0617 - val_acc: 0.9072\n",
      "Epoch 142/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0739 - acc: 0.8889 - val_loss: 0.0615 - val_acc: 0.9176\n",
      "Epoch 143/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0725 - acc: 0.8951 - val_loss: 0.0615 - val_acc: 0.9163\n",
      "Epoch 144/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0722 - acc: 0.8921 - val_loss: 0.0597 - val_acc: 0.9163\n",
      "Epoch 145/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0726 - acc: 0.8951 - val_loss: 0.0595 - val_acc: 0.9124\n",
      "Epoch 146/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0707 - acc: 0.8990 - val_loss: 0.0593 - val_acc: 0.9163\n",
      "Epoch 147/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0727 - acc: 0.8902 - val_loss: 0.0626 - val_acc: 0.9098\n",
      "Epoch 148/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0730 - acc: 0.8902 - val_loss: 0.0622 - val_acc: 0.9111\n",
      "Epoch 149/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0700 - acc: 0.8993 - val_loss: 0.0596 - val_acc: 0.9124\n",
      "Epoch 150/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0710 - acc: 0.8954 - val_loss: 0.0607 - val_acc: 0.9176\n",
      "Epoch 151/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0708 - acc: 0.8947 - val_loss: 0.0601 - val_acc: 0.9163\n",
      "Epoch 152/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0709 - acc: 0.8987 - val_loss: 0.0601 - val_acc: 0.9163\n",
      "Epoch 153/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0705 - acc: 0.8970 - val_loss: 0.0593 - val_acc: 0.9176\n",
      "Epoch 154/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0697 - acc: 0.9013 - val_loss: 0.0645 - val_acc: 0.9085\n",
      "Epoch 155/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0704 - acc: 0.8983 - val_loss: 0.0619 - val_acc: 0.9124\n",
      "Epoch 156/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0686 - acc: 0.8983 - val_loss: 0.0598 - val_acc: 0.9150\n",
      "Epoch 157/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0676 - acc: 0.9019 - val_loss: 0.0600 - val_acc: 0.9137\n",
      "Epoch 158/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0686 - acc: 0.8987 - val_loss: 0.0614 - val_acc: 0.9098\n",
      "Epoch 159/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0679 - acc: 0.9039 - val_loss: 0.0574 - val_acc: 0.9176\n",
      "Epoch 160/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0682 - acc: 0.8993 - val_loss: 0.0628 - val_acc: 0.9098\n",
      "Epoch 161/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0676 - acc: 0.9042 - val_loss: 0.0583 - val_acc: 0.9150\n",
      "Epoch 162/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0682 - acc: 0.9023 - val_loss: 0.0635 - val_acc: 0.9150\n",
      "Epoch 163/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0701 - acc: 0.8941 - val_loss: 0.0586 - val_acc: 0.9176\n",
      "Epoch 164/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0675 - acc: 0.9003 - val_loss: 0.0571 - val_acc: 0.9216\n",
      "Epoch 165/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0676 - acc: 0.8983 - val_loss: 0.0592 - val_acc: 0.9124\n",
      "Epoch 166/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0674 - acc: 0.9023 - val_loss: 0.0610 - val_acc: 0.9176\n",
      "Epoch 167/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0665 - acc: 0.9055 - val_loss: 0.0578 - val_acc: 0.9190\n",
      "Epoch 168/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0659 - acc: 0.9023 - val_loss: 0.0618 - val_acc: 0.9072\n",
      "Epoch 169/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0683 - acc: 0.9003 - val_loss: 0.0573 - val_acc: 0.9190\n",
      "Epoch 170/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0665 - acc: 0.9019 - val_loss: 0.0650 - val_acc: 0.9046\n",
      "Epoch 171/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0663 - acc: 0.9032 - val_loss: 0.0603 - val_acc: 0.9098\n",
      "Epoch 172/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0679 - acc: 0.9006 - val_loss: 0.0563 - val_acc: 0.9203\n",
      "Epoch 173/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0663 - acc: 0.9026 - val_loss: 0.0616 - val_acc: 0.9098\n",
      "Epoch 174/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0638 - acc: 0.9065 - val_loss: 0.0584 - val_acc: 0.9163\n",
      "Epoch 175/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0650 - acc: 0.9055 - val_loss: 0.0581 - val_acc: 0.9137\n",
      "Epoch 176/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0624 - acc: 0.9081 - val_loss: 0.0578 - val_acc: 0.9203\n",
      "Epoch 177/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0634 - acc: 0.9045 - val_loss: 0.0577 - val_acc: 0.9137\n",
      "Epoch 178/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0635 - acc: 0.9091 - val_loss: 0.0582 - val_acc: 0.9190\n",
      "Epoch 179/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0654 - acc: 0.9023 - val_loss: 0.0571 - val_acc: 0.9176\n",
      "Epoch 180/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0644 - acc: 0.9029 - val_loss: 0.0616 - val_acc: 0.9176\n",
      "Epoch 181/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0641 - acc: 0.9088 - val_loss: 0.0576 - val_acc: 0.9163\n",
      "Epoch 182/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0604 - acc: 0.9176 - val_loss: 0.0580 - val_acc: 0.9176\n",
      "Epoch 183/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0663 - acc: 0.9045 - val_loss: 0.0556 - val_acc: 0.9190\n",
      "Epoch 184/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0647 - acc: 0.9111 - val_loss: 0.0561 - val_acc: 0.9163\n",
      "Epoch 185/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0636 - acc: 0.9078 - val_loss: 0.0587 - val_acc: 0.9150\n",
      "Epoch 186/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0659 - acc: 0.9006 - val_loss: 0.0565 - val_acc: 0.9190\n",
      "Epoch 187/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0638 - acc: 0.9094 - val_loss: 0.0599 - val_acc: 0.9137\n",
      "Epoch 188/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0628 - acc: 0.9101 - val_loss: 0.0565 - val_acc: 0.9150\n",
      "Epoch 189/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0642 - acc: 0.9029 - val_loss: 0.0574 - val_acc: 0.9137\n",
      "Epoch 190/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0643 - acc: 0.9019 - val_loss: 0.0566 - val_acc: 0.9229\n",
      "Epoch 191/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0619 - acc: 0.9108 - val_loss: 0.0584 - val_acc: 0.9176\n",
      "Epoch 192/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0633 - acc: 0.9065 - val_loss: 0.0591 - val_acc: 0.9216\n",
      "Epoch 193/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0628 - acc: 0.9091 - val_loss: 0.0637 - val_acc: 0.9059\n",
      "Epoch 194/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0645 - acc: 0.9045 - val_loss: 0.0625 - val_acc: 0.9124\n",
      "Epoch 195/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0632 - acc: 0.9075 - val_loss: 0.0571 - val_acc: 0.9137\n",
      "Epoch 196/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0646 - acc: 0.9045 - val_loss: 0.0565 - val_acc: 0.9216\n",
      "Epoch 197/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0615 - acc: 0.9117 - val_loss: 0.0570 - val_acc: 0.9137\n",
      "Epoch 198/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0632 - acc: 0.9068 - val_loss: 0.0619 - val_acc: 0.9111\n",
      "Epoch 199/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0611 - acc: 0.9117 - val_loss: 0.0568 - val_acc: 0.9150\n",
      "Epoch 200/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0597 - acc: 0.9111 - val_loss: 0.0551 - val_acc: 0.9124\n",
      "Epoch 201/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0604 - acc: 0.9121 - val_loss: 0.0627 - val_acc: 0.9098\n",
      "Epoch 202/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0588 - acc: 0.9170 - val_loss: 0.0607 - val_acc: 0.9098\n",
      "Epoch 203/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0628 - acc: 0.9104 - val_loss: 0.0549 - val_acc: 0.9229\n",
      "Epoch 204/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0613 - acc: 0.9108 - val_loss: 0.0569 - val_acc: 0.9190\n",
      "Epoch 205/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0608 - acc: 0.9091 - val_loss: 0.0581 - val_acc: 0.9124\n",
      "Epoch 206/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0573 - acc: 0.9147 - val_loss: 0.0550 - val_acc: 0.9229\n",
      "Epoch 207/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0597 - acc: 0.9140 - val_loss: 0.0573 - val_acc: 0.9137\n",
      "Epoch 208/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0606 - acc: 0.9081 - val_loss: 0.0557 - val_acc: 0.9229\n",
      "Epoch 209/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0593 - acc: 0.9157 - val_loss: 0.0566 - val_acc: 0.9203\n",
      "Epoch 210/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0591 - acc: 0.9150 - val_loss: 0.0541 - val_acc: 0.9190\n",
      "Epoch 211/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0607 - acc: 0.9094 - val_loss: 0.0565 - val_acc: 0.9176\n",
      "Epoch 212/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0603 - acc: 0.9134 - val_loss: 0.0643 - val_acc: 0.9072\n",
      "Epoch 213/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0606 - acc: 0.9140 - val_loss: 0.0567 - val_acc: 0.9163\n",
      "Epoch 214/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0603 - acc: 0.9114 - val_loss: 0.0570 - val_acc: 0.9216\n",
      "Epoch 215/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0594 - acc: 0.9153 - val_loss: 0.0568 - val_acc: 0.9176\n",
      "Epoch 216/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0594 - acc: 0.9127 - val_loss: 0.0555 - val_acc: 0.9150\n",
      "Epoch 217/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0636 - acc: 0.9062 - val_loss: 0.0556 - val_acc: 0.9176\n",
      "Epoch 218/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0577 - acc: 0.9137 - val_loss: 0.0552 - val_acc: 0.9229\n",
      "Epoch 219/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0587 - acc: 0.9193 - val_loss: 0.0581 - val_acc: 0.9150\n",
      "Epoch 220/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0574 - acc: 0.9179 - val_loss: 0.0554 - val_acc: 0.9203\n",
      "Epoch 221/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0576 - acc: 0.9127 - val_loss: 0.0540 - val_acc: 0.9268\n",
      "Epoch 222/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0565 - acc: 0.9196 - val_loss: 0.0578 - val_acc: 0.9176\n",
      "Epoch 223/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0573 - acc: 0.9209 - val_loss: 0.0665 - val_acc: 0.9111\n",
      "Epoch 224/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0574 - acc: 0.9134 - val_loss: 0.0603 - val_acc: 0.9098\n",
      "Epoch 225/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0585 - acc: 0.9163 - val_loss: 0.0537 - val_acc: 0.9190\n",
      "Epoch 226/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0572 - acc: 0.9150 - val_loss: 0.0538 - val_acc: 0.9255\n",
      "Epoch 227/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0557 - acc: 0.9209 - val_loss: 0.0624 - val_acc: 0.9111\n",
      "Epoch 228/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0561 - acc: 0.9215 - val_loss: 0.0538 - val_acc: 0.9203\n",
      "Epoch 229/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0570 - acc: 0.9202 - val_loss: 0.0560 - val_acc: 0.9216\n",
      "Epoch 230/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0567 - acc: 0.9170 - val_loss: 0.0543 - val_acc: 0.9163\n",
      "Epoch 231/600\n",
      "3059/3059 [==============================] - 33s 11ms/step - loss: 0.0570 - acc: 0.9150 - val_loss: 0.0557 - val_acc: 0.9150\n",
      "Epoch 232/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0554 - acc: 0.9176 - val_loss: 0.0561 - val_acc: 0.9163\n",
      "Epoch 233/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0576 - acc: 0.9186 - val_loss: 0.0564 - val_acc: 0.9137\n",
      "Epoch 234/600\n",
      "3059/3059 [==============================] - 34s 11ms/step - loss: 0.0563 - acc: 0.9199 - val_loss: 0.0558 - val_acc: 0.9203\n",
      "Epoch 235/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0553 - acc: 0.9268 - val_loss: 0.0562 - val_acc: 0.9176\n",
      "Epoch 236/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0560 - acc: 0.9206 - val_loss: 0.0586 - val_acc: 0.9059\n",
      "Epoch 237/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0566 - acc: 0.9179 - val_loss: 0.0539 - val_acc: 0.9203\n",
      "Epoch 238/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0553 - acc: 0.9173 - val_loss: 0.0550 - val_acc: 0.9242\n",
      "Epoch 239/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0557 - acc: 0.9193 - val_loss: 0.0539 - val_acc: 0.9176\n",
      "Epoch 240/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0555 - acc: 0.9183 - val_loss: 0.0550 - val_acc: 0.9229\n",
      "Epoch 241/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0571 - acc: 0.9176 - val_loss: 0.0540 - val_acc: 0.9190\n",
      "Epoch 242/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0581 - acc: 0.9137 - val_loss: 0.0606 - val_acc: 0.9111\n",
      "Epoch 243/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0550 - acc: 0.9215 - val_loss: 0.0540 - val_acc: 0.9190\n",
      "Epoch 244/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0558 - acc: 0.9160 - val_loss: 0.0595 - val_acc: 0.9124\n",
      "Epoch 245/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0547 - acc: 0.9153 - val_loss: 0.0536 - val_acc: 0.9190\n",
      "Epoch 246/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0526 - acc: 0.9242 - val_loss: 0.0540 - val_acc: 0.9176\n",
      "Epoch 247/600\n",
      "3059/3059 [==============================] - 35s 12ms/step - loss: 0.0557 - acc: 0.9170 - val_loss: 0.0554 - val_acc: 0.9137\n",
      "Epoch 248/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0558 - acc: 0.9183 - val_loss: 0.0537 - val_acc: 0.9190\n",
      "Epoch 249/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0562 - acc: 0.9170 - val_loss: 0.0588 - val_acc: 0.9111\n",
      "Epoch 250/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0546 - acc: 0.9179 - val_loss: 0.0553 - val_acc: 0.9190\n",
      "Epoch 251/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0543 - acc: 0.9193 - val_loss: 0.0580 - val_acc: 0.9124\n",
      "Epoch 252/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0544 - acc: 0.9242 - val_loss: 0.0563 - val_acc: 0.9163\n",
      "Epoch 253/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.0550 - acc: 0.9238 - val_loss: 0.0546 - val_acc: 0.9176\n",
      "Epoch 254/600\n",
      "3059/3059 [==============================] - 42s 14ms/step - loss: 0.0551 - acc: 0.9206 - val_loss: 0.0562 - val_acc: 0.9111\n",
      "Epoch 255/600\n",
      "3059/3059 [==============================] - 42s 14ms/step - loss: 0.0550 - acc: 0.9173 - val_loss: 0.0539 - val_acc: 0.9203\n",
      "Epoch 256/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.0561 - acc: 0.9163 - val_loss: 0.0589 - val_acc: 0.9085\n",
      "Epoch 257/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.0543 - acc: 0.9258 - val_loss: 0.0561 - val_acc: 0.9229\n",
      "Epoch 258/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0529 - acc: 0.9242 - val_loss: 0.0562 - val_acc: 0.9203\n",
      "Epoch 259/600\n",
      "3059/3059 [==============================] - 40s 13ms/step - loss: 0.0527 - acc: 0.9251 - val_loss: 0.0544 - val_acc: 0.9203\n",
      "Epoch 260/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0561 - acc: 0.9212 - val_loss: 0.0559 - val_acc: 0.9150\n",
      "Epoch 261/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.0531 - acc: 0.9255 - val_loss: 0.0530 - val_acc: 0.9216\n",
      "Epoch 262/600\n",
      "3059/3059 [==============================] - 40s 13ms/step - loss: 0.0539 - acc: 0.9235 - val_loss: 0.0527 - val_acc: 0.9216\n",
      "Epoch 263/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0510 - acc: 0.9271 - val_loss: 0.0547 - val_acc: 0.9203\n",
      "Epoch 264/600\n",
      "3059/3059 [==============================] - 41s 13ms/step - loss: 0.0533 - acc: 0.9245 - val_loss: 0.0559 - val_acc: 0.9190\n",
      "Epoch 265/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0510 - acc: 0.9314 - val_loss: 0.0543 - val_acc: 0.9190\n",
      "Epoch 266/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0520 - acc: 0.9278 - val_loss: 0.0544 - val_acc: 0.9203\n",
      "Epoch 267/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0525 - acc: 0.9225 - val_loss: 0.0555 - val_acc: 0.9242\n",
      "Epoch 268/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0507 - acc: 0.9238 - val_loss: 0.0567 - val_acc: 0.9085\n",
      "Epoch 269/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0534 - acc: 0.9212 - val_loss: 0.0575 - val_acc: 0.9098\n",
      "Epoch 270/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0513 - acc: 0.9245 - val_loss: 0.0543 - val_acc: 0.9150\n",
      "Epoch 271/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0519 - acc: 0.9255 - val_loss: 0.0541 - val_acc: 0.9203\n",
      "Epoch 272/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0510 - acc: 0.9268 - val_loss: 0.0565 - val_acc: 0.9163\n",
      "Epoch 273/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0524 - acc: 0.9255 - val_loss: 0.0545 - val_acc: 0.9176\n",
      "Epoch 274/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0517 - acc: 0.9271 - val_loss: 0.0566 - val_acc: 0.9137\n",
      "Epoch 275/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0505 - acc: 0.9294 - val_loss: 0.0583 - val_acc: 0.9111\n",
      "Epoch 276/600\n",
      "3059/3059 [==============================] - 38s 12ms/step - loss: 0.0532 - acc: 0.9245 - val_loss: 0.0543 - val_acc: 0.9163\n",
      "Epoch 277/600\n",
      "3059/3059 [==============================] - 39s 13ms/step - loss: 0.0557 - acc: 0.9196 - val_loss: 0.0552 - val_acc: 0.9255\n",
      "Epoch 278/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0517 - acc: 0.9232 - val_loss: 0.0553 - val_acc: 0.9176\n",
      "Epoch 279/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0526 - acc: 0.9248 - val_loss: 0.0531 - val_acc: 0.9190\n",
      "Epoch 280/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0516 - acc: 0.9255 - val_loss: 0.0551 - val_acc: 0.9150\n",
      "Epoch 281/600\n",
      "3059/3059 [==============================] - 35s 11ms/step - loss: 0.0506 - acc: 0.9245 - val_loss: 0.0539 - val_acc: 0.9137\n",
      "Epoch 282/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0509 - acc: 0.9238 - val_loss: 0.0540 - val_acc: 0.9190\n",
      "Epoch 283/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0519 - acc: 0.9238 - val_loss: 0.0561 - val_acc: 0.9176\n",
      "Epoch 284/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0496 - acc: 0.9261 - val_loss: 0.0538 - val_acc: 0.9176\n",
      "Epoch 285/600\n",
      "3059/3059 [==============================] - 37s 12ms/step - loss: 0.0491 - acc: 0.9320 - val_loss: 0.0549 - val_acc: 0.9190\n",
      "Epoch 286/600\n",
      "3059/3059 [==============================] - 36s 12ms/step - loss: 0.0515 - acc: 0.9258 - val_loss: 0.0540 - val_acc: 0.9190\n",
      "Epoch 287/600\n",
      "3059/3059 [==============================] - 38s 13ms/step - loss: 0.0490 - acc: 0.9307 - val_loss: 0.0534 - val_acc: 0.9203\n",
      "765/765 [==============================] - 4s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadik/.local/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3060 samples, validate on 764 samples\n",
      "Epoch 1/600\n",
      "3060/3060 [==============================] - 42s 14ms/step - loss: 0.5666 - acc: 0.5562 - val_loss: 0.2709 - val_acc: 0.5026\n",
      "Epoch 2/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.2528 - acc: 0.5614 - val_loss: 0.2282 - val_acc: 0.5759\n",
      "Epoch 3/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.2157 - acc: 0.6052 - val_loss: 0.2134 - val_acc: 0.5798\n",
      "Epoch 4/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.2076 - acc: 0.6118 - val_loss: 0.2058 - val_acc: 0.6021\n",
      "Epoch 5/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.2038 - acc: 0.6176 - val_loss: 0.2033 - val_acc: 0.6073\n",
      "Epoch 6/600\n",
      "3060/3060 [==============================] - 39s 13ms/step - loss: 0.2016 - acc: 0.6268 - val_loss: 0.1996 - val_acc: 0.6113\n",
      "Epoch 7/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.1990 - acc: 0.6278 - val_loss: 0.2002 - val_acc: 0.6322\n",
      "Epoch 8/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1958 - acc: 0.6484 - val_loss: 0.1976 - val_acc: 0.6767\n",
      "Epoch 9/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1895 - acc: 0.6539 - val_loss: 0.1805 - val_acc: 0.6492\n",
      "Epoch 10/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1776 - acc: 0.6951 - val_loss: 0.1636 - val_acc: 0.7866\n",
      "Epoch 11/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.1571 - acc: 0.7601 - val_loss: 0.1472 - val_acc: 0.7840\n",
      "Epoch 12/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1454 - acc: 0.7709 - val_loss: 0.1286 - val_acc: 0.8089\n",
      "Epoch 13/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1386 - acc: 0.7866 - val_loss: 0.1230 - val_acc: 0.8298\n",
      "Epoch 14/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1346 - acc: 0.7938 - val_loss: 0.1204 - val_acc: 0.8325\n",
      "Epoch 15/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.1342 - acc: 0.7941 - val_loss: 0.1222 - val_acc: 0.8259\n",
      "Epoch 16/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1287 - acc: 0.8062 - val_loss: 0.1174 - val_acc: 0.8285\n",
      "Epoch 17/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.1302 - acc: 0.8003 - val_loss: 0.1112 - val_acc: 0.8377\n",
      "Epoch 18/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.1243 - acc: 0.8147 - val_loss: 0.1151 - val_acc: 0.8338\n",
      "Epoch 19/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1227 - acc: 0.8095 - val_loss: 0.1146 - val_acc: 0.8272\n",
      "Epoch 20/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1228 - acc: 0.8167 - val_loss: 0.1103 - val_acc: 0.8429\n",
      "Epoch 21/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1215 - acc: 0.8160 - val_loss: 0.1124 - val_acc: 0.8325\n",
      "Epoch 22/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1196 - acc: 0.8225 - val_loss: 0.1065 - val_acc: 0.8495\n",
      "Epoch 23/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1177 - acc: 0.8183 - val_loss: 0.1048 - val_acc: 0.8560\n",
      "Epoch 24/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1173 - acc: 0.8225 - val_loss: 0.1023 - val_acc: 0.8547\n",
      "Epoch 25/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1148 - acc: 0.8307 - val_loss: 0.1038 - val_acc: 0.8547\n",
      "Epoch 26/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.1147 - acc: 0.8242 - val_loss: 0.1009 - val_acc: 0.8626\n",
      "Epoch 27/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.1152 - acc: 0.8245 - val_loss: 0.1012 - val_acc: 0.8495\n",
      "Epoch 28/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1133 - acc: 0.8297 - val_loss: 0.1208 - val_acc: 0.8220\n",
      "Epoch 29/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1110 - acc: 0.8320 - val_loss: 0.0991 - val_acc: 0.8521\n",
      "Epoch 30/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1087 - acc: 0.8402 - val_loss: 0.1019 - val_acc: 0.8455\n",
      "Epoch 31/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1094 - acc: 0.8379 - val_loss: 0.1082 - val_acc: 0.8416\n",
      "Epoch 32/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1069 - acc: 0.8444 - val_loss: 0.0997 - val_acc: 0.8613\n",
      "Epoch 33/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1082 - acc: 0.8376 - val_loss: 0.0988 - val_acc: 0.8626\n",
      "Epoch 34/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1034 - acc: 0.8441 - val_loss: 0.1008 - val_acc: 0.8586\n",
      "Epoch 35/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1068 - acc: 0.8402 - val_loss: 0.0988 - val_acc: 0.8639\n",
      "Epoch 36/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.1041 - acc: 0.8422 - val_loss: 0.0961 - val_acc: 0.8639\n",
      "Epoch 37/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1062 - acc: 0.8428 - val_loss: 0.0980 - val_acc: 0.8626\n",
      "Epoch 38/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.1055 - acc: 0.8438 - val_loss: 0.0965 - val_acc: 0.8626\n",
      "Epoch 39/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.1016 - acc: 0.8474 - val_loss: 0.0984 - val_acc: 0.8652\n",
      "Epoch 40/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.1059 - acc: 0.8431 - val_loss: 0.0966 - val_acc: 0.8613\n",
      "Epoch 41/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1010 - acc: 0.8441 - val_loss: 0.0950 - val_acc: 0.8639\n",
      "Epoch 42/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1034 - acc: 0.8500 - val_loss: 0.0993 - val_acc: 0.8534\n",
      "Epoch 43/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.1008 - acc: 0.8507 - val_loss: 0.0957 - val_acc: 0.8613\n",
      "Epoch 44/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0983 - acc: 0.8520 - val_loss: 0.0969 - val_acc: 0.8665\n",
      "Epoch 45/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0989 - acc: 0.8546 - val_loss: 0.0977 - val_acc: 0.8613\n",
      "Epoch 46/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0976 - acc: 0.8611 - val_loss: 0.0935 - val_acc: 0.8678\n",
      "Epoch 47/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0980 - acc: 0.8562 - val_loss: 0.0938 - val_acc: 0.8652\n",
      "Epoch 48/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0968 - acc: 0.8536 - val_loss: 0.0977 - val_acc: 0.8599\n",
      "Epoch 49/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0970 - acc: 0.8526 - val_loss: 0.0926 - val_acc: 0.8665\n",
      "Epoch 50/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0941 - acc: 0.8585 - val_loss: 0.0934 - val_acc: 0.8678\n",
      "Epoch 51/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0964 - acc: 0.8549 - val_loss: 0.0936 - val_acc: 0.8704\n",
      "Epoch 52/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0937 - acc: 0.8601 - val_loss: 0.0921 - val_acc: 0.8665\n",
      "Epoch 53/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0932 - acc: 0.8670 - val_loss: 0.0966 - val_acc: 0.8665\n",
      "Epoch 54/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0940 - acc: 0.8601 - val_loss: 0.0938 - val_acc: 0.8665\n",
      "Epoch 55/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0942 - acc: 0.8605 - val_loss: 0.0937 - val_acc: 0.8704\n",
      "Epoch 56/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0934 - acc: 0.8588 - val_loss: 0.0932 - val_acc: 0.8691\n",
      "Epoch 57/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0928 - acc: 0.8660 - val_loss: 0.0919 - val_acc: 0.8730\n",
      "Epoch 58/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0917 - acc: 0.8663 - val_loss: 0.0918 - val_acc: 0.8717\n",
      "Epoch 59/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0896 - acc: 0.8693 - val_loss: 0.0906 - val_acc: 0.8704\n",
      "Epoch 60/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0920 - acc: 0.8601 - val_loss: 0.0906 - val_acc: 0.8743\n",
      "Epoch 61/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0908 - acc: 0.8686 - val_loss: 0.0912 - val_acc: 0.8730\n",
      "Epoch 62/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0870 - acc: 0.8732 - val_loss: 0.0978 - val_acc: 0.8665\n",
      "Epoch 63/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0885 - acc: 0.8696 - val_loss: 0.0912 - val_acc: 0.8757\n",
      "Epoch 64/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0884 - acc: 0.8667 - val_loss: 0.0960 - val_acc: 0.8665\n",
      "Epoch 65/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0911 - acc: 0.8644 - val_loss: 0.0897 - val_acc: 0.8743\n",
      "Epoch 66/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0878 - acc: 0.8657 - val_loss: 0.0916 - val_acc: 0.8757\n",
      "Epoch 67/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0882 - acc: 0.8735 - val_loss: 0.0886 - val_acc: 0.8743\n",
      "Epoch 68/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0871 - acc: 0.8745 - val_loss: 0.0917 - val_acc: 0.8743\n",
      "Epoch 69/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0862 - acc: 0.8719 - val_loss: 0.0877 - val_acc: 0.8770\n",
      "Epoch 70/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0876 - acc: 0.8732 - val_loss: 0.0902 - val_acc: 0.8770\n",
      "Epoch 71/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0863 - acc: 0.8742 - val_loss: 0.0899 - val_acc: 0.8796\n",
      "Epoch 72/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0865 - acc: 0.8752 - val_loss: 0.0872 - val_acc: 0.8743\n",
      "Epoch 73/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0852 - acc: 0.8739 - val_loss: 0.0876 - val_acc: 0.8757\n",
      "Epoch 74/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0856 - acc: 0.8755 - val_loss: 0.0870 - val_acc: 0.8743\n",
      "Epoch 75/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0838 - acc: 0.8804 - val_loss: 0.0898 - val_acc: 0.8796\n",
      "Epoch 76/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0852 - acc: 0.8716 - val_loss: 0.0882 - val_acc: 0.8770\n",
      "Epoch 77/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0842 - acc: 0.8755 - val_loss: 0.0894 - val_acc: 0.8796\n",
      "Epoch 78/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0814 - acc: 0.8781 - val_loss: 0.0938 - val_acc: 0.8796\n",
      "Epoch 79/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0836 - acc: 0.8797 - val_loss: 0.0922 - val_acc: 0.8770\n",
      "Epoch 80/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0812 - acc: 0.8797 - val_loss: 0.0885 - val_acc: 0.8796\n",
      "Epoch 81/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0825 - acc: 0.8794 - val_loss: 0.0860 - val_acc: 0.8822\n",
      "Epoch 82/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0845 - acc: 0.8771 - val_loss: 0.0871 - val_acc: 0.8796\n",
      "Epoch 83/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0819 - acc: 0.8804 - val_loss: 0.0865 - val_acc: 0.8861\n",
      "Epoch 84/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0820 - acc: 0.8784 - val_loss: 0.0921 - val_acc: 0.8783\n",
      "Epoch 85/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0806 - acc: 0.8788 - val_loss: 0.0851 - val_acc: 0.8822\n",
      "Epoch 86/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0824 - acc: 0.8846 - val_loss: 0.0859 - val_acc: 0.8809\n",
      "Epoch 87/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0797 - acc: 0.8817 - val_loss: 0.0842 - val_acc: 0.8796\n",
      "Epoch 88/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0817 - acc: 0.8810 - val_loss: 0.0838 - val_acc: 0.8822\n",
      "Epoch 89/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0791 - acc: 0.8824 - val_loss: 0.0849 - val_acc: 0.8822\n",
      "Epoch 90/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0791 - acc: 0.8837 - val_loss: 0.0858 - val_acc: 0.8770\n",
      "Epoch 91/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0799 - acc: 0.8833 - val_loss: 0.0849 - val_acc: 0.8796\n",
      "Epoch 92/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0772 - acc: 0.8886 - val_loss: 0.0864 - val_acc: 0.8770\n",
      "Epoch 93/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0807 - acc: 0.8840 - val_loss: 0.0851 - val_acc: 0.8796\n",
      "Epoch 94/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0780 - acc: 0.8859 - val_loss: 0.0842 - val_acc: 0.8822\n",
      "Epoch 95/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0791 - acc: 0.8824 - val_loss: 0.0843 - val_acc: 0.8809\n",
      "Epoch 96/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0765 - acc: 0.8889 - val_loss: 0.0879 - val_acc: 0.8796\n",
      "Epoch 97/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0785 - acc: 0.8837 - val_loss: 0.0840 - val_acc: 0.8874\n",
      "Epoch 98/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0780 - acc: 0.8797 - val_loss: 0.0839 - val_acc: 0.8835\n",
      "Epoch 99/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0770 - acc: 0.8902 - val_loss: 0.0859 - val_acc: 0.8822\n",
      "Epoch 100/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0782 - acc: 0.8873 - val_loss: 0.0831 - val_acc: 0.8901\n",
      "Epoch 101/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0755 - acc: 0.8908 - val_loss: 0.0838 - val_acc: 0.8809\n",
      "Epoch 102/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0752 - acc: 0.8899 - val_loss: 0.0837 - val_acc: 0.8796\n",
      "Epoch 103/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0777 - acc: 0.8846 - val_loss: 0.0831 - val_acc: 0.8874\n",
      "Epoch 104/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0747 - acc: 0.8935 - val_loss: 0.0839 - val_acc: 0.8848\n",
      "Epoch 105/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0774 - acc: 0.8869 - val_loss: 0.0834 - val_acc: 0.8874\n",
      "Epoch 106/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0746 - acc: 0.8892 - val_loss: 0.0824 - val_acc: 0.8848\n",
      "Epoch 107/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0749 - acc: 0.8889 - val_loss: 0.0819 - val_acc: 0.8927\n",
      "Epoch 108/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0733 - acc: 0.8918 - val_loss: 0.0832 - val_acc: 0.8822\n",
      "Epoch 109/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0735 - acc: 0.8961 - val_loss: 0.0819 - val_acc: 0.8822\n",
      "Epoch 110/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0733 - acc: 0.8938 - val_loss: 0.0818 - val_acc: 0.8835\n",
      "Epoch 111/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0723 - acc: 0.8980 - val_loss: 0.0817 - val_acc: 0.8861\n",
      "Epoch 112/600\n",
      "3060/3060 [==============================] - 38s 13ms/step - loss: 0.0741 - acc: 0.8882 - val_loss: 0.0839 - val_acc: 0.8874\n",
      "Epoch 113/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0737 - acc: 0.8922 - val_loss: 0.0842 - val_acc: 0.8927\n",
      "Epoch 114/600\n",
      "3060/3060 [==============================] - 39s 13ms/step - loss: 0.0724 - acc: 0.8961 - val_loss: 0.0810 - val_acc: 0.8887\n",
      "Epoch 115/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0722 - acc: 0.8961 - val_loss: 0.0831 - val_acc: 0.8848\n",
      "Epoch 116/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0724 - acc: 0.8954 - val_loss: 0.0810 - val_acc: 0.8861\n",
      "Epoch 117/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0734 - acc: 0.8928 - val_loss: 0.0805 - val_acc: 0.8940\n",
      "Epoch 118/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0748 - acc: 0.8925 - val_loss: 0.0804 - val_acc: 0.8927\n",
      "Epoch 119/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0715 - acc: 0.8977 - val_loss: 0.0814 - val_acc: 0.8848\n",
      "Epoch 120/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0747 - acc: 0.8895 - val_loss: 0.0805 - val_acc: 0.8901\n",
      "Epoch 121/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0715 - acc: 0.8912 - val_loss: 0.0865 - val_acc: 0.8822\n",
      "Epoch 122/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0691 - acc: 0.9013 - val_loss: 0.0811 - val_acc: 0.8887\n",
      "Epoch 123/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0732 - acc: 0.8931 - val_loss: 0.0836 - val_acc: 0.8861\n",
      "Epoch 124/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0714 - acc: 0.8961 - val_loss: 0.0801 - val_acc: 0.8914\n",
      "Epoch 125/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0703 - acc: 0.8993 - val_loss: 0.0809 - val_acc: 0.8914\n",
      "Epoch 126/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0719 - acc: 0.8931 - val_loss: 0.0821 - val_acc: 0.8835\n",
      "Epoch 127/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0705 - acc: 0.8951 - val_loss: 0.0813 - val_acc: 0.8848\n",
      "Epoch 128/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0721 - acc: 0.8935 - val_loss: 0.0801 - val_acc: 0.8953\n",
      "Epoch 129/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0678 - acc: 0.9010 - val_loss: 0.0810 - val_acc: 0.8953\n",
      "Epoch 130/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0700 - acc: 0.8961 - val_loss: 0.0796 - val_acc: 0.8992\n",
      "Epoch 131/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0713 - acc: 0.8990 - val_loss: 0.0782 - val_acc: 0.8940\n",
      "Epoch 132/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0720 - acc: 0.8928 - val_loss: 0.0790 - val_acc: 0.8979\n",
      "Epoch 133/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0715 - acc: 0.8967 - val_loss: 0.0799 - val_acc: 0.8914\n",
      "Epoch 134/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0695 - acc: 0.9000 - val_loss: 0.0816 - val_acc: 0.8861\n",
      "Epoch 135/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0709 - acc: 0.8993 - val_loss: 0.0813 - val_acc: 0.8835\n",
      "Epoch 136/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0690 - acc: 0.9013 - val_loss: 0.0786 - val_acc: 0.8914\n",
      "Epoch 137/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0695 - acc: 0.8971 - val_loss: 0.0795 - val_acc: 0.8940\n",
      "Epoch 138/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0674 - acc: 0.9000 - val_loss: 0.0789 - val_acc: 0.8992\n",
      "Epoch 139/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0699 - acc: 0.8958 - val_loss: 0.0790 - val_acc: 0.8887\n",
      "Epoch 140/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0669 - acc: 0.9029 - val_loss: 0.0788 - val_acc: 0.8901\n",
      "Epoch 141/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0681 - acc: 0.9026 - val_loss: 0.0838 - val_acc: 0.8848\n",
      "Epoch 142/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0665 - acc: 0.9036 - val_loss: 0.0771 - val_acc: 0.8992\n",
      "Epoch 143/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0686 - acc: 0.8980 - val_loss: 0.0782 - val_acc: 0.8901\n",
      "Epoch 144/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0671 - acc: 0.9010 - val_loss: 0.0823 - val_acc: 0.8874\n",
      "Epoch 145/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0672 - acc: 0.9065 - val_loss: 0.0798 - val_acc: 0.8861\n",
      "Epoch 146/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0672 - acc: 0.9010 - val_loss: 0.0795 - val_acc: 0.8914\n",
      "Epoch 147/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0659 - acc: 0.8997 - val_loss: 0.0816 - val_acc: 0.8887\n",
      "Epoch 148/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0666 - acc: 0.9007 - val_loss: 0.0793 - val_acc: 0.8901\n",
      "Epoch 149/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0658 - acc: 0.9036 - val_loss: 0.0792 - val_acc: 0.8953\n",
      "Epoch 150/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0648 - acc: 0.9056 - val_loss: 0.0785 - val_acc: 0.8914\n",
      "Epoch 151/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0661 - acc: 0.9042 - val_loss: 0.0800 - val_acc: 0.8914\n",
      "Epoch 152/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0646 - acc: 0.9020 - val_loss: 0.0782 - val_acc: 0.8901\n",
      "Epoch 153/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0669 - acc: 0.9016 - val_loss: 0.0835 - val_acc: 0.8874\n",
      "Epoch 154/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0620 - acc: 0.9092 - val_loss: 0.0788 - val_acc: 0.8927\n",
      "Epoch 155/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0657 - acc: 0.9039 - val_loss: 0.0798 - val_acc: 0.8887\n",
      "Epoch 156/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0681 - acc: 0.8961 - val_loss: 0.0759 - val_acc: 0.8992\n",
      "Epoch 157/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0630 - acc: 0.9101 - val_loss: 0.0783 - val_acc: 0.8953\n",
      "Epoch 158/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0642 - acc: 0.9069 - val_loss: 0.0761 - val_acc: 0.8966\n",
      "Epoch 159/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0630 - acc: 0.9065 - val_loss: 0.0787 - val_acc: 0.8927\n",
      "Epoch 160/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0656 - acc: 0.9052 - val_loss: 0.0797 - val_acc: 0.8940\n",
      "Epoch 161/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0657 - acc: 0.9039 - val_loss: 0.0773 - val_acc: 0.8979\n",
      "Epoch 162/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0654 - acc: 0.9036 - val_loss: 0.0780 - val_acc: 0.8914\n",
      "Epoch 163/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0661 - acc: 0.9059 - val_loss: 0.0802 - val_acc: 0.8901\n",
      "Epoch 164/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0635 - acc: 0.9062 - val_loss: 0.0769 - val_acc: 0.8966\n",
      "Epoch 165/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0634 - acc: 0.9092 - val_loss: 0.0770 - val_acc: 0.8940\n",
      "Epoch 166/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0635 - acc: 0.9059 - val_loss: 0.0754 - val_acc: 0.9031\n",
      "Epoch 167/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0641 - acc: 0.9075 - val_loss: 0.0759 - val_acc: 0.9045\n",
      "Epoch 168/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0629 - acc: 0.9065 - val_loss: 0.0750 - val_acc: 0.9031\n",
      "Epoch 169/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0614 - acc: 0.9121 - val_loss: 0.0750 - val_acc: 0.9058\n",
      "Epoch 170/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0642 - acc: 0.9036 - val_loss: 0.0783 - val_acc: 0.8940\n",
      "Epoch 171/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0608 - acc: 0.9072 - val_loss: 0.0757 - val_acc: 0.9031\n",
      "Epoch 172/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0617 - acc: 0.9127 - val_loss: 0.0756 - val_acc: 0.9005\n",
      "Epoch 173/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0637 - acc: 0.9085 - val_loss: 0.0753 - val_acc: 0.9005\n",
      "Epoch 174/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0624 - acc: 0.9105 - val_loss: 0.0766 - val_acc: 0.9018\n",
      "Epoch 175/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0641 - acc: 0.9056 - val_loss: 0.0746 - val_acc: 0.9045\n",
      "Epoch 176/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0599 - acc: 0.9141 - val_loss: 0.0815 - val_acc: 0.8861\n",
      "Epoch 177/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0594 - acc: 0.9134 - val_loss: 0.0762 - val_acc: 0.8966\n",
      "Epoch 178/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0600 - acc: 0.9118 - val_loss: 0.0752 - val_acc: 0.9005\n",
      "Epoch 179/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0613 - acc: 0.9121 - val_loss: 0.0784 - val_acc: 0.8914\n",
      "Epoch 180/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0623 - acc: 0.9092 - val_loss: 0.0743 - val_acc: 0.9058\n",
      "Epoch 181/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0633 - acc: 0.9078 - val_loss: 0.0757 - val_acc: 0.8992\n",
      "Epoch 182/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0619 - acc: 0.9065 - val_loss: 0.0824 - val_acc: 0.8809\n",
      "Epoch 183/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0607 - acc: 0.9150 - val_loss: 0.0742 - val_acc: 0.8992\n",
      "Epoch 184/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0605 - acc: 0.9121 - val_loss: 0.0743 - val_acc: 0.9005\n",
      "Epoch 185/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0605 - acc: 0.9147 - val_loss: 0.0752 - val_acc: 0.9045\n",
      "Epoch 186/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0587 - acc: 0.9137 - val_loss: 0.0748 - val_acc: 0.9005\n",
      "Epoch 187/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0598 - acc: 0.9144 - val_loss: 0.0737 - val_acc: 0.9031\n",
      "Epoch 188/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0564 - acc: 0.9212 - val_loss: 0.0742 - val_acc: 0.9045\n",
      "Epoch 189/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0588 - acc: 0.9180 - val_loss: 0.0788 - val_acc: 0.8940\n",
      "Epoch 190/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0605 - acc: 0.9098 - val_loss: 0.0753 - val_acc: 0.8966\n",
      "Epoch 191/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0598 - acc: 0.9147 - val_loss: 0.0737 - val_acc: 0.9058\n",
      "Epoch 192/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0570 - acc: 0.9203 - val_loss: 0.0763 - val_acc: 0.9005\n",
      "Epoch 193/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0583 - acc: 0.9180 - val_loss: 0.0782 - val_acc: 0.8953\n",
      "Epoch 194/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0593 - acc: 0.9111 - val_loss: 0.0758 - val_acc: 0.8979\n",
      "Epoch 195/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0585 - acc: 0.9147 - val_loss: 0.0737 - val_acc: 0.9045\n",
      "Epoch 196/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0582 - acc: 0.9150 - val_loss: 0.0745 - val_acc: 0.9031\n",
      "Epoch 197/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0586 - acc: 0.9160 - val_loss: 0.0766 - val_acc: 0.8940\n",
      "Epoch 198/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0556 - acc: 0.9206 - val_loss: 0.0733 - val_acc: 0.9031\n",
      "Epoch 199/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0574 - acc: 0.9216 - val_loss: 0.0764 - val_acc: 0.8953\n",
      "Epoch 200/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0576 - acc: 0.9160 - val_loss: 0.0757 - val_acc: 0.9058\n",
      "Epoch 201/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0563 - acc: 0.9180 - val_loss: 0.0753 - val_acc: 0.8979\n",
      "Epoch 202/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0570 - acc: 0.9170 - val_loss: 0.0759 - val_acc: 0.9018\n",
      "Epoch 203/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0563 - acc: 0.9203 - val_loss: 0.0730 - val_acc: 0.9058\n",
      "Epoch 204/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0554 - acc: 0.9196 - val_loss: 0.0749 - val_acc: 0.9005\n",
      "Epoch 205/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0566 - acc: 0.9193 - val_loss: 0.0763 - val_acc: 0.8979\n",
      "Epoch 206/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0587 - acc: 0.9131 - val_loss: 0.0748 - val_acc: 0.9045\n",
      "Epoch 207/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0563 - acc: 0.9186 - val_loss: 0.0732 - val_acc: 0.9058\n",
      "Epoch 208/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0578 - acc: 0.9154 - val_loss: 0.0767 - val_acc: 0.8992\n",
      "Epoch 209/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0602 - acc: 0.9131 - val_loss: 0.0750 - val_acc: 0.9018\n",
      "Epoch 210/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0589 - acc: 0.9141 - val_loss: 0.0763 - val_acc: 0.8966\n",
      "Epoch 211/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0544 - acc: 0.9203 - val_loss: 0.0726 - val_acc: 0.9045\n",
      "Epoch 212/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0559 - acc: 0.9183 - val_loss: 0.0731 - val_acc: 0.9058\n",
      "Epoch 213/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0579 - acc: 0.9163 - val_loss: 0.0730 - val_acc: 0.9084\n",
      "Epoch 214/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0532 - acc: 0.9271 - val_loss: 0.0781 - val_acc: 0.8979\n",
      "Epoch 215/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0567 - acc: 0.9186 - val_loss: 0.0751 - val_acc: 0.9031\n",
      "Epoch 216/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0552 - acc: 0.9209 - val_loss: 0.0756 - val_acc: 0.8953\n",
      "Epoch 217/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0567 - acc: 0.9167 - val_loss: 0.0761 - val_acc: 0.9018\n",
      "Epoch 218/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0556 - acc: 0.9157 - val_loss: 0.0743 - val_acc: 0.9045\n",
      "Epoch 219/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0540 - acc: 0.9225 - val_loss: 0.0729 - val_acc: 0.9058\n",
      "Epoch 220/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0560 - acc: 0.9193 - val_loss: 0.0740 - val_acc: 0.9031\n",
      "Epoch 221/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0562 - acc: 0.9183 - val_loss: 0.0724 - val_acc: 0.9058\n",
      "Epoch 222/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0539 - acc: 0.9245 - val_loss: 0.0728 - val_acc: 0.9045\n",
      "Epoch 223/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0558 - acc: 0.9196 - val_loss: 0.0734 - val_acc: 0.9031\n",
      "Epoch 224/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0540 - acc: 0.9206 - val_loss: 0.0715 - val_acc: 0.9110\n",
      "Epoch 225/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0544 - acc: 0.9199 - val_loss: 0.0774 - val_acc: 0.8927\n",
      "Epoch 226/600\n",
      "3060/3060 [==============================] - 38s 13ms/step - loss: 0.0552 - acc: 0.9180 - val_loss: 0.0734 - val_acc: 0.9031\n",
      "Epoch 227/600\n",
      "3060/3060 [==============================] - 39s 13ms/step - loss: 0.0543 - acc: 0.9222 - val_loss: 0.0716 - val_acc: 0.9071\n",
      "Epoch 228/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0513 - acc: 0.9229 - val_loss: 0.0750 - val_acc: 0.8992\n",
      "Epoch 229/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0531 - acc: 0.9242 - val_loss: 0.0745 - val_acc: 0.9058\n",
      "Epoch 230/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0553 - acc: 0.9209 - val_loss: 0.0735 - val_acc: 0.9071\n",
      "Epoch 231/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0535 - acc: 0.9248 - val_loss: 0.0747 - val_acc: 0.9045\n",
      "Epoch 232/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0501 - acc: 0.9327 - val_loss: 0.0726 - val_acc: 0.9097\n",
      "Epoch 233/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0521 - acc: 0.9278 - val_loss: 0.0747 - val_acc: 0.9084\n",
      "Epoch 234/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0559 - acc: 0.9199 - val_loss: 0.0730 - val_acc: 0.9084\n",
      "Epoch 235/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0542 - acc: 0.9235 - val_loss: 0.0724 - val_acc: 0.9071\n",
      "Epoch 236/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0555 - acc: 0.9190 - val_loss: 0.0740 - val_acc: 0.9058\n",
      "Epoch 237/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0518 - acc: 0.9216 - val_loss: 0.0745 - val_acc: 0.9097\n",
      "Epoch 238/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0532 - acc: 0.9235 - val_loss: 0.0717 - val_acc: 0.9097\n",
      "Epoch 239/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0511 - acc: 0.9261 - val_loss: 0.0728 - val_acc: 0.9045\n",
      "Epoch 240/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0528 - acc: 0.9245 - val_loss: 0.0741 - val_acc: 0.9031\n",
      "Epoch 241/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0524 - acc: 0.9258 - val_loss: 0.0734 - val_acc: 0.9071\n",
      "Epoch 242/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0536 - acc: 0.9225 - val_loss: 0.0725 - val_acc: 0.9071\n",
      "Epoch 243/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0520 - acc: 0.9255 - val_loss: 0.0726 - val_acc: 0.9071\n",
      "Epoch 244/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0525 - acc: 0.9258 - val_loss: 0.0716 - val_acc: 0.9084\n",
      "Epoch 245/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0538 - acc: 0.9222 - val_loss: 0.0710 - val_acc: 0.9084\n",
      "Epoch 246/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0519 - acc: 0.9265 - val_loss: 0.0736 - val_acc: 0.9110\n",
      "Epoch 247/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0533 - acc: 0.9196 - val_loss: 0.0720 - val_acc: 0.9097\n",
      "Epoch 248/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0519 - acc: 0.9248 - val_loss: 0.0732 - val_acc: 0.9045\n",
      "Epoch 249/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0517 - acc: 0.9242 - val_loss: 0.0735 - val_acc: 0.9045\n",
      "Epoch 250/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0499 - acc: 0.9278 - val_loss: 0.0715 - val_acc: 0.9136\n",
      "Epoch 251/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0542 - acc: 0.9232 - val_loss: 0.0728 - val_acc: 0.9149\n",
      "Epoch 252/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0513 - acc: 0.9271 - val_loss: 0.0714 - val_acc: 0.9123\n",
      "Epoch 253/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0501 - acc: 0.9278 - val_loss: 0.0739 - val_acc: 0.9058\n",
      "Epoch 254/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0512 - acc: 0.9291 - val_loss: 0.0712 - val_acc: 0.9071\n",
      "Epoch 255/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0518 - acc: 0.9222 - val_loss: 0.0710 - val_acc: 0.9097\n",
      "Epoch 256/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0513 - acc: 0.9265 - val_loss: 0.0716 - val_acc: 0.9071\n",
      "Epoch 257/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0495 - acc: 0.9317 - val_loss: 0.0714 - val_acc: 0.9058\n",
      "Epoch 258/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0491 - acc: 0.9271 - val_loss: 0.0734 - val_acc: 0.9031\n",
      "Epoch 259/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0495 - acc: 0.9304 - val_loss: 0.0726 - val_acc: 0.9084\n",
      "Epoch 260/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0491 - acc: 0.9327 - val_loss: 0.0723 - val_acc: 0.9097\n",
      "Epoch 261/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0499 - acc: 0.9271 - val_loss: 0.0733 - val_acc: 0.9071\n",
      "Epoch 262/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0505 - acc: 0.9304 - val_loss: 0.0752 - val_acc: 0.9031\n",
      "Epoch 263/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0504 - acc: 0.9281 - val_loss: 0.0716 - val_acc: 0.9097\n",
      "Epoch 264/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0498 - acc: 0.9245 - val_loss: 0.0717 - val_acc: 0.9058\n",
      "Epoch 265/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0511 - acc: 0.9261 - val_loss: 0.0733 - val_acc: 0.9097\n",
      "Epoch 266/600\n",
      "3060/3060 [==============================] - 35s 12ms/step - loss: 0.0501 - acc: 0.9275 - val_loss: 0.0751 - val_acc: 0.9031\n",
      "Epoch 267/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0527 - acc: 0.9255 - val_loss: 0.0734 - val_acc: 0.9110\n",
      "Epoch 268/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0518 - acc: 0.9242 - val_loss: 0.0710 - val_acc: 0.9110\n",
      "Epoch 269/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0487 - acc: 0.9291 - val_loss: 0.0736 - val_acc: 0.9045\n",
      "Epoch 270/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0506 - acc: 0.9301 - val_loss: 0.0731 - val_acc: 0.9071\n",
      "Epoch 271/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0497 - acc: 0.9284 - val_loss: 0.0717 - val_acc: 0.9071\n",
      "Epoch 272/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0498 - acc: 0.9271 - val_loss: 0.0722 - val_acc: 0.9084\n",
      "Epoch 273/600\n",
      "3060/3060 [==============================] - 34s 11ms/step - loss: 0.0499 - acc: 0.9268 - val_loss: 0.0741 - val_acc: 0.9058\n",
      "Epoch 274/600\n",
      "3060/3060 [==============================] - 53s 17ms/step - loss: 0.0492 - acc: 0.9288 - val_loss: 0.0713 - val_acc: 0.9097\n",
      "Epoch 275/600\n",
      "3060/3060 [==============================] - 51s 17ms/step - loss: 0.0488 - acc: 0.9297 - val_loss: 0.0739 - val_acc: 0.9031\n",
      "Epoch 276/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0477 - acc: 0.9304 - val_loss: 0.0714 - val_acc: 0.9123\n",
      "Epoch 277/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0478 - acc: 0.9337 - val_loss: 0.0732 - val_acc: 0.9058\n",
      "Epoch 278/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0481 - acc: 0.9340 - val_loss: 0.0737 - val_acc: 0.9058\n",
      "Epoch 279/600\n",
      "3060/3060 [==============================] - 38s 13ms/step - loss: 0.0485 - acc: 0.9317 - val_loss: 0.0726 - val_acc: 0.9084\n",
      "Epoch 280/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0494 - acc: 0.9288 - val_loss: 0.0701 - val_acc: 0.9136\n",
      "Epoch 281/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0478 - acc: 0.9304 - val_loss: 0.0702 - val_acc: 0.9084\n",
      "Epoch 282/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0509 - acc: 0.9278 - val_loss: 0.0700 - val_acc: 0.9071\n",
      "Epoch 283/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0502 - acc: 0.9232 - val_loss: 0.0703 - val_acc: 0.9097\n",
      "Epoch 284/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0476 - acc: 0.9297 - val_loss: 0.0711 - val_acc: 0.9005\n",
      "Epoch 285/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0482 - acc: 0.9333 - val_loss: 0.0762 - val_acc: 0.8979\n",
      "Epoch 286/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0482 - acc: 0.9294 - val_loss: 0.0728 - val_acc: 0.9045\n",
      "Epoch 287/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0476 - acc: 0.9288 - val_loss: 0.0718 - val_acc: 0.9123\n",
      "Epoch 288/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0472 - acc: 0.9320 - val_loss: 0.0702 - val_acc: 0.9110\n",
      "Epoch 289/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0467 - acc: 0.9343 - val_loss: 0.0721 - val_acc: 0.9084\n",
      "Epoch 290/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0471 - acc: 0.9307 - val_loss: 0.0725 - val_acc: 0.9058\n",
      "Epoch 291/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0488 - acc: 0.9304 - val_loss: 0.0705 - val_acc: 0.9136\n",
      "Epoch 292/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0451 - acc: 0.9353 - val_loss: 0.0702 - val_acc: 0.9097\n",
      "Epoch 293/600\n",
      "3060/3060 [==============================] - 35s 11ms/step - loss: 0.0483 - acc: 0.9278 - val_loss: 0.0724 - val_acc: 0.9018\n",
      "Epoch 294/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0451 - acc: 0.9359 - val_loss: 0.0728 - val_acc: 0.9045\n",
      "Epoch 295/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0462 - acc: 0.9333 - val_loss: 0.0708 - val_acc: 0.8992\n",
      "Epoch 296/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0483 - acc: 0.9271 - val_loss: 0.0734 - val_acc: 0.9110\n",
      "Epoch 297/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0480 - acc: 0.9284 - val_loss: 0.0740 - val_acc: 0.9084\n",
      "Epoch 298/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0456 - acc: 0.9363 - val_loss: 0.0742 - val_acc: 0.9097\n",
      "Epoch 299/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0460 - acc: 0.9346 - val_loss: 0.0711 - val_acc: 0.9045\n",
      "Epoch 300/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0461 - acc: 0.9346 - val_loss: 0.0723 - val_acc: 0.9071\n",
      "Epoch 301/600\n",
      "3060/3060 [==============================] - 38s 12ms/step - loss: 0.0463 - acc: 0.9379 - val_loss: 0.0707 - val_acc: 0.9071\n",
      "Epoch 302/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0474 - acc: 0.9317 - val_loss: 0.0703 - val_acc: 0.9084\n",
      "Epoch 303/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0478 - acc: 0.9304 - val_loss: 0.0704 - val_acc: 0.9110\n",
      "Epoch 304/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0452 - acc: 0.9350 - val_loss: 0.0722 - val_acc: 0.9110\n",
      "Epoch 305/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0451 - acc: 0.9376 - val_loss: 0.0714 - val_acc: 0.9071\n",
      "Epoch 306/600\n",
      "3060/3060 [==============================] - 36s 12ms/step - loss: 0.0455 - acc: 0.9382 - val_loss: 0.0704 - val_acc: 0.9136\n",
      "Epoch 307/600\n",
      "3060/3060 [==============================] - 37s 12ms/step - loss: 0.0459 - acc: 0.9353 - val_loss: 0.0706 - val_acc: 0.9162\n",
      "764/764 [==============================] - 4s 5ms/step\n",
      "Average accuracy: 0.9032 +/- 0.0126\n",
      "Average sensitivity: 0.8893 +/- 0.0155\n",
      "Average specificity: 0.9143 +/- 0.0230\n",
      "Average MCC: 0.8069 +/- 0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadik/.local/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_scores=[]\n",
    "all_preds=[]\n",
    "conf_mat=[]\n",
    "reports=[]\n",
    "roc=[]\n",
    "roc_auc=[]\n",
    "tprs=[]\n",
    "acc_scores=[]\n",
    "sens_scores=[]\n",
    "spec_scores=[]\n",
    "mcc_scores=[]\n",
    "\n",
    "for i,(train_index,test_index) in enumerate(kf.split(training_data)):\n",
    "    x_train, x_test=training_data[train_index],training_data[test_index]\n",
    "    y_train, y_test=training_label[train_index],training_label[test_index]\n",
    "    y_train=np.asarray(y_train)\n",
    "    mode1= CapsNet(input_shape=(55,20),n_class=2,num_routing=3)\n",
    "    mode1.compile(optimizers.RMSprop(lr=0.0001),loss=margin_loss,metrics=['acc'])\n",
    "    call=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=25)]\n",
    "    history=mode1.fit(x=x_train,y=y_train,batch_size=128,epochs=600,validation_data=[x_test,y_test], callbacks=call)\n",
    "    mode1.save('./TCap'+str(i)+'Lastepoch_new.h5') # save your model\n",
    "    plt.plot(history.history['loss'],label='train'+str(i))\n",
    "    scores=mode1.evaluate(x_test,y_test)\n",
    "    all_scores.append(scores)\n",
    "    pred=mode1.predict(x_test)\n",
    "    all_preds.append(pred)\n",
    "    y_prediction=np.argmax(pred,1)\n",
    "    y_true=np.argmax(y_test,1)\n",
    "    conf= metrics.confusion_matrix(y_true,y_prediction)\n",
    "    conf_mat.append(conf)\n",
    "    report=metrics.classification_report(y_true,y_prediction)\n",
    "    reports.append(report)\n",
    "    fpr,tpr,_=metrics.roc_curve(y_true=y_test[:,1],y_score=pred[:,1])\n",
    "    roc.append([fpr,tpr])\n",
    "    roc_auc.append(metrics.auc(fpr,tpr))\n",
    "    tpr=interp(base_fpr,fpr,tpr)\n",
    "    tpr[0]=0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    # calculate metrics\n",
    "    acc_scores.append(metrics.accuracy_score(y_true, y_prediction))\n",
    "    sens_scores.append(metrics.recall_score(y_true, y_prediction))\n",
    "    spec_scores.append(metrics.precision_score(y_true, y_prediction))\n",
    "    mcc_scores.append(metrics.matthews_corrcoef(y_true, y_prediction))\n",
    "\n",
    "# calculate mean and standard deviation of metrics\n",
    "mean_acc = np.mean(acc_scores)\n",
    "std_acc = np.std(acc_scores)\n",
    "mean_sens = np.mean(sens_scores)\n",
    "std_sens = np.std(sens_scores)\n",
    "mean_spec = np.mean(spec_scores)\n",
    "std_spec = np.std(spec_scores)\n",
    "mean_mcc = np.mean(mcc_scores)\n",
    "std_mcc = np.std(mcc_scores)\n",
    "\n",
    "print(f\"Average accuracy: {mean_acc:.4f} +/- {std_acc:.4f}\")\n",
    "print(f\"Average sensitivity: {mean_sens:.4f} +/- {std_sens:.4f}\")\n",
    "print(f\"Average specificity: {mean_spec:.4f} +/- {std_spec:.4f}\")\n",
    "print(f\"Average MCC: {mean_mcc:.4f} +/- {std_mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab8f9e46-b33b-4e1a-b475-dd3bd5e566fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fprs=[]\n",
    "for i,[fpr,tpr] in enumerate(roc):\n",
    "    fprs.append(fpr)\n",
    "    plt.plot(fpr,tpr,label='ROC fold {} (AUC = {:.4f})'.format(i,roc_auc[i]),lw=1,alpha=0.3)\n",
    "plt.plot(base_fpr, np.average(tprs,axis=0),\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (np.mean(roc_auc),np.std(roc_auc)),lw=1,alpha=.8,color='b')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, alpha=.8,color='c')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate',fontweight='bold')\n",
    "plt.ylabel('True Positive Rate',fontweight='bold')\n",
    "plt.savefig( 'five_fold_roc_capsulnet_lstm.pdf') # use your own dictionay \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4a484c-55e9-4812-9b65-afb8cb4fa260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGwCAYAAAB7MGXBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbMklEQVR4nO3deXxU1f3/8dfMJJns+x5Cwr5vsomIoERBrTsVla8CVbRVqy3ab+VrC2p/FbdSqqK0WqVaq6h1X3CJgKLsi+z7EpYsBMhC9szc3x+HTIiAJhC4wLyfj8c8kszcuXPmEpg353zOOQ7LsixEREREbOK0uwEiIiLi3xRGRERExFYKIyIiImIrhRERERGxlcKIiIiI2EphRERERGylMCIiIiK2CrC7AY3h9XrZs2cPEREROBwOu5sjIiIijWBZFqWlpaSmpuJ0Hrv/44wII3v27CE9Pd3uZoiIiMhx2LlzJy1atDjm42dEGImIiADMm4mMjLS5NSIiItIYJSUlpKen+z7Hj+WMCCN1QzORkZEKIyIiImeYnyqxUAGriIiI2EphRERERGylMCIiIiK2OiNqRkRERE4Gj8dDTU2N3c04YwUGBuJyuU74PAojIiLidyzLIi8vj6KiIrubcsaLjo4mOTn5hNYBUxgRERG/UxdEEhMTCQ0N1YKax8GyLMrLyykoKAAgJSXluM+lMCIiIn7F4/H4gkhcXJzdzTmjhYSEAFBQUEBiYuJxD9mogFVERPxKXY1IaGiozS05O9RdxxOpvVEYERERv6ShmebRHNdRYURERERspTAiIiIitlIYERER8WOZmZlMnTrV1jb4dRgpKK1k5/5yKqo9djdFRETkRzkcjh+9PfTQQ8d13sWLF3P77bc3b2ObyK+n9t7x6lKW5xTxwi19uLhzkt3NEREROabc3Fzf9zNnzmTixIls2LDBd194eLjve8uy8Hg8BAT89Md8QkJC8zb0OPh1z0hd/a/Xsmxth4iI2MuyLMqra225WY38DEpOTvbdoqKicDgcvp/Xr19PREQEn376Kb1798btdjNv3jy2bNnCVVddRVJSEuHh4fTt25cvv/yywXl/OEzjcDh48cUXueaaawgNDaVdu3Z88MEHzXm5j+DXPSPOQ9ORlEVERPxbRY2HzhM/s+W11z4yjNCg5vk4fuCBB3jqqado3bo1MTEx7Ny5k8suu4w///nPuN1uXnnlFa644go2bNhAy5Ytj3mehx9+mCeeeIInn3ySZ555hlGjRrFjxw5iY2ObpZ0/5Nc9I/VhRGlERETOfI888ggXX3wxbdq0ITY2lh49enDHHXfQtWtX2rVrx5/+9CfatGnzkz0dY8aM4cYbb6Rt27Y8+uijHDx4kEWLFp20dvt1z0jdOi1eZREREb8WEuhi7SPDbHvt5tKnT58GPx88eJCHHnqIjz/+mNzcXGpra6moqCAnJ+dHz9O9e3ff92FhYURGRvr2oDkZ/DqM1PWMqGZERMS/ORyOZhsqsVNYWFiDn++//36++OILnnrqKdq2bUtISAgjRoygurr6R88TGBjY4GeHw4HX62329tY586/8CXAeGqRSGBERkbPRt99+y5gxY7jmmmsA01Oyfft2ext1FKoZQQWsIiJydmrXrh3vvPMOK1as4Pvvv+emm246qT0cx8uvw0gd9YyIiMjZaMqUKcTExHDeeedxxRVXMGzYMM455xy7m3UE/x6m8dWM2NwQERGRJhgzZgxjxozx/TxkyJCjzgzNzMzkq6++anDfXXfd1eDnHw7bHO08RUVFx93WxvDrnhHnodk0mtorIiJiHz8PI6oZERERsZtfhxGHpvaKiIjYzq/DiFOLnomIiNjOz8OIekZERETs5t9h5NC7VwGriIiIffw6jDjQ1F4RERG7+XcY0dReERER2/l1GNGiZyIiIvbz8zBivqqAVURETncOh+NHbw899NAJnfu9995rtrY2lZaDR4ueiYjI6S83N9f3/cyZM5k4cSIbNmzw3RceHm5Hs5qFX/eMaNEzERE5UyQnJ/tuUVFROByOBve98cYbdOrUieDgYDp27Mhzzz3ne251dTV33303KSkpBAcHk5GRweTJkwGzfw3ANddcg8Ph8P18Kvl5z4j5qpoRERE/Z1lQU27PaweG1s+oOE6vvfYaEydO5Nlnn6VXr14sX76ccePGERYWxujRo3n66af54IMPePPNN2nZsiU7d+5k586dACxevJjExERefvllhg8fjsvlao531SR+HUYcqhkREREwQeTRVHte+//2QFDYCZ1i0qRJ/OUvf+Haa68FoFWrVqxdu5a///3vjB49mpycHNq1a8f555+Pw+EgIyPD99yEhAQAoqOjSU5OPqF2HC+/DiP1NSMKIyIicmYqKytjy5Yt3HrrrYwbN853f21tLVFRUQCMGTOGiy++mA4dOjB8+HB+9rOfcckll9jV5CP4dRhxqIBVRETADJX83x77XvsEHDx4EIAXXniB/v37N3isbsjlnHPOYdu2bXz66ad8+eWXXH/99WRlZfH222+f0Gs3F78OI6oZERERwIzbn+BQiV2SkpJITU1l69atjBo16pjHRUZGMnLkSEaOHMmIESMYPnw4+/fvJzY2lsDAQDwezylsdUN+HkY0m0ZERM58Dz/8MPfccw9RUVEMHz6cqqoqlixZwoEDBxg/fjxTpkwhJSWFXr164XQ6eeutt0hOTiY6OhowM2qys7MZOHAgbrebmJiYU9p+v57a69Ry8CIicha47bbbePHFF3n55Zfp1q0bgwcPZsaMGbRq1QqAiIgInnjiCfr06UPfvn3Zvn07n3zyCc5DO8b+5S9/4YsvviA9PZ1evXqd8vY7rDPgk7ikpISoqCiKi4uJjIxstvM+9MEaZny3nbsvbMv9wzo023lFROT0VVlZybZt22jVqhXBwcF2N+eM92PXs7Gf337eM6JhGhEREbv5dRhxqIBVRETEdn4dRnw1IyiNiIiI2MXPw4jWGREREbGbX4cR30Z5GqcREfE7Z8D8jTNCc1xHvw4jWvRMRMT/BAYGAlBebtPGeGeZuutYd12PhxY9Q7NpRET8icvlIjo6moKCAgBCQ0N9PeXSeJZlUV5eTkFBAdHR0Se026+fhxHzVV11IiL+pW532rpAIsevOXb79eswgq9nxOZ2iIjIKeVwOEhJSSExMZGamhq7m3PGCgwMPKEekTp+HUY0tVdExL+5XK5m+TCVE3NcBazTpk0jMzOT4OBg+vfvz6JFi4557IwZM3A4HA1up8vyu071jIiIiNiuyWFk5syZjB8/nkmTJrFs2TJ69OjBsGHDfnTcLTIyktzcXN9tx44dJ9To5qKaEREREfs1OYxMmTKFcePGMXbsWDp37sz06dMJDQ3lpZdeOuZzHA4HycnJvltSUtIJNbq51K8zYnNDRERE/FiTwkh1dTVLly4lKyur/gROJ1lZWcyfP/+Yzzt48CAZGRmkp6dz1VVXsWbNmh99naqqKkpKShrcTgZN7RUREbFfk8JIYWEhHo/niJ6NpKQk8vLyjvqcDh068NJLL/H+++/z73//G6/Xy3nnnceuXbuO+TqTJ08mKirKd0tPT29KMxtNi56JiIjY76SvwDpgwABuueUWevbsyeDBg3nnnXdISEjg73//+zGfM2HCBIqLi323nTt3npS2OVQzIiIiYrsmTe2Nj4/H5XKRn5/f4P78/PxGL3gSGBhIr1692Lx58zGPcbvduN3upjTtuGiYRkRExH5N6hkJCgqid+/eZGdn++7zer1kZ2czYMCARp3D4/GwatUqUlJSmtbSk6CugFVRRERExD5NXvRs/PjxjB49mj59+tCvXz+mTp1KWVkZY8eOBeCWW24hLS2NyZMnA/DII49w7rnn0rZtW4qKinjyySfZsWMHt912W/O+k+OgmhERERH7NTmMjBw5kr179zJx4kTy8vLo2bMns2bN8hW15uTk4HTWd7gcOHCAcePGkZeXR0xMDL179+a7776jc+fOzfcujpOGaUREROznsM6A6s2SkhKioqIoLi4mMjKy2c776vzt/PH9NVzWLZnnRvVutvOKiIhI4z+/T/psmtOZFj0TERGxn1+HEQ3TiIiI2M+vw4hDBawiIiK28+swUjebRpN7RURE7OPXYcRXM6IsIiIiYhu/DiOqGREREbGfn4cR81U9IyIiIvbx8zByaDl49YyIiIjYxq/DSP1sGoURERERu/h5GNGiZyIiInbz6zDiVM+IiIiI7fw8jByqGbG5HSIiIv7Mz8OI+aoCVhEREfv4dRjRomciIiL28+swokXPRERE7OfnYcR8Vc+IiIiIffw8jGjRMxEREbv5dRhBU3tFRERs59dhpL5nxOaGiIiI+DE/DyPmq2pGRERE7OPnYUQ1IyIiInbz6zCijfJERETs59dhxKlFz0RERGynMIJ6RkREROzk12HE4dubxt52iIiI+DO/DiPaKE9ERMR+fh1GtFGeiIiI/fw6jKhmRERExH5+HkbMV2URERER+/h5GFHPiIiIiN38Ooxo0TMRERH7+XcYQQWsIiIidvPrMOI89O41tVdERMQ+/h1GfBvl2dwQERERP+bnYcR8Vc2IiIiIffw6jGjRMxEREfv5dRjR1F4RERH7+XkYMV+VRUREROzj52FEPSMiIiJ28+swUkdhRERExD5+HUacTk3tFRERsZt/hxHVjIiIiNjOz8OIakZERETs5tdhRBvliYiI2M+vw4hTi56JiIjYTmHkEG2WJyIiYg+/DiOOw75X74iIiIg9/DqMHN4zoroRERERe/h1GHEc9u6VRUREROzh12FEPSMiIiL28/MwUv+9soiIiIg9/DyMqGdERETEbn4dRg7LIgojIiIiNvHrMNKwZ8TGhoiIiPix4woj06ZNIzMzk+DgYPr378+iRYsa9bw33ngDh8PB1VdffTwv2+wOX2dEi56JiIjYo8lhZObMmYwfP55JkyaxbNkyevTowbBhwygoKPjR523fvp3777+fQYMGHXdjm1vDFVhtbIiIiIgfa3IYmTJlCuPGjWPs2LF07tyZ6dOnExoayksvvXTM53g8HkaNGsXDDz9M69atf/I1qqqqKCkpaXA7GVQzIiIiYr8mhZHq6mqWLl1KVlZW/QmcTrKyspg/f/4xn/fII4+QmJjIrbfe2qjXmTx5MlFRUb5benp6U5rZaA6H47Cde0/KS4iIiMhPaFIYKSwsxOPxkJSU1OD+pKQk8vLyjvqcefPm8c9//pMXXnih0a8zYcIEiouLfbedO3c2pZlNUjdUo5oRERERewSczJOXlpZy880388ILLxAfH9/o57ndbtxu90lsWT2nAzyoZ0RERMQuTQoj8fHxuFwu8vPzG9yfn59PcnLyEcdv2bKF7du3c8UVV/ju83q95oUDAtiwYQNt2rQ5nnY3G4fDAViqGREREbFJk4ZpgoKC6N27N9nZ2b77vF4v2dnZDBgw4IjjO3bsyKpVq1ixYoXvduWVV3LhhReyYsWKk1YL0hR1NawKIyIiIvZo8jDN+PHjGT16NH369KFfv35MnTqVsrIyxo4dC8Att9xCWloakydPJjg4mK5duzZ4fnR0NMAR99ulvmbE5oaIiIj4qSaHkZEjR7J3714mTpxIXl4ePXv2ZNasWb6i1pycHJzOM2dh17rN8hRGRERE7OGwzoBpJCUlJURFRVFcXExkZGSznrvbpM8oraplzv1DyIwPa9Zzi4iI+LPGfn6fOV0YJ0n9OiOnfSYTERE5K/l9GHEeGqfR1F4RERF7KIxo0TMRERFb+X0YqZ/aa2szRERE/JbCiKNumEZpRERExA5+H0Y0tVdERMReCiPqGREREbGVwoh6RkRERGzl92FENSMiIiL28vswUrdyvcKIiIiIPRRGHFr0TERExE5+H0bq1hnRomciIiL28Psw4luB1eZ2iIiI+Cu/DyO+jfI0TiMiImILvw8jqhkRERGxl8KINsoTERGxld+HEd8wjbKIiIiILfw+jGg5eBEREXv5fRip7xlRGBEREbGD34eR+poRmxsiIiLipxRG6jbK00ojIiIitvD7MOLbKM9rc0NERET8lN+HEadqRkRERGylMKJFz0RERGylMKJFz0RERGzl92EELXomIiJiK78PI6oZERERsZfCSN0wjc3tEBER8VcKI6oZERERsZXfhxEtBy8iImIvvw8jTi16JiIiYiuFEfWMiIiI2EphRBvliYiI2Mrvw4hqRkREROylMKLl4EVERGzl92GkrmbE0kojIiIitlAYUc+IiIiIrRRGtOiZiIiIrfw+jPgKWNU1IiIiYgu/DyMaphEREbGX34cRTe0VERGxl9+HES16JiIiYi+/DyMOTe0VERGxld+HEdWMiIiI2EthRDUjIiIitlIYUc2IiIiIrfw+jPj2ptE4jYiIiC38PozUD9PY2w4RERF/5fdhROuMiIiI2Mvvw4ivZsTmdoiIiPgrhRFtlCciImIrvw8jGqYRERGxl9+HES16JiIiYq/jCiPTpk0jMzOT4OBg+vfvz6JFi4557DvvvEOfPn2Ijo4mLCyMnj178uqrrx53g5ubFj0TERGxV5PDyMyZMxk/fjyTJk1i2bJl9OjRg2HDhlFQUHDU42NjY3nwwQeZP38+K1euZOzYsYwdO5bPPvvshBvfHLTomYiIiL2aHEamTJnCuHHjGDt2LJ07d2b69OmEhoby0ksvHfX4IUOGcM0119CpUyfatGnDvffeS/fu3Zk3b94JN75Z1PWMaJxGRETEFk0KI9XV1SxdupSsrKz6EzidZGVlMX/+/J98vmVZZGdns2HDBi644IJjHldVVUVJSUmD28mimhERERF7NSmMFBYW4vF4SEpKanB/UlISeXl5x3xecXEx4eHhBAUFcfnll/PMM89w8cUXH/P4yZMnExUV5bulp6c3pZlNUlczYmmlEREREVucktk0ERERrFixgsWLF/PnP/+Z8ePHM2fOnGMeP2HCBIqLi323nTt3nrS2qWZERETEXgFNOTg+Ph6Xy0V+fn6D+/Pz80lOTj7m85xOJ23btgWgZ8+erFu3jsmTJzNkyJCjHu92u3G73U1p2nHzbZSnNCIiImKLJvWMBAUF0bt3b7Kzs333eb1esrOzGTBgQKPP4/V6qaqqaspLnzSa2isiImKvJvWMAIwfP57Ro0fTp08f+vXrx9SpUykrK2Ps2LEA3HLLLaSlpTF58mTA1H/06dOHNm3aUFVVxSeffMKrr77K888/37zv5DipgFVERMReTQ4jI0eOZO/evUycOJG8vDx69uzJrFmzfEWtOTk5OJ31HS5lZWXceeed7Nq1i5CQEDp27Mi///1vRo4c2Xzv4gQc6hjR3jQiIiI2cVhnwKdwSUkJUVFRFBcXExkZ2aznnjZ7M09+toGRfdJ5fET3Zj23iIiIP2vs57ff703j0NReERERW/l9GFHNiIiIiL0URjSbRkRExFYKI1r0TERExFZ+H0a06JmIiIi9/D6M1A/T2NsOERERf+X3YaRunRH1jIiIiNjD78OI01lXM6IwIiIiYge/DyMOFbCKiIjYyu/DiKb2ioiI2KvJe9OcVWZPZtCGZXRxDMFrJdndGhEREb/k3z0jW76iZd7ntHDsVc2IiIiITfw7jLgjAAinQlN7RUREbKIwAoQ5KlQzIiIiYhM/DyPhgHpGRERE7OTnYSQSgAhHhWpGREREbOLnYeTQMA2VWmdERETEJv4dRoIODdOoZkRERMQ2/h1GDvWMRKAwIiIiYheFESBMBawiIiK2URjBDNOogFVERMQeCiNoaq+IiIidFEaAcEelakZERERs4t9hJKh+0bOqGq/NjREREfFP/h1GDi16Fuqooryy0ubGiIiI+Cc/DyPhvm9rK0ptbIiIiIj/8u8wEuDGcrnN91WleFXFKiIicsr5dxgBX+9IKBWUVdfa3BgRERH/ozBy2PTe0kqFERERkVPN78OI47CFz0oqa2xujYiIiP/x+zBCUH3PSEmFekZERERONYWRw3tGKtQzIiIicqopjPhqRioprVIYEREROdUURtz1q7BqmEZEROTUUxjRMI2IiIitFEYOLQkfjmbTiIiI2EFhpG6zPIeGaUREROygMHL4omcqYBURETnlFEYa1IyoZ0RERORUUxiJSAEgzVGomhEREREbKIzEtwMgzbGP6vJSmxsjIiLifxRGQmOpDY4FIKYyx+bGiIiI+B+FEcAb2xaApOqdWJZlc2tERET8i8II4Dg0VJPJbiprvDa3RkRExL8ojAABiR0AaO3IVRGriIjIKaYwQn3PSBvHHi0JLyIicoopjADEtwdMz0hxeZXNjREREfEvCiMAMRnU4iLEUc3mLRvtbo2IiIhfURgBcAVSGpYBQN66+TY3RkRExL8ojBziaD0EgLS9X1NR7bG3MSIiIn5EYeSQqJ5XATDEsYz5Wwpsbo2IiIj/UBg5xJFxHpXOMBIcJWxcOsfu5oiIiPgNhZE6AUGUtBgCgHPjx+TsK7e3PSIiIn7iuMLItGnTyMzMJDg4mP79+7No0aJjHvvCCy8waNAgYmJiiImJISsr60ePt1NC3+sAGOP4hP++OUNLw4uIiJwCTQ4jM2fOZPz48UyaNIlly5bRo0cPhg0bRkHB0ess5syZw4033sjs2bOZP38+6enpXHLJJezevfuEG9/cHF2uprTtlQQ5PNyR9xD/eO8LBRIREZGTzGE18dO2f//+9O3bl2effRYAr9dLeno6v/71r3nggQd+8vkej4eYmBieffZZbrnllka9ZklJCVFRURQXFxMZGdmU5jZdbTW50y4l5cASvvZ047tzp/PA8A7gCjy5rysiInKWaeznd5N6Rqqrq1m6dClZWVn1J3A6ycrKYv78xq3PUV5eTk1NDbGxscc8pqqqipKSkga3UyYgiJT/+QceZxAXuFbx60UXUfF4ByhYf+raICIi4keaFEYKCwvxeDwkJSU1uD8pKYm8vLxGneP3v/89qampDQLND02ePJmoqCjfLT09vSnNPHFxbXANGg9AmKOKkOp9VP3rWiht3HsUERGRxjuls2kee+wx3njjDd59912Cg4OPedyECRMoLi723Xbu3HkKW3nI4P/Fuv5V/trir2z1JuMu20317CdOfTtERETOck0KI/Hx8bhcLvLz8xvcn5+fT3Jy8o8+96mnnuKxxx7j888/p3v37j96rNvtJjIyssHtlHO6cHS+kl+MuplX3SMB2LNp+alvh4iIyFmuSWEkKCiI3r17k52d7bvP6/WSnZ3NgAEDjvm8J554gj/96U/MmjWLPn36HH9rbRAVEsilg84DwF2aQ43Ha3OLREREzi5NHqYZP348L7zwAv/6179Yt24dv/rVrygrK2Ps2LEA3HLLLUyYMMF3/OOPP84f//hHXnrpJTIzM8nLyyMvL4+DBw8237s4yc7p0ROAJGsfX689/aYki4iInMkCmvqEkSNHsnfvXiZOnEheXh49e/Zk1qxZvqLWnJwcnM76jPP8889TXV3NiBEjGpxn0qRJPPTQQyfW+lMkIDKJamcwQd5KvlmynKHdTnFBrYiIyFmsyeuM2OGUrjNyDJV/60vwgY2M9Uzg2T/cT5i7yTlORETEr5yUdUb8mTuhNQCp3nzW5p7CdU9ERETOcgojjeSIaQVAuqOA1buLbW6NiIjI2UNhpLFiMgFo6ShgzR71jIiIiDQXhZHGOiyMqGdERESk+SiMNJYvjOSzuaCUyhqPve0RERE5SyiMNFZ0SwAiHRWEeQ+yMb/U5gaJiIicHRRGGisoFMLNkveqGxEREWk+CiNNcVjdyIY89YyIiIg0B4WRpjgsjOwuqrC3LSIiImcJhZGmOBRG0h355BVX2tsWERGRs4TCSFMc1jOSW6yeERERkeagMNIUh4WRwoPVVNVqeq+IiMiJUhhpikNhJNWxDxce8our7G2PiIjIWUBhpCnCk8DlJsDhJcWxT0M1IiIizUBhpCmcTojJAOrqRlTEKiIicqIURpqqQRGrwoiIiMiJUhhpKs2oERERaVYKI02lnhEREZFmpTDSVL6Fz9QzIiIi0hwURprqsJ4RrcIqIiJy4hRGmirazKaJcRyk+uABKmu08JmIiMiJUBhpKnc4VlgCAOmOvazX7r0iIiInRGHkODgOqxtZvbvY3saIiIic4RRGjoevbiSfNXsURkRERE6EwsjxOKyIdZV6RkRERE6IwsjxOCyMbMgrpbrWa297REREzmAKI8fjUBjJcO2lxmOxMV9FrCIiIsdLYeR4HJrem0YhTrwqYhURETkBCiPHIzIVnIEEUEsy+5m3udDuFomIiJyxFEaOh9MF0S0BaOksIHtdAeXVtTY3SkRE5MykMHK8DtWN9AwvoqLGw+z1e+1tj4iIyBlKYeR4HQojFySUAfDxqj02NkZEROTMpTByvA6Fkc7B+wH4cl0Be4q0i6+IiEhTKYwcr0NhJKpyN/1axVJd6+WpzzbY2yYREZEzkMLI8ToURhwHtvOHyzsB8M7y3dz35vd8s0n1IyIiIo2lMHK8YsxaI5QX0j3Bxc97twDgv8t2cfsrSymuqLGxcSIiImcOhZHjFRwFIbHm+wM7eOy67kz/n960jg+josbD20t32ds+ERGRM4TCyIk4NFTDge24nA6Gd03mF+e3AuDFb7Zy+dPf8PCHa+xrn4iIyBlAYeRE+MLINt9d1/RKI9wdQG5xJWv2lPDyt9vJ2VduT/tERETOAAojJyK5q/m6/hPfXWHuAO68sA2BLgcJEW4A3lyy047WiYiInBEURk5EjxvB4YKc7yB/re/uO4e0Zf2fLuWhK7oA8PbSXdR6vHa1UkRE5LSmMHIiIlOh42Xm+yUvNXjI5XSQ1TmR2LAg8koq+cN7q1m5q4gtew/a0FAREZHTl8LIiepzq/m6/N+w4VN4czTMfhQAd4CL32a1A+CNxTu58tlvGfqXudz+yhJ2HVAdiYiICIDDsizL7kb8lJKSEqKioiguLiYyMtLu5jRkWfDva2HLVw3vv30OpPYC4NvNhfzpo7UUldew92AVHq9FdGggz406h/PaxB/73O/+Eg4WwKi3zE7BIiIiZ5DGfn6rZ+REORzw8xmQaOpDcAaYr/P+6jtkYNt4Zv3mAhb831A+vXcQPVpEUVRewy3/XMS/F+w4+nmrDsL3r8OWbDiw/aS+BRERETspjDSH4CgY8xFc908YO8vct/YDKNx0xKHtkyKYeccAruqZSq3X4g/vrWbYX7/mkQ/XUlnjqT+wZHf996W5J/kNiIiI2EdhpLmExkK3EZDeF9oPByxYOgOqy2HXEvP1kOBAF1NH9uR/h3fA6YAN+aW89O02fvPGCmo9XvYUVbBi9er6c5cojIiIyNkrwO4GnJV6j4GNs2Dlm5C/BrbOBmcgDP49DP4dAA6HgzuHtOW6c1owb1MhE95Zxaw1ebR98FMARrrm0zPw0PlK99jzPkRERE4B9YycDG2zIDQeygpMEAHw1sDsP8PupbD4Rd+6JEmRwVzXuwVTb+hJuNtkQ5fTQWbAft/p9ucdo65ERETkLKCekZPBFQjdfg4Lnzc/X/A72LcZ1rwLL10KniqIawd3LzYFsMBl3VIY2imRsioPwYFOHO+9B4fWUVu6ag0tB5bSITnCnvcjIiJyEqln5GQ552azOmtUOpz/W7joj2amjafKPL5vE+TMb/AUd4CL2LAgQoMCCCmvH5qJ9e7jphcW8MaiHP725SYKSitP5TsRERE5qRRGTpakLnDH13BbNgSFQVwbGDYZ0vtDqwvMMctePfbzD5tNkx5QxL6yav7z7nucM3cMD05/k398vYVfzFjMtsKyk/xGRERETi4N05xMdRvp1el/u7nlLIRtl5hhm4v+AFFpDY/zeqG4PowkcIBBbWIZn/cYvTyrKSh5k/s+iQPAgRnieWX+diZe0ZmokEDmbNhL74wYerSIxul0sHp3MemxoUSFBCIiInK6URixQ3o/SOoK+avh7xfA9a9A5sD6x8sLDw3nmHoSh7eWVy8PgX+sAuAC12piAgIoqqwle30BX2/aS43H4rZ/LaHWY1FaVQvApV2TubpXGne8upR+mbHMvONcHIdqVERERE4XxzVMM23aNDIzMwkODqZ///4sWrTomMeuWbOG6667jszMTBwOB1OnTj3etp49HA4Y+SokdTPB4z/XQ/Yj8Neu8N0zULzTHBeRAuGJ5vvvngbMyv0JHGDR7S24rGsKADUeC6cDDpTXUFpVS6v4MAJdDj5dncf9b30PwKLt+/loZS4Pf7iG77YUnup3LCIickxNDiMzZ85k/PjxTJo0iWXLltGjRw+GDRtGQUHBUY8vLy+ndevWPPbYYyQnJ59wg88asa3hti+g1WCoPgjf/MWEkM//AIsP7QAc1cIEEoDV/zVfXUEABG6fy6+GtMHpsIgIDuCdOwfSNzOGkX3S+fTeQdxxQRsASitrfS/569eX8/K32xnz0mLmb9l3yt6qiIjIj2lyGJkyZQrjxo1j7NixdO7cmenTpxMaGspLL7101OP79u3Lk08+yQ033IDb7T7hBp9VAkPghtcg/VwIijDBBGDFv83XqBYQmVp/fEAInPdr8/2WbLo6t7M24Y8sSptKz+gq3vrleTw+ojvBgS7uvLANyZHBAIwdmInLaYZnnA6o9ni5/ZUlrNlTfKreqYiIyDE1qWakurqapUuXMmHCBN99TqeTrKws5s+f/yPPbJqqqiqqqqp8P5eUlDTbuU877ggY+yl4D/VgvH4D5CyAmEzo8wtY9q/6Y382BZK7mV6UzV/CltkEWx4o2QovXASjPzABpmgnofFtef3qaPLWfUu/4VlEBgeyZ808HiufyMeBl3DPvusY/dJieqZH4Q5wcdeFbQkNchHmDiAhoplDY93G0KpXERGRo2hSGCksLMTj8ZCUlNTg/qSkJNavX99sjZo8eTIPP/xws53vtOd0gtMMv3DzOw0fKyuAVW/B+eOh503mg/388TB/milybXWB2btm3yZ4bQS43LB3HbS5iFY7vqNVbSUUfcxvfz4DdrwORaVcWf1f1sdmEleylsu2LqTKCuSaVQ9zgEiucC3kKfc/+DTz91R3GsH1fdMpq6rF5XQQHOgym/+5IyCiCUNuM/8Hdi2GO76BiKSfPl5ERPzKaTmbZsKECYwfP973c0lJCenp6Ta2yEZdr4O2F0NwpPnZ4YCsSTDgLsj93oSRiiJ48SLYv7X+eVu+OnS8E3YugGd6Q039miT/W/6X+j99B/w64D2mWT9nUsDLuL0V9NnyLIPXtmFTQSlvLN5JZHAg7/08hoT/XGLCyLjZEJPx0+3ftwXWf2S+X/WmWYm2KAdu+I8ZphIREb/XpDASHx+Py+UiPz+/wf35+fnNWpzqdrtVX3K4uiByuLB4aDvUfB+eADe9Cf++DmJawaDfwsJ/QFpv6HINvP0LyDfTgul3O6z/BEp2Qcb50PYiyH6EsUHZjGlj4dxkhsRaOAq51vUNq79dz0UcYHNVCza88REJnmoo38fu567kG/cgaqMySOs5jP49OhMadJRfp7rCW4B5f4XyQ4Wz6z82uxyfKE8tLH8FkrtDiz4nfj4RETnlmhRGgoKC6N27N9nZ2Vx99dUAeL1esrOzufvuu09G+6SxEjvBb1abIR8wm/XVGfeVmRp8YBsMnWj2yinZAyk9TE/L9m9xbMnGsekzc3zrC2HrbJ4M/EfD16gBr+WgiDDSarZzQ812OAieXX/mvQ8Hs7/lcIaddw4tO/cnt7iCLfkHGbjqLXyVIuWHzeD5/g0TlDw1EBhsencAQqKb9r4XTofPHzQ9QIMfgMH/e2RtitcDGz6F1oNNr46IiJxWmjxMM378eEaPHk2fPn3o168fU6dOpaysjLFjxwJwyy23kJaWxuTJkwFT9Lp27Vrf97t372bFihWEh4fTtm3bZnwr4gsiPxQQBBfcX/+zO6J+/RKAy/8CX/wRAsOg/TAzq+fpXlBVTFVYKoExaTh3LQYgO+B8Poy+mXGhc4l0VhG4dzWp5eu5zjEHds2BN2FHzADeLWpLYs1uzg/YiNflxpnaywwXOZxgefFszqbmL90IdgHX/dPUlTgD4M75JjxUlUBkmgkqx1JVCvOmmO8tL8x51Mw+Oufmhsd9MwVm/z84ZzRc+XRTr6qIiJxkDsuqm+rQeM8++yxPPvkkeXl59OzZk6effpr+/fsDMGTIEDIzM5kxYwYA27dvp1WrVkecY/DgwcyZM6dRr1dSUkJUVBTFxcVERh5lyEKaX1EOVJaYPXYcDtg6FzZ9bjb9C4tvcKi1cxHFXz7FgdytpFVtJcjhafD4Z84LGHLt7bjf/h8OdL6Z3esW0NXadNgRDuoWdKPdJbDjO7P2SmAYXD3N9KDU+fpJ+H6m6QHJWQBL/glxbc0uyXMmQ0gM3L2kvo2eGrOY3ME8CI6C+zebcCYiIiddYz+/jyuMnGoKI2cGy7KY/t6XBC99gbTgKvp1bsPj6+L5b2lnWifHUpq3lVziuNb1DU8F/p2F3o50cu4kkjJqHEEEWtW+c3kdATitWggIhiEToLrM9JzMefTIFx7xMnS6Av5xoamN6XAZXP+qqYvZMR/e+2X9saPehnYXH98b3DDLFAYPnQju8OM7h4iIH1EYEdtszC8lPSaUkCAXn6/J4/ZXl/oeczqgfWI4D50fzOj3C+ntXc19AW/xTO3V/I8rmyzXMpZ723Jz9QO8GDadcz1LjnyBzEGw41sIT4Khk6Dnjeb+3UvhpeHgqYbwZNMbUico3PS2tL8U2mVBp6tM4W+d0nzIWwkt+sKGT2DTF9D3tvo9gw7sgOfOhZpyOPdOGD7Z9Lps/hJa9IOwuJNwJRth3lTTi3XpE+A6LSfHiYgfUxiR04JlWUx8fw0rdhYx4dKODGhzaLdhh4OVu4rYVlhGuDuAl7/dzsrN27nctZCvAwdRYoViVZXwl8DphFFBATH0c65npasL285/ik8WrCIqJp6xgztycefD1i5Z/zHMvBksD16cOPGaIHLZUw17SMKT4NLHITTeLCy35j3w1vhqWnzaDIXOV5m1XrZ/Y+5zuODG12Hh32FL9qGl/bMhwG2eGxhm6neqDppgFBrbuItVUWQ2T6ytgjYX/fQicQe2w996mO9/PqPhcNbJ5qlV+BGRn6QwImecbYVlrNlTzMA28VjAd1sKsSx4beEOFmzdT5DLSbXHe8Tz/nB5J7YVlrFyVzFTru9B5J5v+e+sz3ixuB/p4RZ/H3MuyakZMH0QFG7EExqH6/BekzphiWaRuYAQM216wycNg4nLbaYP7/j2yOeGxtXPFgqKgJTusGuJWZguvj10HWEWrQsMMVOcg8LhvLvrZ/ds+hLeGm16bwAu/hMkdjavdf5vTL3LD81+FOY+br5v0Q+i06GmAka8dHLXcFn/sSk4vvQJ6DfO3FdTab7+WMHxD337NKz70OxaHZnS/O1sTjUVJqgGaMkBkaZQGJGzSlWth1qPxYPvrmLuxr3cNqg1+SWVvDJ/R4PjYsOCqKj2UFFTX0QbFuQiJMjFNd0T2Vtcxmdrcnm55Sy6Vy3He7AAT5uLiRpyN6T0NCvMhsZBWBy1ezcTsPI/JlQEBEOPkdByALw52gyNhCWYQPHxffUh4qc4A00PDJhi2+gMCAw1K9R6a+oDkTPAzCrCMrs7/3wGxLUxQ0MbP4Xy/fWbK/7QwHvNENPsR03RcVpv6HUzdPyZmd4dEnP03pr1n5jjQ2Oh5yjzej/k9cLzA2DvetMj9OtlZubT8+ebto79BKJbws7FsPptszhfdMsjz1NWCFM6m7DWdxxc/lTjrt/JsPFzKNxoht+ONiOtuty8Z1cQ/Oo7cAWe+jaKnKEURuSsZVkWDocDy7IY/+b3vLt8N0EBTtKiQ9hWaFaZ7ZsZw4TLOnHHq0vZW1p11PO4nA48XouECDevjzsXl9PB52vyaJ8UQUZcKKNeXEi7pAj+cXNv3AHmQ8pxtKGT/DVQsA4yzzc9GAXrIHeFCQFR6aauZMnLZmqz5TX7C1WXNVwxF8wwyzX/gP/+wvQYgAlBtYd6HdxRJrDUlNc/JzgKMgaaXhxXkBkW8q3s8oO/2iGxULHfhJ8hE+DcX5kP1r0bTW/N9/+pPzY0Dm79woSeqLT6DRs3zILXR9Yfd8fXsPod+Haq+Tm+PfS4AeY8boJGdEv4+b/MnknhiaYWJ2eBeU/znz30B+GG61407e105ZHDUwcLzGyudhebdWhqq83MqYDgo68r0xQ1FfBEG7M68cjXoNPPjjxmzbvw1hjz/dhZkDHg+F9PxM8ojIhfqKzx8J+FOfTOiKFFTAjPz9lCj/RoftY9BYfDwf6yanbsK2NvaRV//mQdNbVeBndI5PVFOQCEBLoa9KKAKbLNjA9j614TbHq0iGJbYRnnt4vnoSu78Mp3O3A44NzWcZzbOo6lOw5QXFFDVqdEX1gpqawhMvgH/4OuKIKS3ZDQ0fR67Flmpk/XlJvhmtZDwOkyAeCz/zM9Ne0uhg/vhZz59ZsphiebcLBnmVnorfv18PkfoP8dsOxV0yMBZr2YAXeZD/9FL0B1acOamMQuEBQGuxYdaqADeo82vRoFa2gw5To4yvQQYJl2OAPM164jTHDyVJmwVHXYTtAut7m/TmQLM8PpcEERpl11rvm7WUX4wDYzXfvzP8Kif5gQltTVBJvPJpgeHICfTYU+Y4/9C1KUA9vnmW0VjjbEsvYDePPQujSZg2DMR3BwLyx83vQmxbYyQ1J14XDQ/TD0j8d+vRX/MfVFVzxths3q5K0yWyF0vvrUbxhZuAlKc83WESKnmMKIyA9YloXXMmHjpW+3Y1kWl3VL4dZ/LWFdbglOB7SMDWX7PtPzEBLoosbjpdZb/1ckONBJZU19HUlkcAAllSYkPHhZJ8Zd0Jppszfzl883MOa8VvRrFcOULzby64vacUWP1ONvfHUZFO8CHOYD0hVoCmSDwhp+uNVUmr2A0nqb4+qU7TvUW3OOqfn4/I+mlwRMQW67i83wTsZ5ULwbXrgQDuaDO9IMQR1eO+OOND0rn9Xv3k3G+XDVM/DdM1C0E9L7mR6Sd39pinKrSs05HC6zyWLJbjMkdeUzZqdqhxMsjxlCqio1QSe+vRk+gSODTV1QCgyDNhdCai8479cNA0fFAXh+oHmtrtfBtS/WD8PsXmYKgNd9CGsO25zyl/Pg0wdgxzxzDW9+F55qX987ldIT7ph79D+j8v0wtZu5Xq2HwM3vmT+bZa/AR+NNoDr3Thj2aP2f2e5Dw1yth5gZW+X7zMrITtfRX+PAdhMMQ2LMz/u2mOnmPW4017CyuGH9TU0FTO1uhv6ufRG6/9wMtVWXHr0O6WgW/h2+exZGvmKuszS/iiJTR3YWFoUrjIg0kmVZ1Hjq/xqMeXkR323Zx5+v6Up0SBAffL+bbmlR/PXLTXi8FumxIfTNiOXLdfmUVNYS4HRQ67VwOGBoxyS+XFe/d1PdUFCgy8Fj13YnNjyIAa3jzA7Ix9HO9XmltE4Iwx3gYk9RBcmRwTidx/E/7bJ9ZsG4wBDTA/HDXZiLd0PhBhMyasrM1Gd3uPlwC4k1hapTu5kPz4yBcPVzEJN57NerKDI9NHFtzc7Ny14x9Tdp55jhKnck/PMS2L/lyOde9Zz5EPzXzw59WPc0U6tnP1o/wwnMkFhlifkw7nGDWWOmbosDgPgOZqgowG2Gzg4X38G8X3ekCQd1Dm2NQEQqlO4x992/yYSknQvNa2YMNENZ2Y+YOp46lx2qg/nksNWPwYS+rIcPhZTfmJA27FGYPdmEhNA4SD3HhMXCzXDl30w7sh82w33BkebcHS6Faf1N2IptY4JIZRH8z39NuAFYOsP0rIEJbhkDTA1UZZGZJXbVcz++Zk5tNfylg2lL6jlm1pjDYa4JDhOcQmNNMK4qNde+utyE58OnzoOpd8Jx+nzgemphxb/N72Tm+fX311abIcuUnpDa8+S3I/d7ePFiaDUIbnrr2Ctpn6EURkSOk9drsbuogvTY0Ab3Z6/L55tNhdx9UVviw91U1nhYlnOAtgnhPDt7c4Ni2s4pkazNNR9qUSGBFFfU+B7rkBTBrYNaUV3rJSY0iB7pUdR4LP700VpKK2volhbNrYNakRZdPyNmzZ5i/t9H65i/dR8DWsdxWfcU/vjeaq7plcaU63vgcDjYd7AKj2WRGNGEGS0nYv828wGU0r15zrd1DrwxCtoPN/UzX/0/6PML6H+7ebx8v+mhqKtfKd8Pq96G2gr49m8N9z6q43CZoarvnuGIGpq6oaaodLMY3qtXm+EMMEEpZ379sZf8PzP8kvv9kb00zgAzJLbjO9OWtlmHhZ1DPTjnjzfTyWf93twd3dIMIR3Z4CPbGRBiwuKBbQ3vj2oJxUc5R2xrU+9TUQRv3GRCVnC0CSA/FN/e9HJtnWP+Z37hBBNa8leZXhevB965rf74q6ebuqQP76l/7+2GmVlfVaWmhmfZK6ZXLethc+23zjY1RHuWmyD6i8/NsOT8aaZXJ72v6dlJ6nJk+wo3m16qbtebnqWdi6BsrwlbkYf1NBblmN6b7iOhRe8jz5OzwASkVheYXsXy/fDf28zUfJfbbEMR18aE7jdvMfVdzgDIeggG3G3WMMpZYNqYMdCs4lx10PQOVpbAVdPMcKKn2uz9VZprrmnJHlM4ntT5yDbVeXM0rH3PfD/8cTj3l8c+9lh2LTU9fLuXmh7U9P5w7T+O3cN2CimMiJxCXq/F3I17WbW7mOjQQG7q15L/9/E6SitrefDyTvzure/ZVljGgfJqDpTXNHiuwwHBAQ1rV9wBTga2jSfcHcCOfWV8v6v4hy/pM+X6HlzYIZGL//o1xRXVPHJVV27s15IDZdV8vWkvQzokkldcyTeb9nJjv5aEuc3/TKtrvXyzaS+bCw7SOTWS/q3iCAqw+X9ltVXHN322rNDUhsRkmP/5b5trhiM6X2l6SfZvNR+u5fvNB2WrQebD87MHzU7W59xshrhWvw2WZXaUfvFi0xty4YPQeywseM5sygjmg7vNRaZXYnf9on5kDIRbPoCv/mQCEpapPbnymfohmw9/Y4akHE7TS7LhUzM7KTQOxs2G0jwztBUYYsLWlmxz7qiWZihs5yIzpbuuhmjES+Y9x7Y2PTN1gapOUAT8ap7pJYlIMUNoNRWmKPdgwx3YCUs0H6h1wcXhMm2tCz4BwabdNeVHLix4NHVF04dL6GiCY9nehvfHtjEBIKmzmWEVGgcvX2qeH9va/NnVtSskBnrcBCvfMMFkz/JDPWxRcOtnZuPQ/dvMc7bNMb1Wdc9L7Ax7VpgevzqZg8wMsi/+aNp1+Ky3HjeadYhqK8zPcW1NSF7xH/PnBGZWXNGh/4y0vtAE2bqhPYDuN5jA8sNeof3b4Jlz6odBA4LNcGP/X5q/B4v+YYbywuLNAo+hsebnLyeZwvm4tiaAfXJ/w6FUMOfx1JgarH7j6oNJZbGZVh+RbGqYftiD1cwURkROQ4UHq3hy1gZ2HignzB1AQUmlL2ic0zKaUf0zeHPJThZua/gPuMvp4LJuKXRICuepz00dRUKEm72lVYQFuRjQJo4v1xX4ju/fKpYte8soPFhFXFgQpZW1VHu83NivJY9c1YXXF+Xwty83sa+sfgn+jskR3H9JB95YvJML2sdz87kZDWYPVdV6cAfY/z+tU8ZTa/4Br7sGlmWmUntqzP/K69Zy2b0UchaaXoZWg+rD1M5FpnD1nFsaTgfOWWh6K9oPN8NGBevNEMx5vzY1O4cr328KaN2RJtDUfXDsWWECT8sBDTfBXPehOR5MCLG8prfjvF8f+f7K98OXD5kdtNtdbHp96qaKBx6qRaqbsn7nAvhiUv2wV8b5MPoD85y175kP+MJN8M1TkH6umZU0e3L9B37fcaa3a+YoU8sDJmD0vc30KG38rP7D/8dEZ5jAcrThvDoBISbI/LBY+ocF1oldTE/Ou3c0DA6JXeD6f5k21YVPMEN55YUNe+BC48yHe10wPLxnK6WHCX+bPjd/Dj1uNLVLrkAz8614Fyx+0Qz3tbnI9NBs/LT+vCExpui5Tl0ImvNYw6HEOm2zzJBryR7z+3S49P5my4y6Hqm6WixngClCj0gytV7DHzPfNyOFEZEzxOaCg6zNLWFYlyTcAS4sy2LJjgNsLjhIWVUt8eFu+reOJSUqxLei7Ya8UqaNOoffzFzOt5vr/3G8tlcaH3y/x1d0+8OF4lxOBz1aRLEspwiAxAg3PdOjWbR9P0U/6LE5r00ctV6Ln3VPweV08MiHaxnVP4OJV9R3OXu8FutyS+iQHEGg6+wa6z5jFW42dSA/rAM6Fq/X1ClUlpjhlshUSOhkehrevd3Uilwx1Rz33dPmf/2XPdVwtlCd0jzTu+J0ml6u3JWmxiWhg3k8Z4Hp1el4udlFuy6kle83oc7hgLXvw5r3TXCIaws3vG5qOOI7mJljtVWm1mbPChNm1rxjejOu+bupj6nrrXAGmKExb60ZJut7mwlPhRvNVPPM8w/1Vr1qerEC3GajzsG/r1+875sp5oM9ubtZQ8frMZt17l1vakr6/AI2fwGz/g8G3mOu+ezJ0PdWM/PK6TTr98wcdWTPRR1noJnF1aKvKT6f8xgUmJ3uiUgxU/AXvdBwTaGMgabH7tPfmyG11hfCqLfM9bQseOd2WPWmWQyxYO2R6yBFpJrQsWd5w/tPwtR1hRERP1BWVcvolxaxZMcBhnRIYMbYfuwpquDfC3YQEuhizMBM3l2+m6iQQD5YsYfs9ab3JCzIxQOXdeLGvukEuJzs3F/OLS8tYlthGee1iWP+1n0c61+GrE5JrNhZxLXnpLG54CBfrS+gU0okt1/QCq8XKms9dE6JpFfLmB9te63Hy/7yal+Ny3ebC/nd2ysZOzCT2wa1btbrJGcYr8cMq0WmNm1TSk+tCVFleyG5a+NnDP2Ywk2mxufHhg9/anuE1f81+0iB6VnzVJnQlt7XDOPVhTUwBbTfPW3ef9ZDJjSU5pmhlcKNprZl6EQzk666zPQsZQ5quPqx12vCS3RLM3y08k0T0up69S74X3PeXUthxWsmuEW3NEXNRwuZJ0BhRMRPlFfXMnv9XoZ0SPDVgxzN+rwSrnr2W4IDXbzyi370SI9u8HhljYfc4kpaxYexaNt+Fm7dR0WNh+fmmC7xTimRrMs9SvfwMZzbOpYd+8qJDA7k0m7JjOjdgm82FbI85wA39mvJpA/WsHZPCW/9cgAOh4ObXlhAebWHsCAX300YSlSI+V+z12sxb3MhucUVuJxOIoIDWLbjAMlRwYw5L/PoC9GJyGlBYUREjrDrQDmRIYFHLsj2I77bUsjO/eWM6J3OQx+sYfu+Mga3T+C5OVvwWhZPXNed2Rv2sqXgIO5AM1Tz7eZCvI38l+W8NnFs3VtGXkn9uP1dF7ahRUwofTNjeearTby/Ys9Rn/vbrPbcM7StbzbRspwiWsaG0iHZ7PmTV1zJip1FXNI5yTcF+mBVLWFBLrwWLN6+n+4toggNOk2mm4qcZRRGROSkqqzx4PFaR+2NWb27mHmbC+mcEsne0ipfUW5kcABtE8NZllNETGggRRU1vuGgzLhQRp+XycMfrj3ifAFOB4PaxVPt8bK/rIbUqGDfkFPQoVqVw2tjru/TgjHntWLsjEXkl1Rx79B2/Pbi9ry2cAcPfbCG1vHhRIUGsmjbfga0juM/4/of0cOSW1zBjO+2M3t9AVf1TOPW81sd1/owIv5MYURETivbCsuIDQsiMjiAbzfvo11SOI98uJaPV5mpqK/d1p8+mTEMfmIOeSWVpEQFk1tsekumjuzJ1b3SGpzv2a82+Raiq5MZV7+C7uECnA4u6ZLEJ6uOPhX1hVv6cHFnM4tgy96DPPvVZj48rBAYoEVMCP87vCNdUiMJCwogKiSQkKCjh5O6f1Y1hCT+TmFERE57m/JLGfmPBVzZI5WHrjSLXu3cX87uogr6ZcayPq+UqlrPMYthy6pqKaqowQFEBAcQERzI0h37mfTBGlbvLiEqJJCuaZENZhzdfWFbyqs97NhXRnRoEP9dtouWsaGMOS+TFTuL+HhVri/g9G8Vy0UdE3n52+0NhpEAAl0O7rmoHRnxYazaVcRdF7YlKiSQNxbv5PFZ63EHOLmieyr3D+tAcKCLWo+X1XtKaBETQnz4caylcgw795fz+/+uJCMujHuGtiUlKuSnnyRyiiiMiIjf8ngt5m4soHV8OBHBAfz+vyuJCQ3ihn4t6Z1RH2xKKmu46Kk5FB6sbvD8oR0TuTerHd1bRAOmSPjvc7fy32W7KKmoobza06DXBGB4l2RiwgJ5fdHOBvdf1TOV89rEMW32FnL2lxPgdNAnM4akyGB27CsnLiyIp2/sRXCgi/ySSipqPLSKCzvqMv+WZfH83C3MWp3HpCu6kBYdws///h0795sFucKCXLx310DaJUVQ4/Eyb1MhvTNjmlQjJNKcFEZERBohZ185by7ZyZo9xbRPimB41+SfnJYM8Obinfzh/dUEBzgbhBOnA34/vCPJUcHc9+b3P7rRYp2sTols3VvG1kKzSFj3FlF0SY1kXW4p57aOY0TvNFKiQnj0k3W8ttAsAR8RHECQy8m+smoy4kIJCwpgbW4Jw7skM2VkD+58bRlzNuzl3NaxPDeqNy98s5VB7eJJjAjmrSU7Ob9dPOe3jcfhcFBcUcP63BIcDge9M2JwOR1YlnVCw0yVNR7V2IjCiIjIyXagrJrgQBfT527hb9mbcDjM8vzX9GoBwL++286kD9YQGRzAPUPbMap/BruLylmeU8S+smoCnA4e/WSdb+ZRgNOB0+FoUIxbJzTIRXm1B4cDWsWHsXWvCS4dkiJ4cXQfKmo8DJv6NZYFrRPqH687ZkN+KWCGl+o2huyZHs1N/Vse2hfJrCJ6addkkiKDeXf5bu66sA23DMhk2Y4DZK8vICHCzbW90kiMNGtaFJRW8rcvN9G/dRxXHrYr9azVudzz+gouaJ/AeW3ieP/7Pfwmqx0Xdkhs5j8BOd0pjIiInCI1Hi8vfLOVDkkRDO3UcDntdbklpEaFEBV69KGS5+ds4fFZ6xnULp6nb+iFx7J4ad42yqs9dEqJ4Iu1+czesBeP1yIjLpSJP+tMv1axTP50PS1jQ/nFwFa+PYV+/fpyPvzeTIOODQuiS2ok32wqBOp3kAbokR7NxrzSBvshJUW6OVBWc9QgdDh3gJOXxvSldUIYo15Y6OvNubFfOn+6qivlNR4uemouhQerGjwvLiyIj+8ZxOaCg/RtFYM7wOWbZj1/yz6+3VLITf0zSIsOYWN+Kc9+tZkuqZGMHdiKz9fm0SEpgnZJEb7zrdpVzC/+tZhR/Vvym6z2P/lnJPZQGBEROUMUlFSSEOE+5rBI4cEqthWW0aNF9I9uZrhzfzl3/2cZnVIi+d/hHTlYWcuQp2bjteDeoe24qGMiRRU1XNAunoLSKv737ZXM3biXK3uk8sSI7szbVMgv/70Ud4CTG/u15NUFO6iq9RITGsjQTklszC9l5a5i4sKCcDkdFJSavY/2l1djWWZ4ateBcl5bmEPL2FCcDthfVk1EcCC7iyp82xP0zoghKdLNJ6vyCAtyUVZtQlGEO4BuLaJYuG2/LzhFhwZSVF6Dy+lgxDkt6JoWySVdkrnrtWUs2WH2uXn6xl5c2SOV6lovS3ccYM2eYlonhNGvVRzhP7IQ4LHUeryMf/N7Zq8v4NJuydx9YTtaxoX+9BPlCAojIiLCawt3sCn/IBMu63jERoeWZVFQWkXiYUFo5/5yQoJcxIe7KauqpbzaQ3x4EA6Hg8oaD9c+9x1rD63E2y4xnJfH9uXbzYX8/r+rcDjwrRvzyi/6MbBtPB6vxbKcA9zwjwXHbGOgy0HL2FC2HDa0dF6bOBZv30+Nx/INUdX54c9BLidX9Uzl6017yS+p75GJDQvimRt7MbBtfIPXO1hVy9wNe1m4bR+VNR4SItx0SonknWW7WZ9bQkp0CEsPBR2AtOgQ/n5zb16Zv532SREMbp9AVa2XihoPUSGBtEsMb3R9zZtLdvLq/B30ammGyDomn92faQojIiLS7LYXlnHvG8vpnBrFH3/WidCgACzL4rZ/LSF7fQEOB0z6WWfGDGzV4HnvLt/F/rIa+mTEcPfry3A5HPzl+h6EBAaQEOEmJjSQj1flUl7toXdGDO2TIli5q4jvtuxjZJ901uWW8PnafOZv2eerfxk3qBW7DlTw6er69WPiwoLo1TKGdbkl7C6qwOmAjsmR1Hq95BZXkhYdwvZ9ZUctJD6cwwETLu3IvxfkkLP/yLVrDpcWHULfzBgGto3nyp6puAPMVO5dByrYkF/KxytzWb2nmCu6pzJt9mZfUXO4O4C3fjnA7KxdVUu4O4CkyOAjzr+3tIr3lu/m8u4ppEQFs62wjMzDZlyVVdWSV1JJi5iQ025nbYURERE5ZfYdrOKZrzZzYcdEBrdP+NFjPV4Lp+P4FoWrrPHwxKwN5Owv4y8/70lkSABzNuxl5mIzQ+j6PukEBTiprPHw4Lur+e+yXUc9T+v4MAZ3SCA+3E3OvnJW7CyiXVI4l3VL4ct1+Qxun8BVPdNYtauYa5//lhqPRaeUSIICnGwtOEio20VoUAC5xRUNgk1cWBBh7gDyiiuPWX8zuH0CpZU1LMspalBQDGbhvt4ZsXRLi6RjSiR5xZVM/nQd+SVVdEyO4JLOSTz91WZuPjeDP13dla837uVX/15KWbWHkEAXf7q6KyN6t2jwersOlDPj2+0szTlARbWHv93Qy7dlwsmmMCIiIn4vZ185G/NLcTkdpEaHsOtAOYkRwXRNi2x0GJq3qZCVu4sYe16rI1bdraj2sHDbPpbtOMDMJTsbDBO5A5y0Tginb2YMEcEBTJ+7lYzYUN67eyCWF66b/h2bCw7idEBEcCAHq2obrCj8U0b0bsH7K3ZT47EIcDqoPRTyru6ZRm5xJaVVNZRVedi5v7zBFPO6YSeAtXtKWJtbwto9JUy/uTexYUGNfv3GUBgRERE5hapqPazIKSLA5SAxIpi06JAGi9cVHqwiNMjl25ixuLyGNbnFdEuLIiI4kNLKGhZt28/3u4pZvbuYjfmlxIUF0TczltToEB75yOzblBjhpqC0PvRc0SOVp37enYc+WHPEont1BraNY0TvFjydvZlthWVHPea12/ofUV9zohRGREREzhJer8X4N1ew92AVz9x4DuPfXMH+smruuKANl3VLxuEwU7enz91CSUUNHZIjiAk1Q0YJEW5axYcBpuZn/Jsr2LK3DJfTQYekCLqkRtI5NZJB7RJIiGi+rQpAYURERERs1tjP72NPWBcRERE5BRRGRERExFYKIyIiImIrhRERERGxlcKIiIiI2EphRERERGylMCIiIiK2UhgRERERWymMiIiIiK0URkRERMRWCiMiIiJiK4URERERsZXCiIiIiNhKYURERERsFWB3AxrDsizAbEUsIiIiZ4a6z+26z/FjOSPCSGlpKQDp6ek2t0RERESaqrS0lKioqGM+7rB+Kq6cBrxeL3v27CEiIgKHw9Fs5y0pKSE9PZ2dO3cSGRnZbOc92+g6NY6u00/TNWocXafG0XX6aXZfI8uyKC0tJTU1Fafz2JUhZ0TPiNPppEWLFift/JGRkfpFbgRdp8bRdfppukaNo+vUOLpOP83Oa/RjPSJ1VMAqIiIitlIYEREREVv5dRhxu91MmjQJt9ttd1NOa7pOjaPr9NN0jRpH16lxdJ1+2plyjc6IAlYRERE5e/l1z4iIiIjYT2FEREREbKUwIiIiIrZSGBERERFb+XUYmTZtGpmZmQQHB9O/f38WLVpkd5Ns89BDD+FwOBrcOnbs6Hu8srKSu+66i7i4OMLDw7nuuuvIz8+3scWnxtdff80VV1xBamoqDoeD9957r8HjlmUxceJEUlJSCAkJISsri02bNjU4Zv/+/YwaNYrIyEiio6O59dZbOXjw4Cl8FyffT12nMWPGHPH7NXz48AbHnO3XafLkyfTt25eIiAgSExO5+uqr2bBhQ4NjGvP3LCcnh8svv5zQ0FASExP53e9+R21t7al8KydVY67TkCFDjvh9+uUvf9ngmLP5Oj3//PN0797dt5DZgAED+PTTT32Pn4m/R34bRmbOnMn48eOZNGkSy5Yto0ePHgwbNoyCggK7m2abLl26kJub67vNmzfP99hvf/tbPvzwQ9566y3mzp3Lnj17uPbaa21s7alRVlZGjx49mDZt2lEff+KJJ3j66aeZPn06CxcuJCwsjGHDhlFZWek7ZtSoUaxZs4YvvviCjz76iK+//prbb7/9VL2FU+KnrhPA8OHDG/x+vf766w0eP9uv09y5c7nrrrtYsGABX3zxBTU1NVxyySWUlZX5jvmpv2cej4fLL7+c6upqvvvuO/71r38xY8YMJk6caMdbOikac50Axo0b1+D36YknnvA9drZfpxYtWvDYY4+xdOlSlixZwkUXXcRVV13FmjVrgDP098jyU/369bPuuusu388ej8dKTU21Jk+ebGOr7DNp0iSrR48eR32sqKjICgwMtN566y3ffevWrbMAa/78+aeohfYDrHfffdf3s9frtZKTk60nn3zSd19RUZHldrut119/3bIsy1q7dq0FWIsXL/Yd8+mnn1oOh8PavXv3KWv7qfTD62RZljV69GjrqquuOuZz/PE6FRQUWIA1d+5cy7Ia9/fsk08+sZxOp5WXl+c75vnnn7ciIyOtqqqqU/sGTpEfXifLsqzBgwdb99577zGf44/XKSYmxnrxxRfP2N8jv+wZqa6uZunSpWRlZfnuczqdZGVlMX/+fBtbZq9NmzaRmppK69atGTVqFDk5OQAsXbqUmpqaBterY8eOtGzZ0q+v17Zt28jLy2twXaKioujfv7/vusyfP5/o6Gj69OnjOyYrKwun08nChQtPeZvtNGfOHBITE+nQoQO/+tWv2Ldvn+8xf7xOxcXFAMTGxgKN+3s2f/58unXrRlJSku+YYcOGUVJS4vtf8dnmh9epzmuvvUZ8fDxdu3ZlwoQJlJeX+x7zp+vk8Xh44403KCsrY8CAAWfs79EZsVFecyssLMTj8TT4gwBISkpi/fr1NrXKXv3792fGjBl06NCB3NxcHn74YQYNGsTq1avJy8sjKCiI6OjoBs9JSkoiLy/PngafBure+9F+j+oey8vLIzExscHjAQEBxMbG+tW1Gz58ONdeey2tWrViy5Yt/N///R+XXnop8+fPx+Vy+d118nq9/OY3v2HgwIF07doVoFF/z/Ly8o76+1b32NnmaNcJ4KabbiIjI4PU1FRWrlzJ73//ezZs2MA777wD+Md1WrVqFQMGDKCyspLw8HDeffddOnfuzIoVK87I3yO/DCNypEsvvdT3fffu3enfvz8ZGRm8+eabhISE2NgyORvccMMNvu+7detG9+7dadOmDXPmzGHo0KE2tswed911F6tXr25QlyVHOtZ1OryWqFu3bqSkpDB06FC2bNlCmzZtTnUzbdGhQwdWrFhBcXExb7/9NqNHj2bu3Ll2N+u4+eUwTXx8PC6X64jq4vz8fJKTk21q1eklOjqa9u3bs3nzZpKTk6murqaoqKjBMf5+vere+4/9HiUnJx9RFF1bW8v+/fv9+tq1bt2a+Ph4Nm/eDPjXdbr77rv56KOPmD17Ni1atPDd35i/Z8nJyUf9fat77GxyrOt0NP379wdo8Pt0tl+noKAg2rZtS+/evZk8eTI9evTgb3/72xn7e+SXYSQoKIjevXuTnZ3tu8/r9ZKdnc2AAQNsbNnp4+DBg2zZsoWUlBR69+5NYGBgg+u1YcMGcnJy/Pp6tWrViuTk5AbXpaSkhIULF/quy4ABAygqKmLp0qW+Y7766iu8Xq/vH1B/tGvXLvbt20dKSgrgH9fJsizuvvtu3n33Xb766itatWrV4PHG/D0bMGAAq1atahDcvvjiCyIjI+ncufOpeSMn2U9dp6NZsWIFQIPfp7P9Ov2Q1+ulqqrqzP09sqVs9jTwxhtvWG6325oxY4a1du1a6/bbb7eio6MbVBf7k/vuu8+aM2eOtW3bNuvbb7+1srKyrPj4eKugoMCyLMv65S9/abVs2dL66quvrCVLllgDBgywBgwYYHOrT77S0lJr+fLl1vLlyy3AmjJlirV8+XJrx44dlmVZ1mOPPWZFR0db77//vrVy5Urrqquuslq1amVVVFT4zjF8+HCrV69e1sKFC6158+ZZ7dq1s2688Ua73tJJ8WPXqbS01Lr//vut+fPnW9u2bbO+/PJL65xzzrHatWtnVVZW+s5xtl+nX/3qV1ZUVJQ1Z84cKzc313crLy/3HfNTf89qa2utrl27Wpdccom1YsUKa9asWVZCQoI1YcIEO97SSfFT12nz5s3WI488Yi1ZssTatm2b9f7771utW7e2LrjgAt85zvbr9MADD1hz5861tm3bZq1cudJ64IEHLIfDYX3++eeWZZ2Zv0d+G0Ysy7KeeeYZq2XLllZQUJDVr18/a8GCBXY3yTYjR460UlJSrKCgICstLc0aOXKktXnzZt/jFRUV1p133mnFxMRYoaGh1jXXXGPl5uba2OJTY/bs2RZwxG306NGWZZnpvX/84x+tpKQky+12W0OHDrU2bNjQ4Bz79u2zbrzxRis8PNyKjIy0xo4da5WWltrwbk6eH7tO5eXl1iWXXGIlJCRYgYGBVkZGhjVu3Lgjgv/Zfp2Odn0A6+WXX/Yd05i/Z9u3b7cuvfRSKyQkxIqPj7fuu+8+q6am5hS/m5Pnp65TTk6OdcEFF1ixsbGW2+222rZta/3ud7+ziouLG5znbL5Ov/jFL6yMjAwrKCjISkhIsIYOHeoLIpZ1Zv4eOSzLsk5dP4yIiIhIQ35ZMyIiIiKnD4URERERsZXCiIiIiNhKYURERERspTAiIiIitlIYEREREVspjIiIiIitFEZERETEVgojInJcxowZg8PhYMiQIXY35YQ5HA4cDgczZsywuykifklhROQMMGTIEN8H5g9v7733nt3Na7LD38+f//xn3/3r169XMBDxQwF2N0BEGi8oKIhevXo1uC82Ntam1jSPJ598kl/96ldn/Pv4MdXV1QQFBdndDJHTlnpGRM4gKSkpLFiwoMHtggsuAGDGjBm+XoU5c+bQq1cvgoOD6d69O3Pnzm1wnnnz5jFs2DCioqJwu9106tSJJ598Eo/H4zvGsiyee+45evXqRUhICBEREfTr18+3XfvhXnzxRVq1akVERAQ/+9nPyMvLa/R7Ki4u5vHHHz/m43PmzPG9r+3bt/vu/2EPyuHv/6233vK1+9JLL2Xv3r288MILpKenExcXx5133klNTc0Rr1VSUsLo0aOJiIggISGBiRMncvj2XcXFxdx7771kZGQQFBREixYtGD9+POXl5b5jDh++euKJJ2jRogXBwcGNvh4ifsm2LfpEpNEGDx5sAVZGRsYxj3n55Zd9O5yGhoZanTp1skJCQizACgsLs3bv3m1ZltlhNyAgwAKsmJgYq127dr7n3Xbbbb7z3X333b774+LirC5dulhBQUHWu+++a1mWZY0ePdoCrJCQECs4OLjBeW666aZGvZ+2bdtaERERVkhIiLV7925r3bp1R+zSeviOwNu2bfOd44fHHf7+Q0JCrI4dO1oOh8MCrE6dOlmBgYFW+/btfcdMnz79iHOFhYVZqampVlpamu++v/3tb5ZlWVZVVZXVs2dPC7CCg4Ot7t27W8HBwRZgXXTRRZbX621wXYKCgiyn02l16tTJiouLa8wfs4jfUs+IyBlkx44dR9SMHM2UKVNYu3YtixcvJiAggLKyMp5++mkAJk2aRG1tLRkZGWzdupWNGzdy7733AvDPf/6TrVu3sn37dqZNmwbANddcw549e1i9ejW7du2iT58+DV6rqqqKBQsWsHHjRq655hoAsrOzG/V+4uLiGD9+PBUVFTzyyCPHdU2O5sEHH2TdunXcdNNNAKxbt46XX36ZDRs2cP755wMwe/bsI553zjnnsH37drZt28agQYMAePTRRwF4/fXXWbFiBUFBQaxcuZLvv/+eBQsWAPDVV1/x1VdfNThXdXU1H330EWvXriU/P7/Z3pvI2UhhROQMEhQURP/+/RvcjubGG28EoEuXLnTr1g2AVatWAbB48WIALrvsMqKjowF8H9qWZbF06VIWL17sG5647777fPUOCQkJtGjRosFrdevWjR49egDQuXNngCZ9+N53333Ex8fzz3/+k82bNzf6eT/miiuuACAzM/OI+1q3bn3MNo4YMYLAwEACAwMZMWKE77i9e/eyaNEiwISM9u3b43A46Nmzp++5dcGkTocOHbj00ksBcLlczfK+RM5WKmAVOYPU1YycTuoCDUBAQNP/SYmIiGDChAncd999TJo06YjHD+/9qatpKS4u/tFzRkZGHtGeuvvqzmcdVgvSFEcrIgaIiYlp8HNSUtJxnV/EH6lnROQsNHPmTMAMT9T1iNT1kPTt2xeATz75hKKiIsAMQYD5oO7duzd9+/b1fWhPnTqV6upqAPbt28euXbuavb133XUX6enpLFu27IjHEhMTfd9v3LgRgLfeeqvZ2wDwzjvvUFtbS21tLe+88w5gQkVCQoLvunk8Hp577jlfAfGcOXP43e9+5+tdqnOsITQROZLCiMgZJDc3l3PPPbfBrS54HO7++++nS5cu9OnTh9raWkJDQ/n1r38NwMMPP0xAQAA7duygdevWtG/fnqlTpwJw66230rp1azIzM7nrrrsAePvtt0lLS6Nbt26kpaWxZMmSZn9fbrf7qL0iAO3ataNly5aAGU668MILfW1rbkuWLCEzM5PMzEzfDKQHHngAMENf3bt3x+Px0LdvX7p27UqHDh2Ijo5mxIgRvmAnIk2nMCJyBqmurmbhwoUNbrm5uUcc9+mnnxIcHExtbS1du3blww8/JC0tDTALjs2ePZuLL74Yj8fD9u3b6dixI48//jjTp0/3nePpp59m2rRp9OzZk4MHD7Jt2za6d+/eoA6jOY0ZM4YOHToccX9AQAAzZ86kV69eVFZWsn//ft59992T0oZHH32UoUOHUlxcTFxcHA8++CD33HMPYALT3Llzueeee0hPT2fjxo0cOHCAPn368Oc//1nDMiInwGEd78CpiJxWZsyYwdixY4Hjr4cQEbGDekZERETEVgojIiIiYisN04iIiIit1DMiIiIitlIYEREREVspjIiIiIitFEZERETEVgojIiIiYiuFEREREbGVwoiIiIjYSmFEREREbPX/AQASKZE87ACKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='Train')\n",
    "pyplot.plot(history.history['val_loss'], label='Test')\n",
    "pyplot.legend()\n",
    "plt.xlabel('Epoch Number',fontweight='bold')\n",
    "pyplot.savefig('train_loss_lstm.pdf')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be1e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(temp, OutputDir):\n",
    "    validation_result = temp;\n",
    "    for x in [validation_result]:\n",
    "\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "        i = 0\n",
    "        for val in x:\n",
    "            tpr = val['tpr']\n",
    "            fpr = val['fpr']\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.4f)' % (i+1, roc_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        print;\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=1,color='c' , alpha=.8)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.4f)' % (mean_auc, std_auc),\n",
    "                 lw=1, alpha=.8)\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate',fontweight='bold')\n",
    "        plt.ylabel('True Positive Rate',fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "       # plt.savefig( OutputDir + '/'+'roc_capsulnet_independet.pdf') #save the figure in your file path\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "        \n",
    "        \n",
    "         #************************** Precision Recall Curve*********************************\n",
    "        j=0\n",
    "        prs = []\n",
    "        pre_aucs = []\n",
    "        mean_recal= np.linspace(0, 1, 100)\n",
    "        for val in x:\n",
    "            pre = val['prec']\n",
    "            rec = val['reca']\n",
    "            prs.append(interp(mean_recal, rec, pre))\n",
    "            prs[-1][0] = 0.0\n",
    "            p_r_auc = auc(rec, pre)\n",
    "            pre_aucs.append(p_r_auc)\n",
    "            plt.plot(rec, pre, lw=1, alpha=0.3,label='PRC fold %d (AUC = %0.3f)' % (j+1, p_r_auc))\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        print;\n",
    "\n",
    "        #plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_pre = np.mean(prs, axis=0)\n",
    "        mean_pre[-1] = 1.0\n",
    "        mean_auc = auc(mean_recal, mean_pre)\n",
    "        std_auc = np.std(pre_aucs)\n",
    "        plt.plot(mean_recal, mean_pre, color='b',\n",
    "                 label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.4f)' % (mean_auc, std_auc),\n",
    "                 lw=1, alpha=.8)\n",
    "\n",
    "        std_pre = np.std(prs, axis=0)\n",
    "        pre_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "        pre_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "        plt.fill_between(mean_recal, pre_lower, pre_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([0.0, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('Recall',fontweight='bold')\n",
    "        plt.ylabel('Precision',fontweight='bold')\n",
    "        plt.title('Prc capsulnet',fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        #plt.savefig( OutputDir + '/' +'PRC_capsulnet_lstmcaps_ind.pdf') ## save file on your own path\n",
    "        plt.show()\n",
    "        plt.close('all')\n",
    "\n",
    "        \n",
    "def cal(X,y, model):\n",
    "    pred_y =  model.predict(X)[::,1]\n",
    "    ROCArea = roc_auc_score(y, pred_y)\n",
    "    print(ROCArea)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_y)\n",
    "    lossValue = None\n",
    "    \n",
    "    pre, rec, threshlds = precision_recall_curve(y, pred_y)\n",
    "    pre = np.fliplr([pre])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "    rec = np.fliplr([rec])[0]  \n",
    "    AUC_prec_rec = np.trapz(rec,pre)\n",
    "    AUC_prec_rec = abs(AUC_prec_rec)\n",
    "    \n",
    "    \n",
    "    return { 'fpr' : fpr, 'tpr' : tpr, 'thresholds' : thresholds,'pre_recall_curve':AUC_prec_rec,'prec':pre,'reca':rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18fbb2f4-ecc4-48a7-9f06-9e9bfc079926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent test \n",
    "#ACP212\n",
    "positive_ind_data=read_fasta('acp_test_212_padded.fasta')\n",
    "negative_ind_data=read_fasta('neg_test_212_padded.fasta')\n",
    "all_ind_data=positive_ind_data+negative_ind_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9b0430e-0cf4-421a-85d8-3bb06c7ef0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ind_onehot=onehot(all_ind_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81b10214-b11e-49d1-b819-43b141d80eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ind=len(positive_ind_data)*[1]+ len(negative_ind_data)*[0]\n",
    "label_ind=np.asarray(label_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b54244e9-799e-4ad5-82c4-78febb7b3cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8120/3989221548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmode1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TCap'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Lastepoch_new'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load model weights for five folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mvalidation_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_ind_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#validation_result.append(cal(data_papaya,training_label[:,1], mode1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ENVNAMEE/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \"\"\"\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'keras_version'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Plot the AUC-ROC curve AND calculate the AUC-ROC score, precision-recall curve and calculate the AUC-PR score\n",
    "import warnings\n",
    "from keras.models import load_model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "OutputDir=\"/home/sadik/TCap/indepedent_result\" # give your own path in your personal computer\n",
    "validation_result=[]\n",
    "for i in range(5):\n",
    "        mode1.load_weights('TCap'+str(i+0)+'Lastepoch_new'+'.h5')  # load model weights for five folds\n",
    "        validation_result.append(cal(all_ind_onehot,label_ind, mode1))\n",
    "        #validation_result.append(cal(data_papaya,training_label[:,1], mode1))\n",
    "temp_dict = (validation_result)\n",
    "ROC(temp_dict, OutputDir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5664c-8cd7-4d8d-80cf-a4bb453a5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "OutputDir=\"/home/sadik/TCap/indepedent_result\" # give your own path in your personal computer\n",
    "validation_result = []\n",
    "\n",
    "# Define lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    mode1 = load_model('TCap'+ str(i)+'Lastepoch_new'+'.h5')  # load model weights for five folds\n",
    "    predictions = mode1.predict(all_ind_onehot)\n",
    "    \n",
    "    # Perform thresholding if needed\n",
    "    # thresholded_predictions = (predictions > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(label_ind, predictions)\n",
    "    precision = precision_score(label_ind, predictions)\n",
    "    recall = recall_score(label_ind, predictions)\n",
    "    mcc = matthews_corrcoef(label_ind, predictions)\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    mcc_scores.append(mcc)\n",
    "\n",
    "    # validation_result.append(cal(all_ind_onehot,label_ind, mode1))\n",
    "    # validation_result.append(cal(data_papaya,training_label[:,1], mode1))\n",
    "    # temp_dict = (validation_result)\n",
    "    \n",
    "# Calculate average scores\n",
    "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "average_precision = sum(precision_scores) / len(precision_scores)\n",
    "average_recall = sum(recall_scores) / len(recall_scores)\n",
    "average_mcc = sum(mcc_scores) / len(mcc_scores)\n",
    "\n",
    "# Print or save the average scores\n",
    "print(\"Average Accuracy:\", average_accuracy)\n",
    "print(\"Average Precision:\", average_precision)\n",
    "print(\"Average Recall:\", average_recall)\n",
    "print(\"Average MCC:\", average_mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d67fec-a67a-4249-9e3d-f7fc59ae4a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb43ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
